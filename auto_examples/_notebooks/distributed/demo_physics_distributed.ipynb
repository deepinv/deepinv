{
  "cells": [
    {
      "id": "4fc2630d",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "f063a2fc",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "# Distributed Physics Operators\n\nMany large-scale imaging problems involve operators that can be naturally decomposed as a stack of\nmultiple sub-operators:\n\n\\begin{align}A(x) = \\begin{bmatrix} A_1(x) \\\\ \\vdots \\\\ A_N(x) \\end{bmatrix}\\end{align}\n\nwhere each sub-operator $A_i$ is computationally expensive. Examples include multi-coil MRI,\nradio interferometry with multiple antennas, or multi-sensor imaging systems where each sensor provides\na different measurement.\n\nThe distributed framework enables you to parallelize the computation of these operators across multiple\ndevices. Each device handles a subset of the sub-operators independently, and the results are automatically\nassembled. This is particularly useful when the forward operator $A$, its adjoint $A^{\\top}$,\nor composition $A^{\\top}A$ is computationally intensive.\n\nThis example demonstrates how to use [`deepinv.distributed.distribute`](https://deepinv.github.io/deepinv/api/stubs/deepinv.distributed.distribute.html) to distribute stacked physics\noperators across multiple processes/devices for parallel computation.\n\n**Usage:**\n\n```bash\n# Single process\npython examples/distributed/demo_physics_distributed.py\n```\n```bash\n# Multi-process with torchrun (2 processes)\npython -m torch.distributed.run --nproc_per_node=2 examples/distributed/demo_physics_distributed.py\n```\n**Key Features:**\n\n- Distribute multiple operators across processes/devices\n- Parallel forward operations $A$\n- Parallel adjoint operations $A^{\\top}$\n- Parallel composition $A^{\\top} A$\n- Automatic result assembly from distributed processes\n\n**Key Steps:**\n\n1. Create multiple physics operators with different blur kernels\n2. Stack them using [`deepinv.physics.stack`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.stack.html)\n3. Initialize distributed context\n4. Distribute physics with [`deepinv.distributed.distribute`](https://deepinv.github.io/deepinv/api/stubs/deepinv.distributed.distribute.html)\n5. Apply forward, adjoint, and composition operations\n6. Visualize results\n\n# Import modules and define noisy image generation\nWe start by importing `torch` and the modules of deepinv that we use in this example. We also define a function that generates noisy images to evaluate the distributed framework."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom deepinv.physics import Blur, stack\nfrom deepinv.physics.blur import gaussian_blur\nfrom deepinv.utils.demo import load_example\nfrom deepinv.utils.plotting import plot\n\n# Import distributed framework\nfrom deepinv.distributed import DistributedContext, distribute\n\n\ndef create_stacked_physics(device, img_size=1024):\n    \"\"\"\n    Create stacked physics operators with different Gaussian blur kernels.\n\n    :param device: Device to create operators on\n    :param tuple img_size: Size of the image (H, W)\n    :returns: Tuple of (stacked_physics, clean_image)\n    \"\"\"\n    # Load example image\n    clean_image = load_example(\n        \"CBSD_0010.png\",\n        grayscale=False,\n        device=device,\n        img_size=img_size,\n        resize_mode=\"resize\",\n    )\n\n    # Create different Gaussian blur kernels\n    kernels = [\n        gaussian_blur(sigma=1.0, device=str(device)),  # Small blur\n        gaussian_blur(sigma=2.5, device=str(device)),  # Medium blur\n        gaussian_blur(\n            sigma=(1.5, 3.5), angle=30, device=str(device)\n        ),  # Anisotropic blur\n    ]\n\n    # Create physics operators (without noise for exact comparison)\n    physics_list = []\n\n    for kernel in kernels:\n        # Create blur operator with circular padding\n        physics = Blur(filter=kernel, padding=\"circular\", device=str(device))\n        physics = physics.to(device)\n\n        physics_list.append(physics)\n\n    # Stack physics operators into a single operator\n    stacked_physics = stack(*physics_list)\n\n    return stacked_physics, clean_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Configuration of parallel physics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define distributed context and run algorithm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize distributed context (handles single and multi-process automatically)\nwith DistributedContext(seed=42) as ctx:\n\n    if ctx.rank == 0:\n        print(\"=\" * 70)\n        print(\"Distributed Physics Operators Demo\")\n        print(\"=\" * 70)\n        print(f\"\\nRunning on {ctx.world_size} process(es)\")\n        print(f\"   Device: {ctx.device}\")\n\n    # ---------------------------------------------------------------------------\n    # Step 1: Create stacked physics operators\n    # ---------------------------------------------------------------------------\n\n    stacked_physics, clean_image = create_stacked_physics(ctx.device, img_size=img_size)\n\n    if ctx.rank == 0:\n        print(f\"\\nCreated stacked physics with {len(stacked_physics)} operators\")\n        print(f\"   Image shape: {clean_image.shape}\")\n        print(\n            f\"   Operator types: {[type(p).__name__ for p in stacked_physics.physics_list]}\"\n        )\n\n    # ---------------------------------------------------------------------------\n    # Step 2: Distribute physics across processes\n    # ---------------------------------------------------------------------------\n\n    distributed_physics = distribute(stacked_physics, ctx)\n\n    if ctx.rank == 0:\n        print(f\"\\n Distributed physics created\")\n        print(\n            f\"   Local operators on this rank: {len(distributed_physics.local_indexes)}\"\n        )\n\n    # ---------------------------------------------------------------------------\n    # Step 3: Test forward operation (A)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\n Testing forward operation (A)...\")\n\n    # Apply distributed forward operation\n    measurements = distributed_physics(clean_image)\n\n    # Compare with non-distributed result (only on rank 0)\n    measurements_ref = None\n    if ctx.rank == 0:\n        print(f\"   Output type: {type(measurements).__name__}\")\n        print(f\"   Number of measurements: {len(measurements)}\")\n        for i, m in enumerate(measurements):\n            print(f\"   Measurement {i} shape: {m.shape}\")\n\n        print(f\"\\n Comparing with non-distributed forward operation...\")\n        measurements_ref = stacked_physics(clean_image)\n\n        max_diff = 0.0\n        mean_diff = 0.0\n        for i in range(len(measurements)):\n            diff = torch.abs(measurements[i] - measurements_ref[i])\n            max_diff = max(max_diff, diff.max().item())\n            mean_diff += diff.mean().item()\n        mean_diff /= len(measurements)\n\n        print(f\"   Mean absolute difference: {mean_diff:.2e}\")\n        print(f\"   Max absolute difference:  {max_diff:.2e}\")\n\n        # Assert exact equality (should be zero for deterministic operations)\n        assert (\n            max_diff < 1e-6\n        ), f\"Distributed forward operation differs from non-distributed: max diff = {max_diff}\"\n        print(f\"   Results match exactly!\")\n\n    # ---------------------------------------------------------------------------\n    # Step 4: Test adjoint operation (A^T)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nTesting adjoint operation (A^T)...\")\n\n    # Apply adjoint operation\n    adjoint_result = distributed_physics.A_adjoint(measurements)\n\n    if ctx.rank == 0:\n        print(f\"   Output shape: {adjoint_result.shape}\")\n        print(f\"   Output norm: {torch.linalg.norm(adjoint_result).item():.4f}\")\n\n        # Compare with non-distributed result\n        print(f\"\\nComparing with non-distributed adjoint operation...\")\n        assert measurements_ref is not None\n        adjoint_ref = stacked_physics.A_adjoint(measurements_ref)\n        diff = torch.abs(adjoint_result - adjoint_ref)\n        print(f\"   Mean absolute difference: {diff.mean().item():.2e}\")\n        print(f\"   Max absolute difference:  {diff.max().item():.2e}\")\n\n        # Assert exact equality\n        assert (\n            diff.max().item() < 1e-6\n        ), f\"Distributed adjoint differs from non-distributed: max diff = {diff.max().item()}\"\n        print(f\"   Results match exactly!\")\n\n    # ---------------------------------------------------------------------------\n    # Step 5: Test composition (A^T A)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nTesting composition (A^T A)...\")\n\n    # Apply composition\n    ata_result = distributed_physics.A_adjoint_A(clean_image)\n\n    if ctx.rank == 0:\n        print(f\"   Output shape: {ata_result.shape}\")\n        print(f\"   Output norm: {torch.linalg.norm(ata_result).item():.4f}\")\n\n        # Compare with non-distributed result\n        print(f\"\\nComparing with non-distributed A^T A operation...\")\n        ata_ref = stacked_physics.A_adjoint_A(clean_image)\n        diff = torch.abs(ata_result - ata_ref)\n        print(f\"   Mean absolute difference: {diff.mean().item():.2e}\")\n        print(f\"   Max absolute difference:  {diff.max().item():.2e}\")\n\n        # Assert exact equality\n        assert (\n            diff.max().item() < 1e-6\n        ), f\"Distributed A^T A differs from non-distributed: max diff = {diff.max().item()}\"\n        print(f\"   Results match exactly!\")\n\n    # ---------------------------------------------------------------------------\n    # Step 6: Visualize results (only on rank 0)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nVisualizing results...\")\n\n        # Plot original image and measurements\n        images_to_plot = [clean_image] + [m for m in measurements]\n        titles = [\"Original Image\"] + [\n            f\"Measurement {i+1}\" for i in range(len(measurements))\n        ]\n\n        plot(\n            images_to_plot,\n            titles=titles,\n            save_fn=\"distributed_physics_forward.png\",\n            figsize=(15, 4),\n        )\n\n        # Plot adjoint and A^T A results\n        # Normalize for visualization\n        adjoint_vis = (adjoint_result - adjoint_result.min()) / (\n            adjoint_result.max() - adjoint_result.min() + 1e-8\n        )\n        ata_vis = (ata_result - ata_result.min()) / (\n            ata_result.max() - ata_result.min() + 1e-8\n        )\n\n        plot(\n            [clean_image, adjoint_vis, ata_vis],\n            titles=[\"Original\", r\"$A^T(y)$\", r\"$A^T A(x)$\"],\n            save_fn=\"distributed_physics_adjoint.png\",\n            figsize=(12, 4),\n        )\n\n        print(f\"\\n Demo completed successfully!\")\n        print(f\"   Results saved to:\")\n        print(f\"   - distributed_physics_forward.png\")\n        print(f\"   - distributed_physics_adjoint.png\")\n        print(\"\\n\" + \"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}