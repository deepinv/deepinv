
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="robots" content="noindex">

    <title>deepinv.optim.linear.least_squares &#8212; deepinv 0.3.7 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=9112d68a" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=29d04658"></script>
    <script src="../../../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=35a8b989"></script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-NSEKFKYSGR"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-NSEKFKYSGR');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/deepinv/optim/linear/least_squares';</script>
    <link rel="canonical" href="https://deepinv.github.io/deepinv/_modules/deepinv/optim/linear/least_squares.html" />
    <link rel="icon" href="../../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">ðŸŽ‰ We are part of the <a href='https://landscape.pytorch.org/?item=modeling--computer-vision--deepinverse' target='_blank'> official PyTorch ecosystem!</a><br>ðŸ“§ <a href='https://forms.gle/TFyT7M2HAWkJYfvQ7' target='_blank'> Join our mailing list</a> for releases and updates.</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/deepinv_logolarge.png" class="logo__image only-light" alt="deepinv 0.3.7 documentation - Home"/>
    <img src="../../../../_static/logo_large_dark.png" class="logo__image only-dark pst-js-only" alt="deepinv 0.3.7 documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../finding_help.html">
    Finding Help
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../../community.html">
    Community
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../../changelog.html">
    Change Log
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../finding_help.html">
    Finding Help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../community.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../changelog.html">
    Change Log
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">deepinv.optim.linear.least_squares</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for deepinv.optim.linear.least_squares</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">zeros_like</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd.function</span><span class="w"> </span><span class="kn">import</span> <span class="n">once_differentiable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils.tensorlist</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorList</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils.compat</span><span class="w"> </span><span class="kn">import</span> <span class="n">zip_strict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.bicgstab</span><span class="w"> </span><span class="kn">import</span> <span class="n">bicgstab</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.conjugate_gradient</span><span class="w"> </span><span class="kn">import</span> <span class="n">conjugate_gradient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.lsqr</span><span class="w"> </span><span class="kn">import</span> <span class="n">lsqr</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.minres</span><span class="w"> </span><span class="kn">import</span> <span class="n">minres</span>


<div class="viewcode-block" id="least_squares">
<a class="viewcode-back" href="../../../../api/stubs/deepinv.optim.linear.least_squares.html#deepinv.optim.linear.least_squares">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">least_squares</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">AT</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">AAT</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ATA</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">solver</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CG&quot;</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves :math:`\min_x \|Ax-y\|^2 + \frac{1}{\gamma}\|x-z\|^2` using the specified solver.</span>

<span class="sd">    The solvers are stopped either when :math:`\|Ax-y\| \leq \text{tol} \times \|y\|` or</span>
<span class="sd">    when the maximum number of iterations is reached.</span>

<span class="sd">    The solution depends on the regularization parameter :math:`\gamma`:</span>

<span class="sd">    - If `gamma=None` (:math:`\gamma = \infty`), it solves the unregularized least squares problem :math:`\min_x \|Ax-y\|^2`.</span>
<span class="sd">        - If :math:`A` is overcomplete (rows&gt;=columns), it computes the minimum norm solution :math:`x = A^{\top}(AA^{\top})^{-1}y`.</span>
<span class="sd">        - If :math:`A` is undercomplete (columns&gt;rows), it computes the least squares solution :math:`x = (A^{\top}A)^{-1}A^{\top}y`.</span>
<span class="sd">    - If :math:`0 &lt; \gamma &lt; \infty`, it computes the least squares solution :math:`x = (A^{\top}A + \frac{1}{\gamma}I)^{-1}(A^{\top}y + \frac{1}{\gamma}z)`.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        If :math:`\gamma \leq 0`, the problem can become non-convex and the solvers are not designed for that.</span>
<span class="sd">        A warning is raised, but solvers continue anyway (except for LSQR, which cannot be used for negative :math:`\gamma`).</span>

<span class="sd">    Available solvers are:</span>

<span class="sd">    - `&#39;CG&#39;`: `Conjugate Gradient &lt;https://en.wikipedia.org/wiki/Conjugate_gradient_method&gt;`_.</span>
<span class="sd">    - `&#39;BiCGStab&#39;`: `Biconjugate Gradient Stabilized method &lt;https://en.wikipedia.org/wiki/Biconjugate_gradient_stabilized_method&gt;`_</span>
<span class="sd">    - `&#39;lsqr&#39;`: `Least Squares QR &lt;https://www-leland.stanford.edu/group/SOL/software/lsqr/lsqr-toms82a.pdf&gt;`_</span>
<span class="sd">    - `&#39;minres&#39;`: `Minimal Residual Method &lt;https://en.wikipedia.org/wiki/Minimal_residual_method&gt;`_</span>

<span class="sd">    .. note::</span>

<span class="sd">        Both `&#39;CG&#39;` and `&#39;BiCGStab&#39;` are used for squared linear systems, while `&#39;lsqr&#39;` is used for rectangular systems.</span>

<span class="sd">        If the chosen solver requires a squared system, we map to the problem to the normal equations:</span>
<span class="sd">        If the size of :math:`y` is larger than :math:`x` (overcomplete problem), it computes :math:`(A^{\top} A)^{-1} A^{\top} y`,</span>
<span class="sd">        otherwise (incomplete problem) it computes :math:`A^{\top} (A A^{\top})^{-1} y`.</span>


<span class="sd">    :param Callable A: Linear operator :math:`A` as a callable function.</span>
<span class="sd">    :param Callable AT: Adjoint operator :math:`A^{\top}` as a callable function.</span>
<span class="sd">    :param torch.Tensor y: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor z: input tensor of shape (B, ...) or scalar.</span>
<span class="sd">    :param torch.Tensor init: (Optional) initial guess for the solver. If None, it is set to a tensor of zeros.</span>
<span class="sd">    :param None, float, torch.Tensor gamma: (Optional) inverse regularization parameter. Can be batched (shape (B, ...)) or a scalar.</span>
<span class="sd">        If multi-dimensional tensor, then its shape must match that of :math:`A^{\top} y`.</span>
<span class="sd">        If None, it is set to :math:`\infty` (no regularization).</span>
<span class="sd">    :param str solver: solver to be used, options are `&#39;CG&#39;`, `&#39;BiCGStab&#39;`, `&#39;lsqr&#39;` and `&#39;minres&#39;`.</span>
<span class="sd">    :param Callable AAT: (Optional) Efficient implementation of :math:`A(A^{\top}(x))`. If not provided, it is computed as :math:`A(A^{\top}(x))`.</span>
<span class="sd">    :param Callable ATA: (Optional) Efficient implementation of :math:`A^{\top}(A(x))`. If not provided, it is computed as :math:`A^{\top}(A(x))`.</span>
<span class="sd">    :param int max_iter: maximum number of iterations.</span>
<span class="sd">    :param float tol: relative tolerance for stopping the algorithm.</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param kwargs: Keyword arguments to be passed to the solver.</span>
<span class="sd">    :return: (:class:`torch.Tensor`) :math:`x` of shape (B, ...).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">gamma_provided</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gamma_provided</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">gamma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Regularization parameter of least squares problem (gamma) should be positive.&quot;</span>
                <span class="s2">&quot;Otherwise, the problem can become non-convex and the solvers are not designed for that.&quot;</span>
                <span class="s2">&quot;Continuing anyway...&quot;</span>
            <span class="p">)</span>

    <span class="n">Aty</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if batched gamma</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Aty</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Aty</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ndim</span> <span class="o">=</span> <span class="n">Aty</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ndim</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If gamma is batched, its batch size must match the one of y.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># expand gamma to ATy</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;gamma should either be 0D, 1D, or match same number of dimensions as ATy, but got ndims </span><span class="si">{</span><span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;lsqr&quot;</span><span class="p">:</span>  <span class="c1"># rectangular solver</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="k">if</span> <span class="n">gamma_provided</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lsqr</span><span class="p">(</span>
            <span class="n">A</span><span class="p">,</span>
            <span class="n">AT</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">x0</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">complete</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">overcomplete</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">complete</span> <span class="ow">and</span> <span class="p">(</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;BiCGStab&quot;</span> <span class="ow">or</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;minres&quot;</span><span class="p">):</span>
            <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">AAT</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">AAT</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A</span><span class="p">(</span><span class="n">AT</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ATA</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ATA</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">AT</span><span class="p">(</span><span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">gamma_provided</span><span class="p">:</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">z</span>
                <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ATA</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span>
                <span class="n">overcomplete</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">overcomplete</span><span class="p">:</span>
                    <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">AAT</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">y</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ATA</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">Aty</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;CG&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">conjugate_gradient</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;BiCGStab&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">bicgstab</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;minres&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">minres</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Solver </span><span class="si">{</span><span class="n">solver</span><span class="si">}</span><span class="s2"> not recognized. Choose between &#39;CG&#39;, &#39;lsqr&#39; and &#39;BiCGStab&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">gamma_provided</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">overcomplete</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">complete</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">LeastSquaresSolver</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom autograd function for the least squares solver to enable O(1) memory backward propagation using implicit differentiation.</span>

<span class="sd">    The forward pass solves the following problem using :func:`deepinv.optim.linear.least_squares`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \min_x \|A_\theta x - y \|^2 + \frac{1}{\gamma} \|x - z\|^2</span>

<span class="sd">    where :math:`A_\theta` is a linear operator :class:`deepinv.physics.LinearPhysics` parameterized by :math:`\theta`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function uses a :func:`least squares &lt;deepinv.optim.linear.least_squares&gt;` solver under the hood, which supports various solvers such as Conjugate Gradient (CG), BiCGStab, LSQR, and MinRes (see :func:`deepinv.optim.linear.least_squares` for more details).</span>

<span class="sd">    The backward pass computes the gradients with respect to the inputs using implicit differentiation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE: the physics parameters are handled as side-effects (not inputs/outputs of the autograd function)</span>
    <span class="c1"># we add a dummy input tensor `trigger` to trigger backward when needed (i.e. when physics parameters require grad)</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="n">ctx</span><span class="p">,</span>
        <span class="n">physics</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">trigger</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">extra_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">extra_kwargs</span> <span class="k">if</span> <span class="n">extra_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">solution</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
                <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
                <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Save tensors only</span>
        <span class="n">gamma_orig_shape</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># For broadcasting with other tensors. Note we already have checked gamma shapes</span>
        <span class="c1"># in forward, so the following is just for gamma batched but not shaped.</span>
        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
                <span class="n">ndim</span> <span class="o">=</span> <span class="n">solution</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ndim</span> <span class="o">=</span> <span class="n">solution</span><span class="o">.</span><span class="n">ndim</span>

            <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="c1"># Save other non-tensor contexts</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">physics</span> <span class="o">=</span> <span class="n">physics</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span> <span class="o">=</span> <span class="n">gamma_orig_shape</span>

        <span class="k">return</span> <span class="n">solution</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@once_differentiable</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">physics</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">physics</span>

        <span class="c1"># Solve (A^T A + I/gamma) mv = grad_output</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">mv</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
                <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                <span class="n">z</span><span class="o">=</span><span class="n">grad_output</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
                <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
                <span class="o">**</span><span class="n">ctx</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Build grads aligned with inputs: (physics, y, z, gamma, solver, max_iter, tol, ..., **kwargs)</span>
        <span class="n">needs</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">needs</span><span class="p">)</span>

        <span class="c1"># dL/dy = A mv</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">mv</span><span class="p">)</span>

        <span class="c1"># dL/dz = (1/gamma) mv</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mv</span> <span class="o">/</span> <span class="n">gamma</span>

        <span class="c1"># dL/dgamma = &lt;mv, h - z&gt; / gamma^2</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">z</span>
            <span class="c1"># vdot gives correct conjugation for complex; .real keeps real scalar</span>
            <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">mv</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span> <span class="o">*</span> <span class="n">diff</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mv</span><span class="o">.</span><span class="n">ndim</span><span class="p">)),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># If gamma was batched in the forward, we return a batched grad</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">grad_gamma</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span><span class="p">)</span>
            <span class="c1"># gamma was a scalar in the forward, we accumulate all grads to return a scalar, similar to</span>
            <span class="c1"># torch autograd behavior for broadcasted inputs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_gamma</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(())</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_gamma</span>

        <span class="c1"># Optional: implicit grads w.r.t physics parameters (side-effect accumulation)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="s2">&quot;buffers&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[])()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># pseudo-loss = - &lt;mv, A^T (A h - y)&gt;</span>
            <span class="n">h_det</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">y_det</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">pseudo</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span>
                    <span class="n">mv</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">h_det</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_det</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="p">)</span><span class="o">.</span><span class="n">real</span>
            <span class="n">g_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                <span class="n">pseudo</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">zip_strict</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span>
                <span class="n">g_params</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>


<span class="c1"># wrapper of the autograd function for easier use</span>
<div class="viewcode-block" id="least_squares_implicit_backward">
<a class="viewcode-back" href="../../../../api/stubs/deepinv.optim.linear.least_squares_implicit_backward.html#deepinv.optim.linear.least_squares_implicit_backward">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">least_squares_implicit_backward</span><span class="p">(</span>
    <span class="n">physics</span><span class="p">,</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Least squares solver with O(1) memory backward propagation using implicit differentiation.</span>
<span class="sd">    The function is similar to :func:`deepinv.optim.linear.least_squares` for the forward pass, but uses implicit differentiation for the backward pass, which reduces memory consumption to O(1) in the number of iterations.</span>

<span class="sd">    This function supports backpropagation with respect to the inputs :math:`y`, :math:`z` and :math:`\gamma` and also with respect to the parameters of the physics operator :math:`A_\theta` if they require gradients. See :ref:`sphx_glr_auto_examples_unfolded_demo_unfolded_constant_memory.py` and the notes below for more details.</span>

<span class="sd">    Let :math:`h(z, y, \theta, \gamma)` denote the output of the least squares solver, i.e. the solution of the following problem:</span>

<span class="sd">    .. math::</span>

<span class="sd">        h(z, y, \theta, \gamma) = \underset{x}{\arg\min} \; \frac{\gamma}{2}\|A_\theta x-y\|^2 + \frac{1}{2}\|x-z\|^2</span>

<span class="sd">    When the forward least-squares solver converges to the exact minimizer, we have the following closed-form expressions for :math:`h(z, y, \theta, \gamma)`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        h(z, y, \theta, \gamma) = \left( A_\theta^{\top} A_\theta + \frac{1}{\gamma} I \right)^{-1} \left( A_\theta^{\top} y + \frac{1}{\gamma} z \right)</span>

<span class="sd">    Let :math:`M` denote the inverse :math:`\left( A_\theta^T A_\theta + \frac{1}{\gamma} I \right)^{-1}`. In the forward, we need to compute the vector-Jacobian products (VJPs), which can be computed as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \left( \frac{\partial h}{\partial z} \right)^{\top} v               &amp;= \frac{1}{\gamma} M v \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial y} \right)^{\top} v               &amp;= A_\theta M v \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial \gamma} \right)^{\top} v          &amp;=   (h - z)^\top M  v / \gamma^2 \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial \theta} \right)^{\top} v          &amp;= \frac{\partial p}{\partial \theta}</span>

<span class="sd">    where :math:`p =  (y - A_\theta h)^{\top} A_\theta M v` and :math:`\frac{\partial p}{\partial \theta}` can be computed using the standard backpropagation mechanism (autograd).</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function only supports first-order gradients. Higher-order gradients are not supported. If you need higher-order gradients, please use :func:`deepinv.optim.linear.least_squares` instead but be aware that it requires storing all intermediate iterates, which can be memory-intensive.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function also supports implicit gradients with respect to the parameters of the physics operator :math:`A_\theta` if they require gradients. This is useful for learning the physics parameters in an end-to-end fashion. The gradients are accumulated in-place in the `.grad` attribute of the parameters of the physics operator. To make this work, the function takes as input the physics operator itself (not just its matmul functions) and checks if any of its parameters require gradients. If so, it triggers the backward pass accordingly.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        Implicit gradients can be incorrect if the least squares solver does not converge sufficiently. Make sure to set the `max_iter` and `tol` parameters of the least squares solver appropriately to ensure convergence. You can monitor the convergence by setting `verbose=True` in the least squares solver via `kwargs`. If the solver does not converge, the implicit gradients can be very inaccurate and lead to divergence of the training.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        This function does not support :class:`deepinv.utils.TensorList` inputs yet. If you use :class:`deepinv.utils.TensorList` as inputs, the function will fall back to standard least squares with full backpropagation.</span>

<span class="sd">    .. tip::</span>

<span class="sd">        If you do not need gradients with respect to the physics parameters, you can set `requires_grad=False` for all parameters of the physics operator to avoid the additional backward pass. This can save some computation time.</span>

<span class="sd">    .. tip::</span>

<span class="sd">        Training unfolded network with implicit differentiation can reduce memory consumption significantly, especially when using many iterations. On GPU, we can expect a memory reduction factor of about 2x-3x compared to standard backpropagation and a speed-up of about 1.2x-1.5x. The exact numbers depend on the problem and the number of iterations.</span>

<span class="sd">    :param deepinv.physics.LinearPhysics physics: physics operator :class:`deepinv.physics.LinearPhysics`.</span>
<span class="sd">    :param torch.Tensor y: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor, float, int, None  z: input tensor of shape (B, ...). Default is `None`, which corresponds to a zero tensor.</span>
<span class="sd">    :param None, torch.Tensor init: Optional initial guess, only used for the forward pass. Default is `None`, which corresponds to a zero initialization.</span>
<span class="sd">    :param None, float, torch.Tensor gamma: regularization parameter :math:`\gamma &gt; 0`. Default is `None`. Can be batched (shape (B, ...)) or a scalar.</span>
<span class="sd">    :param kwargs: additional arguments to be passed to the least squares solver.</span>

<span class="sd">    :return: (:class:`torch.Tensor`) :math:`x` of shape (B, ...), the solution of the least squares problem.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># To get correct shape</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="c1"># NOTE: TensorList not supported by autograd function, we fall back to standard least_squares in this case for now</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Warning: least_squares_implicit_backward does not support TensorList inputs. Falling back to standard least_squares with full backpropagation.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">least_squares</span><span class="p">(</span>
            <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
            <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
            <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">physics_requires_grad_params</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="s2">&quot;buffers&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[])()</span>
    <span class="p">)</span>
    <span class="c1"># NOTE: backward is triggered if any of the inputs require grad</span>
    <span class="c1"># When input tensors (y, z, gamma) do not require grad and we want to do backward w.r.t physics parameters, we need to trigger it manually by a dummy tensor.</span>
    <span class="n">trigger_backward</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="ow">or</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gamma</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">physics_requires_grad_params</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">trigger_backward</span><span class="p">:</span>
        <span class="n">trigger</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span>
            <span class="kc">True</span>
        <span class="p">)</span>  <span class="c1"># Dummy tensor to trigger backward</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trigger</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">extra_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">else</span> <span class="n">y</span><span class="o">.</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If gamma is batched, its batch size must match the one of y.&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">LeastSquaresSolver</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">trigger</span><span class="p">,</span> <span class="n">extra_kwargs</span><span class="p">)</span></div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      Â© Copyright deepinverse contributors 2025.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 9.0.4.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>