
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="robots" content="noindex">

    <title>deepinv.optim.utils &#8212; deepinv 0.3.5 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=9112d68a" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=ba50482b"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=35a8b989"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-NSEKFKYSGR"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-NSEKFKYSGR');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/deepinv/optim/utils';</script>
    <link rel="canonical" href="https://deepinv.github.io/deepinv/_modules/deepinv/optim/utils.html" />
    <link rel="icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/deepinv_logolarge.png" class="logo__image only-light" alt="deepinv 0.3.5 documentation - Home"/>
    <img src="../../../_static/logo_large_dark.png" class="logo__image only-dark pst-js-only" alt="deepinv 0.3.5 documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finding_help.html">
    Finding Help
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../community.html">
    Community
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finding_help.html">
    Finding Help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../community.html">
    Community
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">deepinv.optim.utils</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for deepinv.optim.utils</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">zeros_like</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd.function</span><span class="w"> </span><span class="kn">import</span> <span class="n">once_differentiable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.linalg</span><span class="w"> </span><span class="kn">import</span> <span class="n">vector_norm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils.tensorlist</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorList</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.utils.compat</span><span class="w"> </span><span class="kn">import</span> <span class="n">zip_strict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_conv</span><span class="p">(</span>
    <span class="n">X_prev</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
    <span class="n">it</span><span class="p">,</span>
    <span class="n">crit_conv</span><span class="o">=</span><span class="s2">&quot;residual&quot;</span><span class="p">,</span>
    <span class="n">thres_conv</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">crit_conv</span> <span class="o">==</span> <span class="s2">&quot;residual&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_prev</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">X_prev</span> <span class="o">=</span> <span class="n">X_prev</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">crit_cur</span> <span class="o">=</span> <span class="n">vector_norm</span><span class="p">(</span><span class="n">X_prev</span> <span class="o">-</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-06</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">crit_conv</span> <span class="o">==</span> <span class="s2">&quot;cost&quot;</span><span class="p">:</span>
        <span class="n">F_prev</span> <span class="o">=</span> <span class="n">X_prev</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
        <span class="n">crit_cur</span> <span class="o">=</span> <span class="n">vector_norm</span><span class="p">(</span><span class="n">F_prev</span> <span class="o">-</span> <span class="n">F</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-06</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;convergence criteria not implemented&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">crit_cur</span> <span class="o">&lt;</span> <span class="n">thres_conv</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s2">, current converge crit. = </span><span class="si">{</span><span class="n">crit_cur</span><span class="si">:</span><span class="s2">.2E</span><span class="si">}</span><span class="s2">, objective = </span><span class="si">{</span><span class="n">thres_conv</span><span class="si">:</span><span class="s2">.2E</span><span class="si">}</span><span class="s2"> </span><span class="se">\r</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="least_squares">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.least_squares.html#deepinv.optim.utils.least_squares">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">least_squares</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">AT</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">AAT</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ATA</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">solver</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CG&quot;</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves :math:`\min_x \|Ax-y\|^2 + \frac{1}{\gamma}\|x-z\|^2` using the specified solver.</span>

<span class="sd">    The solvers are stopped either when :math:`\|Ax-y\| \leq \text{tol} \times \|y\|` or</span>
<span class="sd">    when the maximum number of iterations is reached.</span>

<span class="sd">    The solution depends on the regularization parameter :math:`\gamma`:</span>

<span class="sd">    - If `gamma=None` (:math:`\gamma = \infty`), it solves the unregularized least squares problem :math:`\min_x \|Ax-y\|^2`.</span>
<span class="sd">        - If :math:`A` is overcomplete (rows&gt;=columns), it computes the minimum norm solution :math:`x = A^{\top}(AA^{\top})^{-1}y`.</span>
<span class="sd">        - If :math:`A` is undercomplete (columns&gt;rows), it computes the least squares solution :math:`x = (A^{\top}A)^{-1}A^{\top}y`.</span>
<span class="sd">    - If :math:`0 &lt; \gamma &lt; \infty`, it computes the least squares solution :math:`x = (A^{\top}A + \frac{1}{\gamma}I)^{-1}(A^{\top}y + \frac{1}{\gamma}z)`.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        If :math:`\gamma \leq 0`, the problem can become non-convex and the solvers are not designed for that.</span>
<span class="sd">        A warning is raised, but solvers continue anyway (except for LSQR, which cannot be used for negative :math:`\gamma`).</span>

<span class="sd">    Available solvers are:</span>

<span class="sd">    - `&#39;CG&#39;`: `Conjugate Gradient &lt;https://en.wikipedia.org/wiki/Conjugate_gradient_method&gt;`_.</span>
<span class="sd">    - `&#39;BiCGStab&#39;`: `Biconjugate Gradient Stabilized method &lt;https://en.wikipedia.org/wiki/Biconjugate_gradient_stabilized_method&gt;`_</span>
<span class="sd">    - `&#39;lsqr&#39;`: `Least Squares QR &lt;https://www-leland.stanford.edu/group/SOL/software/lsqr/lsqr-toms82a.pdf&gt;`_</span>
<span class="sd">    - `&#39;minres&#39;`: `Minimal Residual Method &lt;https://en.wikipedia.org/wiki/Minimal_residual_method&gt;`_</span>

<span class="sd">    .. note::</span>

<span class="sd">        Both `&#39;CG&#39;` and `&#39;BiCGStab&#39;` are used for squared linear systems, while `&#39;lsqr&#39;` is used for rectangular systems.</span>

<span class="sd">        If the chosen solver requires a squared system, we map to the problem to the normal equations:</span>
<span class="sd">        If the size of :math:`y` is larger than :math:`x` (overcomplete problem), it computes :math:`(A^{\top} A)^{-1} A^{\top} y`,</span>
<span class="sd">        otherwise (incomplete problem) it computes :math:`A^{\top} (A A^{\top})^{-1} y`.</span>


<span class="sd">    :param Callable A: Linear operator :math:`A` as a callable function.</span>
<span class="sd">    :param Callable AT: Adjoint operator :math:`A^{\top}` as a callable function.</span>
<span class="sd">    :param torch.Tensor y: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor z: input tensor of shape (B, ...) or scalar.</span>
<span class="sd">    :param torch.Tensor init: (Optional) initial guess for the solver. If None, it is set to a tensor of zeros.</span>
<span class="sd">    :param None, float, torch.Tensor gamma: (Optional) inverse regularization parameter. Can be batched (shape (B, ...)) or a scalar.</span>
<span class="sd">        If multi-dimensional tensor, then its shape must match that of :math:`A^{\top} y`.</span>
<span class="sd">        If None, it is set to :math:`\infty` (no regularization).</span>
<span class="sd">    :param str solver: solver to be used, options are `&#39;CG&#39;`, `&#39;BiCGStab&#39;`, `&#39;lsqr&#39;` and `&#39;minres&#39;`.</span>
<span class="sd">    :param Callable AAT: (Optional) Efficient implementation of :math:`A(A^{\top}(x))`. If not provided, it is computed as :math:`A(A^{\top}(x))`.</span>
<span class="sd">    :param Callable ATA: (Optional) Efficient implementation of :math:`A^{\top}(A(x))`. If not provided, it is computed as :math:`A^{\top}(A(x))`.</span>
<span class="sd">    :param int max_iter: maximum number of iterations.</span>
<span class="sd">    :param float tol: relative tolerance for stopping the algorithm.</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param kwargs: Keyword arguments to be passed to the solver.</span>
<span class="sd">    :return: (class:`torch.Tensor`) :math:`x` of shape (B, ...).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">gamma_provided</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gamma_provided</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">gamma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Regularization parameter of least squares problem (gamma) should be positive.&quot;</span>
                <span class="s2">&quot;Otherwise, the problem can become non-convex and the solvers are not designed for that.&quot;</span>
                <span class="s2">&quot;Continuing anyway...&quot;</span>
            <span class="p">)</span>

    <span class="n">Aty</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if batched gamma</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Aty</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Aty</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ndim</span> <span class="o">=</span> <span class="n">Aty</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">ndim</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If gamma is batched, its batch size must match the one of y.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># expand gamma to ATy</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;gamma should either be 0D, 1D, or match same number of dimensions as ATy, but got ndims </span><span class="si">{</span><span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;lsqr&quot;</span><span class="p">:</span>  <span class="c1"># rectangular solver</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="k">if</span> <span class="n">gamma_provided</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lsqr</span><span class="p">(</span>
            <span class="n">A</span><span class="p">,</span>
            <span class="n">AT</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">x0</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">complete</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">overcomplete</span> <span class="o">=</span> <span class="n">Aty</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">complete</span> <span class="ow">and</span> <span class="p">(</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;BiCGStab&quot;</span> <span class="ow">or</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;minres&quot;</span><span class="p">):</span>
            <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">AAT</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">AAT</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A</span><span class="p">(</span><span class="n">AT</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ATA</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ATA</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">AT</span><span class="p">(</span><span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">gamma_provided</span><span class="p">:</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">z</span>
                <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ATA</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span>
                <span class="n">overcomplete</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">overcomplete</span><span class="p">:</span>
                    <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">AAT</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">y</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">H</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ATA</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">Aty</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;CG&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">conjugate_gradient</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;BiCGStab&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">bicgstab</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;minres&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">minres</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">parallel_dim</span><span class="o">=</span><span class="n">parallel_dim</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Solver </span><span class="si">{</span><span class="n">solver</span><span class="si">}</span><span class="s2"> not recognized. Choose between &#39;CG&#39;, &#39;lsqr&#39; and &#39;BiCGStab&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">gamma_provided</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">overcomplete</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">complete</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">aux</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">ai</span><span class="p">,</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">zip_strict</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">x</span><span class="p">):</span>
            <span class="n">aux</span> <span class="o">+=</span> <span class="p">(</span><span class="n">ai</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span> <span class="o">*</span> <span class="n">bi</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>  <span class="c1"># performs batched dot product</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">aux</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># performs batched dot product</span>
    <span class="k">return</span> <span class="n">dot</span>


<div class="viewcode-block" id="conjugate_gradient">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.conjugate_gradient.html#deepinv.optim.utils.conjugate_gradient">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conjugate_gradient</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e2</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Standard conjugate gradient algorithm.</span>

<span class="sd">    It solves the linear system :math:`Ax=b`, where :math:`A` is a (square) linear operator and :math:`b` is a tensor.</span>

<span class="sd">    For more details see: http://en.wikipedia.org/wiki/Conjugate_gradient_method</span>

<span class="sd">    :param Callable A: Linear operator as a callable function, has to be square!</span>
<span class="sd">    :param torch.Tensor b: input tensor of shape (B, ...)</span>
<span class="sd">    :param int max_iter: maximum number of CG iterations</span>
<span class="sd">    :param float tol: absolute tolerance for stopping the CG algorithm.</span>
<span class="sd">    :param float eps: a small value for numerical stability</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param torch.Tensor init: Optional initial guess.</span>
<span class="sd">    :param bool verbose: Output progress information in the console.</span>
<span class="sd">    :return: torch.Tensor :math:`x` of shape (B, ...) verifying :math:`Ax=b`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parallel_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span>
    <span class="n">rsold</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">*</span> <span class="p">(</span><span class="n">tol</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)):</span>
        <span class="n">Ap</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">rsold</span> <span class="o">/</span> <span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Ap</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">Ap</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="n">rsnew</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">rsnew</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CG Converged at iteration&quot;</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">rsnew</span> <span class="o">/</span> <span class="p">(</span><span class="n">rsold</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>
        <span class="n">rsold</span> <span class="o">=</span> <span class="n">rsnew</span>

    <span class="k">if</span> <span class="n">flag</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CG did not converge&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span></div>



<div class="viewcode-block" id="bicgstab">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.bicgstab.html#deepinv.optim.utils.bicgstab">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">bicgstab</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e2</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">left_precon</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="n">right_precon</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Biconjugate gradient stabilized algorithm.</span>

<span class="sd">    Solves :math:`Ax=b` with :math:`A` squared using the BiCGSTAB algorithm:</span>

<span class="sd">    Van der Vorst, H. A. (1992). &quot;Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the Solution of Nonsymmetric Linear Systems&quot;. SIAM J. Sci. Stat. Comput. 13 (2): 631–644.</span>

<span class="sd">    For more details see: http://en.wikipedia.org/wiki/Biconjugate_gradient_stabilized_method</span>

<span class="sd">    :param Callable A: Linear operator as a callable function.</span>
<span class="sd">    :param torch.Tensor b: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor init: Optional initial guess.</span>
<span class="sd">    :param int max_iter: maximum number of BiCGSTAB iterations.</span>
<span class="sd">    :param float tol: absolute tolerance for stopping the BiCGSTAB algorithm.</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param bool verbose: Output progress information in the console.</span>
<span class="sd">    :param Callable left_precon: left preconditioner as a callable function.</span>
<span class="sd">    :param Callable right_precon: right preconditioner as a callable function.</span>
<span class="sd">    :return: (:class:`torch.Tensor`) :math:`x` of shape (B, ...)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parallel_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">r_hat</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r_hat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)</span>

    <span class="n">tol</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">*</span> <span class="p">(</span><span class="n">tol</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>  <span class="c1"># Breakdown tolerance, to avoid division by zero</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">right_precon</span><span class="p">(</span><span class="n">left_precon</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="c1"># Safeguard: avoid division by small/zero</span>
        <span class="n">alpha_denom</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">r_hat</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">alpha_denom</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">,</span> <span class="n">rho</span> <span class="o">/</span> <span class="n">alpha_denom</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rho</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">right_precon</span><span class="p">(</span><span class="n">left_precon</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># Safeguard: avoid division by small/zero</span>
        <span class="n">left_s</span> <span class="o">=</span> <span class="n">left_precon</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">left_t</span> <span class="o">=</span> <span class="n">left_precon</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">omega_num</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">left_t</span><span class="p">,</span> <span class="n">left_s</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">omega_denom</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">left_t</span><span class="p">,</span> <span class="n">left_t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">omega_denom</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">,</span>
            <span class="n">omega_num</span> <span class="o">/</span> <span class="n">omega_denom</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">omega_num</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">omega</span> <span class="o">*</span> <span class="n">z</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">omega</span> <span class="o">*</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">):</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BiCGSTAB Converged at iteration&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="n">rho_new</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r_hat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># Safeguard for beta: if rho or omega small, set beta=0</span>
        <span class="n">beta_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">rho</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">beta_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">rho_new</span> <span class="o">/</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">omega</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rho_new</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">omega</span> <span class="o">*</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="n">rho_new</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">flag</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BiCGSTAB did not converge&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_sym_ortho</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stable implementation of Givens rotation.</span>

<span class="sd">    Adapted from https://github.com/scipy/scipy/blob/v1.15.1/scipy/sparse/linalg/_isolve/lsqr.py</span>

<span class="sd">    The routine &#39;_sym_ortho&#39; was added for numerical stability. This is</span>
<span class="sd">    recommended by S.-C. Choi in &quot;Iterative Methods for Singular Linear Equations and Least-Squares</span>
<span class="sd">    Problems&quot;.  It removes the unpleasant potential of</span>
<span class="sd">    ``1/eps`` in some important places.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">abs</span><span class="p">()):</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">tau</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">b</span> <span class="o">/</span> <span class="n">s</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="n">b</span> <span class="o">/</span> <span class="n">a</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">tau</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">c</span>
    <span class="k">return</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">r</span>


<div class="viewcode-block" id="lsqr">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.lsqr.html#deepinv.optim.utils.lsqr">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">lsqr</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">AT</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">x0</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">conlim</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e8</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LSQR algorithm for solving linear systems.</span>

<span class="sd">    Code adapted from SciPy&#39;s implementation of LSQR: https://github.com/scipy/scipy/blob/v1.15.1/scipy/sparse/linalg/_isolve/lsqr.py</span>

<span class="sd">    The function solves the linear system :math:`\min_x \|Ax-b\|^2 + \eta \|x-x_0\|^2` in the least squares sense</span>
<span class="sd">    using the LSQR algorithm from</span>

<span class="sd">    Paige, C. C. and M. A. Saunders, &quot;LSQR: An Algorithm for Sparse Linear Equations And Sparse Least Squares,&quot; ACM Trans. Math. Soft., Vol.8, 1982, pp. 43-71.</span>

<span class="sd">    :param Callable A: Linear operator as a callable function.</span>
<span class="sd">    :param Callable AT: Adjoint operator as a callable function.</span>
<span class="sd">    :param torch.Tensor b: input tensor of shape (B, ...)</span>
<span class="sd">    :param float, torch.Tensor eta: damping parameter :math:`eta \geq 0`. Can be batched (shape (B, ...)) or a scalar.</span>
<span class="sd">    :param None, torch.Tensor x0: Optional :math:`x_0`, which is also used as the initial guess.</span>
<span class="sd">    :param float tol: relative tolerance for stopping the LSQR algorithm.</span>
<span class="sd">    :param float conlim: maximum value of the condition number of the system.</span>
<span class="sd">    :param int max_iter: maximum number of LSQR iterations.</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param bool verbose: Output progress information in the console.</span>
<span class="sd">    :retrun: (:class:`torch.Tensor`) :math:`x` of shape (B, ...), (:class:`torch.Tensor`) condition number of the system.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">xt</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parallel_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">device</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">normf</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="p">[[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bi</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">b</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">)):</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
                    <span class="n">u</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">dims</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>  <span class="c1"># don&#39;t keep dim as dims might be different</span>
            <span class="k">return</span> <span class="n">total</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">b_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)):</span>
            <span class="n">b_shape</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)):</span>
                <span class="n">b_shape</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parallel_dim</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)):</span>
            <span class="n">b_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parallel_dim</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">Atb_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xt</span><span class="o">.</span><span class="n">shape</span><span class="p">)):</span>
        <span class="n">Atb_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parallel_dim</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">scalar</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b_domain</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">b_domain</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">TensorList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">vi</span> <span class="o">*</span> <span class="n">alpha</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bi_shape</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">vi</span><span class="p">,</span> <span class="n">bi_shape</span> <span class="ow">in</span> <span class="n">zip_strict</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="n">alpha</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">v</span> <span class="o">*</span> <span class="n">alpha</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">Atb_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">eta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">eta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if batched eta</span>
        <span class="k">if</span> <span class="n">eta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If eta is batched, its batch size must match the one of b.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># ensure eta has ndim as b</span>
            <span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">eta</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Damping parameter eta must be non-negative. LSQR cannot be applied to problems with negative eta.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># this should be safe as eta should be non-negative</span>
    <span class="n">eta_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

    <span class="c1"># ctol = 1 / conlim if conlim &gt; 0 else 0</span>
    <span class="n">anorm</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">acond</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">dampsq</span> <span class="o">=</span> <span class="n">eta</span>
    <span class="n">ddnorm</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="c1"># res2 = 0.0</span>
    <span class="c1"># xnorm = 0.0</span>
    <span class="n">xxnorm</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">z</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">cs2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
    <span class="n">sn2</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">bnorm</span> <span class="o">=</span> <span class="n">normf</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">bnorm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">*</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="n">u</span> <span class="o">-=</span> <span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">normf</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">beta</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">scalar</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">beta</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">normf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alpha</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">scalar</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># v / view(alpha, Atb_shape)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">rhobar</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="n">phibar</span> <span class="o">=</span> <span class="n">beta</span>
    <span class="n">arnorm</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">beta</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">arnorm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">acond</span>

    <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">itn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">-</span> <span class="n">scalar</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">normf</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">beta</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">scalar</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">beta</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">anorm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">anorm</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">dampsq</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">AT</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">-</span> <span class="n">scalar</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">normf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alpha</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">scalar</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">eta</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">rhobar1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rhobar</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">dampsq</span><span class="p">)</span>
            <span class="n">cs1</span> <span class="o">=</span> <span class="n">rhobar</span> <span class="o">/</span> <span class="n">rhobar1</span>
            <span class="n">sn1</span> <span class="o">=</span> <span class="n">eta_sqrt</span> <span class="o">/</span> <span class="n">rhobar1</span>
            <span class="n">psi</span> <span class="o">=</span> <span class="n">sn1</span> <span class="o">*</span> <span class="n">phibar</span>
            <span class="n">phibar</span> <span class="o">=</span> <span class="n">cs1</span> <span class="o">*</span> <span class="n">phibar</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rhobar1</span> <span class="o">=</span> <span class="n">rhobar</span>
            <span class="n">psi</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">cs</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span> <span class="n">rho</span> <span class="o">=</span> <span class="n">_sym_ortho</span><span class="p">(</span><span class="n">rhobar1</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">sn</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="n">rhobar</span> <span class="o">=</span> <span class="o">-</span><span class="n">cs</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">cs</span> <span class="o">*</span> <span class="n">phibar</span>
        <span class="n">phibar</span> <span class="o">=</span> <span class="n">sn</span> <span class="o">*</span> <span class="n">phibar</span>
        <span class="c1"># tau = sn * phi</span>

        <span class="n">t1</span> <span class="o">=</span> <span class="n">phi</span> <span class="o">/</span> <span class="n">rho</span>
        <span class="n">t2</span> <span class="o">=</span> <span class="o">-</span><span class="n">theta</span> <span class="o">/</span> <span class="n">rho</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">scalar</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">scalar</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">v</span> <span class="o">+</span> <span class="n">scalar</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">b_domain</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ddnorm</span> <span class="o">=</span> <span class="n">ddnorm</span> <span class="o">+</span> <span class="n">normf</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="c1"># if calc_var:</span>
        <span class="c1">#    var = var + dk ** 2</span>

        <span class="n">delta</span> <span class="o">=</span> <span class="n">sn2</span> <span class="o">*</span> <span class="n">rho</span>
        <span class="n">gambar</span> <span class="o">=</span> <span class="o">-</span><span class="n">cs2</span> <span class="o">*</span> <span class="n">rho</span>
        <span class="n">rhs</span> <span class="o">=</span> <span class="n">phi</span> <span class="o">-</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">z</span>
        <span class="c1"># zbar = rhs / gambar</span>
        <span class="c1"># xnorm = torch.sqrt(xxnorm + zbar ** 2)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">gambar</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cs2</span> <span class="o">=</span> <span class="n">gambar</span> <span class="o">/</span> <span class="n">gamma</span>
        <span class="n">sn2</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">gamma</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">rhs</span> <span class="o">/</span> <span class="n">gamma</span>
        <span class="n">xxnorm</span> <span class="o">=</span> <span class="n">xxnorm</span> <span class="o">+</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span>

        <span class="n">acond</span> <span class="o">=</span> <span class="n">anorm</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ddnorm</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">rnorm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">phibar</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">psi</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># arnorm = alpha * abs(tau)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">rnorm</span> <span class="o">&lt;=</span> <span class="n">tol</span> <span class="o">*</span> <span class="n">bnorm</span><span class="p">):</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LSQR converged at iteration&quot;</span><span class="p">,</span> <span class="n">itn</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">acond</span> <span class="o">&gt;</span> <span class="n">conlim</span><span class="p">):</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LSQR reached condition number limit </span><span class="si">{</span><span class="n">conlim</span><span class="si">}</span><span class="s2"> at iteration&quot;</span><span class="p">,</span> <span class="n">itn</span><span class="p">)</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">flag</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LSQR did not converge&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">acond</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span></div>



<div class="viewcode-block" id="minres">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.minres.html#deepinv.optim.utils.minres">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">minres</span><span class="p">(</span>
    <span class="n">A</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e2</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">parallel_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">precon</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Minimal Residual Method for solving symmetric equations.</span>

<span class="sd">    Solves :math:`Ax=b` with :math:`A` symmetric using the MINRES algorithm:</span>

<span class="sd">    Christopher C. Paige, Michael A. Saunders (1975). &quot;Solution of sparse indefinite systems of linear equations&quot;. SIAM Journal on Numerical Analysis. 12 (4): 617–629.</span>

<span class="sd">    The method assumes that :math:`A` is hermite.</span>
<span class="sd">    For more details see: https://en.wikipedia.org/wiki/Minimal_residual_method</span>

<span class="sd">    Based on https://github.com/cornellius-gp/linear_operator</span>
<span class="sd">    Modifications and simplifications for compatibility with deepinverse</span>

<span class="sd">    :param Callable A: Linear operator as a callable function.</span>
<span class="sd">    :param torch.Tensor b: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor init: Optional initial guess.</span>
<span class="sd">    :param int max_iter: maximum number of MINRES iterations.</span>
<span class="sd">    :param float tol: absolute tolerance for stopping the MINRES algorithm.</span>
<span class="sd">    :param None, int, list[int] parallel_dim: dimensions to be considered as batch dimensions. If None, all dimensions are considered as batch dimensions.</span>
<span class="sd">    :param bool verbose: Output progress information in the console.</span>
<span class="sd">    :param Callable precon: preconditioner is a callable function (not tested). Must be positive definite</span>
<span class="sd">    :return: (:class:`torch.Tensor`) :math:`x` of shape (B, ...)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parallel_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">parallel_dim</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">parallel_dim</span><span class="p">]</span>

    <span class="c1"># Rescale b</span>
    <span class="n">b_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">b_is_zero</span> <span class="o">=</span> <span class="n">b_norm</span> <span class="o">&lt;</span> <span class="mf">1e-10</span>
    <span class="n">b_norm</span> <span class="o">=</span> <span class="n">b_norm</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">b_is_zero</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">/</span> <span class="n">b_norm</span>

    <span class="c1"># Create space for matmul product, solution</span>
    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="n">init</span> <span class="o">/</span> <span class="n">b_norm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Variables for Lanczos terms</span>
    <span class="n">zvec_prev2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">solution</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># r_(k-1) in wiki</span>
    <span class="n">zvec_prev1</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">A</span><span class="p">(</span><span class="n">solution</span><span class="p">)</span>  <span class="c1"># r_k in wiki</span>
    <span class="n">qvec_prev1</span> <span class="o">=</span> <span class="n">precon</span><span class="p">(</span><span class="n">zvec_prev1</span><span class="p">)</span>
    <span class="n">alpha_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">alpha_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">alpha_curr</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">zvec_prev1</span><span class="p">,</span> <span class="n">qvec_prev1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>

    <span class="c1"># Divide by beta_prev</span>
    <span class="n">zvec_prev1</span> <span class="o">=</span> <span class="n">zvec_prev1</span> <span class="o">/</span> <span class="n">beta_prev</span>
    <span class="n">qvec_prev1</span> <span class="o">=</span> <span class="n">qvec_prev1</span> <span class="o">/</span> <span class="n">beta_prev</span>

    <span class="c1"># Variables for the QR rotation</span>
    <span class="c1"># 1) Components of the Givens rotations</span>
    <span class="n">cos_prev2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">alpha_curr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">sin_prev2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">alpha_curr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cos_prev1</span> <span class="o">=</span> <span class="n">cos_prev2</span>
    <span class="n">sin_prev1</span> <span class="o">=</span> <span class="n">sin_prev2</span>

    <span class="c1"># Variables for the solution updates</span>
    <span class="c1"># 1) The &quot;search&quot; vectors of the solution</span>
    <span class="c1"># Equivalent to the vectors of Q R^{-1}, where Q is the matrix of Lanczos vectors and</span>
    <span class="c1"># R is the QR factor of the tridiagonal Lanczos matrix.</span>
    <span class="n">search_prev2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">solution</span><span class="p">)</span>
    <span class="n">search_prev1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">solution</span><span class="p">)</span>
    <span class="c1"># 2) The &quot;scaling&quot; terms of the search vectors</span>
    <span class="c1"># Equivalent to the terms of V^T Q^T b, where Q is the matrix of Lanczos vectors and</span>
    <span class="c1"># V is the QR orthonormal of the tridiagonal Lanczos matrix.</span>
    <span class="n">scale_prev</span> <span class="o">=</span> <span class="n">beta_prev</span>

    <span class="c1"># Terms for checking for convergence</span>
    <span class="n">solution_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">search_update_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">solution_norm</span><span class="p">)</span>

    <span class="c1"># Perform iterations</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)):</span>
        <span class="c1"># Perform matmul</span>
        <span class="n">prod</span> <span class="o">=</span> <span class="n">A</span><span class="p">(</span><span class="n">qvec_prev1</span><span class="p">)</span>

        <span class="c1"># Get next Lanczos terms</span>
        <span class="c1"># --&gt; alpha_curr, beta_curr, qvec_curr</span>
        <span class="n">alpha_curr</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">qvec_prev1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">prod</span> <span class="o">=</span> <span class="n">prod</span> <span class="o">-</span> <span class="n">alpha_curr</span> <span class="o">*</span> <span class="n">zvec_prev1</span> <span class="o">-</span> <span class="n">beta_prev</span> <span class="o">*</span> <span class="n">zvec_prev2</span>
        <span class="n">qvec_curr</span> <span class="o">=</span> <span class="n">precon</span><span class="p">(</span><span class="n">prod</span><span class="p">)</span>

        <span class="n">beta_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">prod</span><span class="p">,</span> <span class="n">qvec_curr</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>

        <span class="n">prod</span> <span class="o">=</span> <span class="n">prod</span> <span class="o">/</span> <span class="n">beta_curr</span>
        <span class="n">qvec_curr</span> <span class="o">=</span> <span class="n">qvec_curr</span> <span class="o">/</span> <span class="n">beta_curr</span>

        <span class="c1"># Perform JIT-ted update</span>
        <span class="c1">###########################################</span>
        <span class="c1"># Start givens rotation</span>
        <span class="c1"># Givens rotation from 2 steps ago</span>
        <span class="n">subsub_diag_term</span> <span class="o">=</span> <span class="n">sin_prev2</span> <span class="o">*</span> <span class="n">beta_prev</span>
        <span class="n">sub_diag_term</span> <span class="o">=</span> <span class="n">cos_prev2</span> <span class="o">*</span> <span class="n">beta_prev</span>

        <span class="c1"># Givens rotation from 1 step ago</span>
        <span class="n">diag_term</span> <span class="o">=</span> <span class="n">alpha_curr</span> <span class="o">*</span> <span class="n">cos_prev1</span> <span class="o">-</span> <span class="n">sin_prev1</span> <span class="o">*</span> <span class="n">sub_diag_term</span>
        <span class="n">sub_diag_term</span> <span class="o">=</span> <span class="n">sub_diag_term</span> <span class="o">*</span> <span class="n">cos_prev1</span> <span class="o">+</span> <span class="n">sin_prev1</span> <span class="o">*</span> <span class="n">alpha_curr</span>

        <span class="c1"># 3) Compute next Givens terms</span>
        <span class="n">radius_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">diag_term</span> <span class="o">*</span> <span class="n">diag_term</span> <span class="o">+</span> <span class="n">beta_curr</span> <span class="o">*</span> <span class="n">beta_curr</span><span class="p">)</span>
        <span class="n">cos_curr</span> <span class="o">=</span> <span class="n">diag_term</span> <span class="o">/</span> <span class="n">radius_curr</span>
        <span class="n">sin_curr</span> <span class="o">=</span> <span class="n">beta_curr</span> <span class="o">/</span> <span class="n">radius_curr</span>
        <span class="c1"># 4) Apply current Givens rotation</span>
        <span class="n">diag_term</span> <span class="o">=</span> <span class="n">diag_term</span> <span class="o">*</span> <span class="n">cos_curr</span> <span class="o">+</span> <span class="n">sin_curr</span> <span class="o">*</span> <span class="n">beta_curr</span>

        <span class="c1"># Update the solution</span>
        <span class="c1"># --&gt; search_curr, scale_curr solution</span>
        <span class="c1"># 1) Apply the latest Givens rotation to the Lanczos-b ( ||b|| e_1 )</span>
        <span class="c1"># This is getting the scale terms for the &quot;search&quot; vectors</span>
        <span class="n">scale_curr</span> <span class="o">=</span> <span class="o">-</span><span class="n">scale_prev</span> <span class="o">*</span> <span class="n">sin_curr</span>
        <span class="c1"># 2) Get the new search vector</span>
        <span class="n">search_curr</span> <span class="o">=</span> <span class="n">qvec_prev1</span> <span class="o">-</span> <span class="n">sub_diag_term</span> <span class="o">*</span> <span class="n">search_prev1</span>
        <span class="n">search_curr</span> <span class="o">=</span> <span class="p">(</span><span class="n">search_curr</span> <span class="o">-</span> <span class="n">subsub_diag_term</span> <span class="o">*</span> <span class="n">search_prev2</span><span class="p">)</span> <span class="o">/</span> <span class="n">diag_term</span>

        <span class="c1"># 3) Update the solution</span>
        <span class="n">search_update</span> <span class="o">=</span> <span class="n">search_curr</span> <span class="o">*</span> <span class="n">scale_prev</span> <span class="o">*</span> <span class="n">cos_curr</span>
        <span class="n">solution</span> <span class="o">=</span> <span class="n">solution</span> <span class="o">+</span> <span class="n">search_update</span>
        <span class="c1">###########################################</span>

        <span class="c1"># Check convergence criterion</span>
        <span class="n">search_update_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
            <span class="n">search_update</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">solution_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">search_update_norm</span> <span class="o">/</span> <span class="n">solution_norm</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MINRES converged at iteration&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>

        <span class="c1"># Update terms for next iteration</span>
        <span class="c1"># Lanczos terms</span>
        <span class="n">zvec_prev2</span><span class="p">,</span> <span class="n">zvec_prev1</span> <span class="o">=</span> <span class="n">zvec_prev1</span><span class="p">,</span> <span class="n">prod</span>
        <span class="n">qvec_prev1</span> <span class="o">=</span> <span class="n">qvec_curr</span>
        <span class="n">beta_prev</span> <span class="o">=</span> <span class="n">beta_curr</span>
        <span class="c1"># Givens rotations terms</span>
        <span class="n">cos_prev2</span><span class="p">,</span> <span class="n">cos_prev1</span> <span class="o">=</span> <span class="n">cos_prev1</span><span class="p">,</span> <span class="n">cos_curr</span>
        <span class="n">sin_prev2</span><span class="p">,</span> <span class="n">sin_prev1</span> <span class="o">=</span> <span class="n">sin_prev1</span><span class="p">,</span> <span class="n">sin_curr</span>
        <span class="c1"># Search vector terms)</span>
        <span class="n">search_prev2</span><span class="p">,</span> <span class="n">search_prev1</span> <span class="o">=</span> <span class="n">search_prev1</span><span class="p">,</span> <span class="n">search_curr</span>
        <span class="n">scale_prev</span> <span class="o">=</span> <span class="n">scale_curr</span>

    <span class="c1"># For b-s that are close to zero, set them to zero</span>
    <span class="n">solution</span> <span class="o">=</span> <span class="n">solution</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">b_is_zero</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flag</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MINRES did not converge in </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> iterations!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">solution</span> <span class="o">*</span> <span class="n">b_norm</span></div>



<div class="viewcode-block" id="gradient_descent">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.gradient_descent.html#deepinv.optim.utils.gradient_descent">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">grad_f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mf">1e2</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Standard gradient descent algorithm`.</span>

<span class="sd">    :param Callable grad_f: gradient of function to bz minimized as a callable function.</span>
<span class="sd">    :param torch.Tensor x: input tensor.</span>
<span class="sd">    :param torch.Tensor, float step_size: (constant) step size of the gradient descent algorithm.</span>
<span class="sd">    :param int max_iter: maximum number of iterations.</span>
<span class="sd">    :param float tol: absolute tolerance for stopping the algorithm.</span>
<span class="sd">    :return: torch.Tensor :math:`x` minimizing :math:`f(x)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="p">)):</span>
        <span class="n">x_prev</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
        <span class="k">if</span> <span class="n">check_conv</span><span class="p">(</span><span class="n">x_prev</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">thres_conv</span><span class="o">=</span><span class="n">tol</span><span class="p">):</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">x</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">LeastSquaresSolver</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom autograd function for the least squares solver to enable O(1) memory backward propagation using implicit differentiation.</span>

<span class="sd">    The forward pass solves the following problem using :func:`deepinv.optim.utils.least_squares`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \min_x \|A_\theta x - y \|^2 + \frac{1}{\gamma} \|x - z\|^2</span>

<span class="sd">    where :math:`A_\theta` is a linear operator :class:`deepinv.physics.LinearPhysics` parameterized by :math:`\theta`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function uses a :func:`least squares &lt;deepinv.optim.utils.least_squares&gt;` solver under the hood, which supports various solvers such as Conjugate Gradient (CG), BiCGStab, LSQR, and MinRes (see :func:`deepinv.optim.utils.least_squares` for more details).</span>

<span class="sd">    The backward pass computes the gradients with respect to the inputs using implicit differentiation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE: the physics parameters are handled as side-effects (not inputs/outputs of the autograd function)</span>
    <span class="c1"># we add a dummy input tensor `trigger` to trigger backward when needed (i.e. when physics parameters require grad)</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="n">ctx</span><span class="p">,</span>
        <span class="n">physics</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">trigger</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">extra_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">extra_kwargs</span> <span class="k">if</span> <span class="n">extra_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">solution</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
                <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
                <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Save tensors only</span>
        <span class="n">gamma_orig_shape</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># For broadcasting with other tensors. Note we already have checked gamma shapes</span>
        <span class="c1"># in forward, so the following is just for gamma batched but not shaped.</span>
        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
                <span class="n">ndim</span> <span class="o">=</span> <span class="n">solution</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ndim</span> <span class="o">=</span> <span class="n">solution</span><span class="o">.</span><span class="n">ndim</span>

            <span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">solution</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="c1"># Save other non-tensor contexts</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">physics</span> <span class="o">=</span> <span class="n">physics</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span> <span class="o">=</span> <span class="n">gamma_orig_shape</span>

        <span class="k">return</span> <span class="n">solution</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@once_differentiable</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">physics</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">physics</span>

        <span class="c1"># Solve (A^T A + I/gamma) mv = grad_output</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">mv</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span>
                <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
                <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                <span class="n">z</span><span class="o">=</span><span class="n">grad_output</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
                <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
                <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
                <span class="o">**</span><span class="n">ctx</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Build grads aligned with inputs: (physics, y, z, gamma, solver, max_iter, tol, ..., **kwargs)</span>
        <span class="n">needs</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">needs</span><span class="p">)</span>

        <span class="c1"># dL/dy = A mv</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">mv</span><span class="p">)</span>

        <span class="c1"># dL/dz = (1/gamma) mv</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mv</span> <span class="o">/</span> <span class="n">gamma</span>

        <span class="c1"># dL/dgamma = &lt;mv, h - z&gt; / gamma^2</span>
        <span class="k">if</span> <span class="n">needs</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">z</span>
            <span class="c1"># vdot gives correct conjugation for complex; .real keeps real scalar</span>
            <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">mv</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span> <span class="o">*</span> <span class="n">diff</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mv</span><span class="o">.</span><span class="n">ndim</span><span class="p">)),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># If gamma was batched in the forward, we return a batched grad</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">grad_gamma</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">gamma_orig_shape</span><span class="p">)</span>
            <span class="c1"># gamma was a scalar in the forward, we accumulate all grads to return a scalar, similar to</span>
            <span class="c1"># torch autograd behavior for broadcasted inputs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_gamma</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(())</span>
            <span class="n">grads</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_gamma</span>

        <span class="c1"># Optional: implicit grads w.r.t physics parameters (side-effect accumulation)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="s2">&quot;buffers&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[])()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># pseudo-loss = - &lt;mv, A^T (A h - y)&gt;</span>
            <span class="n">h_det</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">y_det</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">pseudo</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span>
                    <span class="n">mv</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">h_det</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_det</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="p">)</span><span class="o">.</span><span class="n">real</span>
            <span class="n">g_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                <span class="n">pseudo</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">zip_strict</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span>
                <span class="n">g_params</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">g</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>


<span class="c1"># wrapper of the autograd function for easier use</span>
<div class="viewcode-block" id="least_squares_implicit_backward">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.least_squares_implicit_backward.html#deepinv.optim.utils.least_squares_implicit_backward">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">least_squares_implicit_backward</span><span class="p">(</span>
    <span class="n">physics</span><span class="p">,</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Least squares solver with O(1) memory backward propagation using implicit differentiation.</span>
<span class="sd">    The function is similar to :func:`deepinv.optim.utils.least_squares` for the forward pass, but uses implicit differentiation for the backward pass, which reduces memory consumption to O(1) in the number of iterations. </span>

<span class="sd">    This function supports backpropagation with respect to the inputs :math:`y`, :math:`z` and :math:`\gamma` and also with respect to the parameters of the physics operator :math:`A_\theta` if they require gradients. See :ref:`sphx_glr_auto_examples_unfolded_demo_unfolded_constant_memory.py` and the notes below for more details. </span>
<span class="sd">    </span>
<span class="sd">    Let :math:`h(z, y, \theta, \gamma)` denote the output of the least squares solver, i.e. the solution of the following problem:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>

<span class="sd">        h(z, y, \theta, \gamma) = \underset{x}{\arg\min} \; \frac{\gamma}{2}\|A_\theta x-y\|^2 + \frac{1}{2}\|x-z\|^2</span>

<span class="sd">    When the forward least-squares solver converges to the exact minimizer, we have the following closed-form expressions for :math:`h(z, y, \theta, \gamma)`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        h(z, y, \theta, \gamma) = \left( A_\theta^{\top} A_\theta + \frac{1}{\gamma} I \right)^{-1} \left( A_\theta^{\top} y + \frac{1}{\gamma} z \right)</span>

<span class="sd">    Let :math:`M` denote the inverse :math:`\left( A_\theta^T A_\theta + \frac{1}{\gamma} I \right)^{-1}`. In the forward, we need to compute the vector-Jacobian products (VJPs), which can be computed as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \left( \frac{\partial h}{\partial z} \right)^{\top} v               &amp;= \frac{1}{\gamma} M v \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial y} \right)^{\top} v               &amp;= A_\theta M v \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial \gamma} \right)^{\top} v          &amp;=   (h - z)^\top M  v / \gamma^2 \\</span>
<span class="sd">        \left( \frac{\partial h}{\partial \theta} \right)^{\top} v          &amp;= \frac{\partial p}{\partial \theta} </span>
<span class="sd">        </span>
<span class="sd">    where :math:`p =  (y - A_\theta h)^{\top} A_\theta M v` and :math:`\frac{\partial p}{\partial \theta}` can be computed using the standard backpropagation mechanism (autograd).</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function only supports first-order gradients. Higher-order gradients are not supported. If you need higher-order gradients, please use :func:`deepinv.optim.utils.least_squares` instead but be aware that it requires storing all intermediate iterates, which can be memory-intensive.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This function also supports implicit gradients with respect to the parameters of the physics operator :math:`A_\theta` if they require gradients. This is useful for learning the physics parameters in an end-to-end fashion. The gradients are accumulated in-place in the `.grad` attribute of the parameters of the physics operator. To make this work, the function takes as input the physics operator itself (not just its matmul functions) and checks if any of its parameters require gradients. If so, it triggers the backward pass accordingly.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        Implicit gradients can be incorrect if the least squares solver does not converge sufficiently. Make sure to set the `max_iter` and `tol` parameters of the least squares solver appropriately to ensure convergence. You can monitor the convergence by setting `verbose=True` in the least squares solver via `kwargs`. If the solver does not converge, the implicit gradients can be very inaccurate and lead to divergence of the training.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        This function does not support :class:`deepinv.utils.TensorList` inputs yet. If you use :class:`deepinv.utils.TensorList` as inputs, the function will fall back to standard least squares with full backpropagation.</span>

<span class="sd">    .. tip::</span>

<span class="sd">        If you do not need gradients with respect to the physics parameters, you can set `requires_grad=False` for all parameters of the physics operator to avoid the additional backward pass. This can save some computation time.</span>

<span class="sd">    .. tip::</span>

<span class="sd">        Training unfolded network with implicit differentiation can reduce memory consumption significantly, especially when using many iterations. On GPU, we can expect a memory reduction factor of about 2x-3x compared to standard backpropagation and a speed-up of about 1.2x-1.5x. The exact numbers depend on the problem and the number of iterations. </span>

<span class="sd">    :param deepinv.physics.LinearPhysics physics: physics operator :class:`deepinv.physics.LinearPhysics`.</span>
<span class="sd">    :param torch.Tensor y: input tensor of shape (B, ...)</span>
<span class="sd">    :param torch.Tensor z: input tensor of shape (B, ...). Default is `None`, which corresponds to a zero tensor.</span>
<span class="sd">    :param None, torch.Tensor init: Optional initial guess, only used for the forward pass. Default is `None`, which corresponds to a zero initialization.</span>
<span class="sd">    :param None, float, torch.Tensor gamma: regularization parameter :math:`\gamma &gt; 0`. Default is `None`. Can be batched (shape (B, ...)) or a scalar.</span>
<span class="sd">    :param kwargs: additional arguments to be passed to the least squares solver.</span>

<span class="sd">    :return: (:class:`torch.Tensor`) :math:`x` of shape (B, ...), the solution of the least squares problem.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># To get correct shape</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="c1"># NOTE: TensorList not supported by autograd function, we fall back to standard least_squares in this case for now</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">TensorList</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Warning: least_squares_implicit_backward does not support TensorList inputs. Falling back to standard least_squares with full backpropagation.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">least_squares</span><span class="p">(</span>
            <span class="n">A</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A</span><span class="p">,</span>
            <span class="n">AT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">AAT</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_A_adjoint</span><span class="p">,</span>
            <span class="n">ATA</span><span class="o">=</span><span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint_A</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">physics_requires_grad_params</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="s2">&quot;buffers&quot;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[])()</span>
    <span class="p">)</span>
    <span class="c1"># NOTE: backward is triggered if any of the inputs require grad</span>
    <span class="c1"># When input tensors (y, z, gamma) do not require grad and we want to do backward w.r.t physics parameters, we need to trigger it manually by a dummy tensor.</span>
    <span class="n">trigger_backward</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="ow">or</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gamma</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">physics_requires_grad_params</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">trigger_backward</span><span class="p">:</span>
        <span class="n">trigger</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span>
            <span class="kc">True</span>
        <span class="p">)</span>  <span class="c1"># Dummy tensor to trigger backward</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trigger</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">extra_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">else</span> <span class="n">y</span><span class="o">.</span><span class="n">real</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gamma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gamma</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If gamma is batched, its batch size must match the one of y.&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">LeastSquaresSolver</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">physics</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">trigger</span><span class="p">,</span> <span class="n">extra_kwargs</span><span class="p">)</span></div>



<div class="viewcode-block" id="GaussianMixtureModel">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GaussianMixtureModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian mixture model including parameter estimation.</span>

<span class="sd">    Implements a Gaussian Mixture Model, its negative log likelihood function and an EM algorithm</span>
<span class="sd">    for parameter estimation.</span>

<span class="sd">    :param int n_components: number of components of the GMM</span>
<span class="sd">    :param int dimension: data dimension</span>
<span class="sd">    :param str device: gpu or cpu.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianMixtureModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_covariance_regularization</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="n">dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_components</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_components</span><span class="p">,</span> <span class="n">dimension</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="mf">0.1</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="mf">0.1</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv_reg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="mf">0.1</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="mf">0.1</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov_reg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span>

<div class="viewcode-block" id="GaussianMixtureModel.set_cov">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.set_cov">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_cov</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the covariance parameters to cov and maintains their log-determinants and inverses</span>

<span class="sd">        :param torch.Tensor cov: new covariance matrices in a n_components x dimension x dimension tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">cov</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_covariance_regularization</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_covariance_regularization</span>
                <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.set_cov_reg">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.set_cov_reg">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_cov_reg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets covariance regularization parameter for evaluating</span>
<span class="sd">        Needed for EPLL.</span>

<span class="sd">        :param float reg: covariance regularization parameter</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_covariance_regularization</span> <span class="o">=</span> <span class="n">reg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_covariance_regularization</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv_reg</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov_reg</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.get_cov">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.get_cov">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_cov</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        get method for covariances</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.get_cov_inv_reg">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.get_cov_inv_reg">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_cov_inv_reg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        get method for covariances</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv_reg</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.set_weights">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.set_weights">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        sets weight parameter while ensuring non-negativity and summation to one</span>

<span class="sd">        :param torch.Tensor weights: non-zero weight tensor of size n_components with non-negative entries</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.get_weights">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.get_weights">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        get method for weights</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.load_state_dict">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.load_state_dict">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override load_state_dict to maintain internal parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_cov</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.component_log_likelihoods">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.component_log_likelihoods">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">component_log_likelihoods</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cov_regularization</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        returns a tensor containing the log likelihood values of x for each component</span>

<span class="sd">        :param torch.Tensor x: input data of shape batch_dimension x dimension</span>
<span class="sd">        :param bool cov_regularization: whether using regularized covariance matrices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cov_regularization</span><span class="p">:</span>
            <span class="n">cov_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv_reg</span>
            <span class="n">logdet_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov_reg</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cov_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cov_inv</span>
            <span class="n">logdet_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logdet_cov</span>
        <span class="n">centered_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">centered_x</span><span class="p">,</span> <span class="n">cov_inv</span><span class="p">)</span> <span class="o">*</span> <span class="n">centered_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="p">(</span>
            <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logdet_cov</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">exponent</span>
            <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimension</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">component_log_likelihoods</span><span class="o">.</span><span class="n">T</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.forward">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        evaluate negative log likelihood function</span>

<span class="sd">        :param torch.Tensor x: input data of shape batch_dimension x dimension</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">component_log_likelihoods</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="n">component_log_likelihoods</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="p">)</span>
        <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">component_log_likelihoods</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">log_likelihoods</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.classify">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.classify">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">classify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cov_regularization</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        returns the index of the most likely component</span>

<span class="sd">        :param torch.Tensor x: input data of shape batch_dimension x dimension</span>
<span class="sd">        :param bool cov_regularization: whether using regularized covariance matrices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">component_log_likelihoods</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">cov_regularization</span><span class="o">=</span><span class="n">cov_regularization</span>
        <span class="p">)</span>
        <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="n">component_log_likelihoods</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="p">)</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">component_log_likelihoods</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ind</span></div>


<div class="viewcode-block" id="GaussianMixtureModel.fit">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.utils.GaussianMixtureModel.html#deepinv.optim.utils.GaussianMixtureModel.fit">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataloader</span><span class="p">,</span>
        <span class="n">max_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">stopping_criterion</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">data_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cov_regularization</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Batched Expectation Maximization algorithm for parameter estimation.</span>


<span class="sd">        :param torch.utils.data.DataLoader dataloader: containing the data</span>
<span class="sd">        :param int max_iters: maximum number of iterations</span>
<span class="sd">        :param float stopping_criterion: stop when objective decrease is smaller than this number.</span>
<span class="sd">            None for performing exactly max_iters iterations</span>
<span class="sd">        :param bool data_init: True for initialize mu by the first data points, False for using current values as initialization</span>
<span class="sd">        :param bool verbose: Output progress information in the console</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">data_init</span><span class="p">:</span>
            <span class="n">first_data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_data</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
                <span class="n">first_data</span> <span class="o">=</span> <span class="n">first_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">first_data</span> <span class="o">=</span> <span class="n">first_data</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">first_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">first_data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># if the first batch does not contain enough data points, fill up the others randomly...</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span> <span class="n">first_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">first_data</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">first_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">first_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:]</span>
                <span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">first_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="n">first_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

        <span class="n">objective</span> <span class="o">=</span> <span class="mf">1e100</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="p">(</span><span class="n">progress_bar</span> <span class="o">:=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">),</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">verbose</span><span class="p">)):</span>
            <span class="n">weights_new</span><span class="p">,</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">cov_new</span><span class="p">,</span> <span class="n">objective_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EM_step</span><span class="p">(</span>
                <span class="n">dataloader</span><span class="p">,</span> <span class="n">verbose</span>
            <span class="p">)</span>
            <span class="c1"># stopping criterion</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights_new</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">mu_new</span>
            <span class="n">cov_new_reg</span> <span class="o">=</span> <span class="n">cov_new</span> <span class="o">+</span> <span class="n">cov_regularization</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimension</span><span class="p">)[</span>
                <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:</span>
            <span class="p">]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cov_new</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_cov</span><span class="p">(</span><span class="n">cov_new_reg</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">stopping_criterion</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">objective</span> <span class="o">-</span> <span class="n">objective_new</span> <span class="o">&lt;</span> <span class="n">stopping_criterion</span><span class="p">:</span>
                    <span class="k">return</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">objective_new</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
                <span class="s2">&quot;Step </span><span class="si">{}</span><span class="s2">, Objective </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">objective</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_EM_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        one step of the EM algorithm</span>

<span class="sd">        :param torch.data.Dataloader dataloader: containing the data</span>
<span class="sd">        :param bool verbose: Output progress information in the console</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">objective</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">weights_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span>
        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">C_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cov</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">objective</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">verbose</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
            <span class="n">n</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">component_log_likelihoods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">component_log_likelihoods</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">log_betas</span> <span class="o">=</span> <span class="n">component_log_likelihoods</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">log_beta_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">log_betas</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_betas</span> <span class="o">=</span> <span class="n">log_betas</span> <span class="o">-</span> <span class="n">log_beta_sum</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">objective</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_beta_sum</span><span class="p">)</span>
            <span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_betas</span><span class="p">)</span>
            <span class="n">weights_new</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">beta_times_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">betas</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">mu_new</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">beta_times_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">C_new</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
                <span class="n">beta_times_x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                <span class="n">x</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="c1"># prevents division by zero if weights_new is zero</span>
        <span class="n">weights_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">weights_new</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weights_new</span><span class="p">))</span>

        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_new</span> <span class="o">/</span> <span class="n">weights_new</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">cov_new</span> <span class="o">=</span> <span class="n">C_new</span> <span class="o">/</span> <span class="n">weights_new</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">mu_new</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">mu_new</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="p">)</span>
        <span class="n">weights_new</span> <span class="o">=</span> <span class="n">weights_new</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span> <span class="o">/</span> <span class="n">n</span>
        <span class="k">return</span> <span class="n">weights_new</span><span class="p">,</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">cov_new</span><span class="p">,</span> <span class="n">objective</span></div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright deepinverse contributors 2025.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>