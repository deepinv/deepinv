
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="robots" content="noindex">

    <title>deepinv.optim.optimizers &#8212; deepinv 0.3.7 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=9112d68a" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=29d04658"></script>
    <script src="../../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=35a8b989"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-NSEKFKYSGR"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-NSEKFKYSGR');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/deepinv/optim/optimizers';</script>
    <link rel="canonical" href="https://deepinv.github.io/deepinv/_modules/deepinv/optim/optimizers.html" />
    <link rel="icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">ðŸŽ‰ We are part of the <a href='https://landscape.pytorch.org/?item=modeling--computer-vision--deepinverse' target='_blank'> official PyTorch ecosystem!</a><br>ðŸ“§ <a href='https://forms.gle/TFyT7M2HAWkJYfvQ7' target='_blank'> Join our mailing list</a> for releases and updates.</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/deepinv_logolarge.png" class="logo__image only-light" alt="deepinv 0.3.7 documentation - Home"/>
    <img src="../../../_static/logo_large_dark.png" class="logo__image only-dark pst-js-only" alt="deepinv 0.3.7 documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finding_help.html">
    Finding Help
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../community.html">
    Community
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../../changelog.html">
    Change Log
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../API.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../finding_help.html">
    Finding Help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../contributing.html">
    Contributing to DeepInverse
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../community.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../changelog.html">
    Change Log
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">deepinv.optim.optimizers</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for deepinv.optim.optimizers</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Iterable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">types</span><span class="w"> </span><span class="kn">import</span> <span class="n">MappingProxyType</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">optim_iterators</span> <span class="k">as</span> <span class="n">_optim_iterators</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.optim_iterators</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">OptimIterator</span><span class="p">,</span>
    <span class="n">ADMMIteration</span><span class="p">,</span>
    <span class="n">PGDIteration</span><span class="p">,</span>
    <span class="n">FISTAIteration</span><span class="p">,</span>
    <span class="n">PMDIteration</span><span class="p">,</span>
    <span class="n">CPIteration</span><span class="p">,</span>
    <span class="n">HQSIteration</span><span class="p">,</span>
    <span class="n">DRSIteration</span><span class="p">,</span>
    <span class="n">GDIteration</span><span class="p">,</span>
    <span class="n">MDIteration</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.fixed_point</span><span class="w"> </span><span class="kn">import</span> <span class="n">FixedPoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.prior</span><span class="w"> </span><span class="kn">import</span> <span class="n">ZeroPrior</span><span class="p">,</span> <span class="n">Prior</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.data_fidelity</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataFidelity</span><span class="p">,</span> <span class="n">ZeroFidelity</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.bregman</span><span class="w"> </span><span class="kn">import</span> <span class="n">Bregman</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Reconstructor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.optim.bregman</span><span class="w"> </span><span class="kn">import</span> <span class="n">BregmanL2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.physics</span><span class="w"> </span><span class="kn">import</span> <span class="n">Physics</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.loss.metric</span><span class="w"> </span><span class="kn">import</span> <span class="n">Metric</span>


<div class="viewcode-block" id="DEQConfig">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.DEQConfig.html#deepinv.optim.DEQConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DEQConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration parameters for Deep Equilibrium models.</span>

<span class="sd">    :param  bool jacobian_free: Whether to use a Jacobian-free backward pass (see :footcite:t:`fung2022jfb`).</span>
<span class="sd">    :param  bool anderson_acceleration_backward: Whether to use Anderson acceleration for solving the backward equilibrium.</span>
<span class="sd">    :param  int history_size_backward: Number of past iterates used in Anderson acceleration for the backward pass.</span>
<span class="sd">    :param  float beta_backward: Momentum coefficient in Anderson acceleration for the backward pass.</span>
<span class="sd">    :param  float eps_backward: Regularization parameter for Anderson acceleration in the backward pass.</span>
<span class="sd">    :param  int max_iter_backward: Maximum number of iterations in the backward equilibrium solver.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">jacobian_free</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">anderson_acceleration_backward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">history_size_backward</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">beta_backward</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">eps_backward</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">max_iter_backward</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span></div>



<div class="viewcode-block" id="AndersonAccelerationConfig">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.AndersonAccelerationConfig.html#deepinv.optim.AndersonAccelerationConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AndersonAccelerationConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration parameters for Anderson acceleration of a fixed-point algorithm.</span>

<span class="sd">    :param  int history_size: Number of past iterates used in Anderson acceleration.</span>
<span class="sd">    :param  float beta: Momentum coefficient in Anderson acceleration.</span>
<span class="sd">    :param  float eps: Regularization parameter for Anderson acceleration.</span>
<span class="sd">    :param  bool full_backprop: Compute backpropagation through all iterates of Anderson acceleration instead of the last iterate only. Default: ``False``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">history_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">full_backprop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></div>



<div class="viewcode-block" id="BacktrackingConfig">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BacktrackingConfig.html#deepinv.optim.BacktrackingConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BacktrackingConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration parameters for backtracking line search on the stepsize.</span>

<span class="sd">    :param  float gamma: Armijo-like parameter (controls sufficient decrease).</span>
<span class="sd">    :param  float eta: Step reduction factor (e.g. multiply step by eta on failure).</span>
<span class="sd">    :param  int max_iter: Maximum number of backtracking steps.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span></div>



<div class="viewcode-block" id="BaseOptim">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">BaseOptim</span><span class="p">(</span><span class="n">Reconstructor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for optimization algorithms, consists in iterating a fixed-point operator.</span>

<span class="sd">    Module solving the problem</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>


<span class="sd">    where the first term :math:`\datafidname:\xset\times\yset \mapsto \mathbb{R}_{+}` enforces data-fidelity, the second</span>
<span class="sd">    term :math:`\regname:\xset\mapsto \mathbb{R}_{+}` acts as a regularization and</span>
<span class="sd">    :math:`\lambda &gt; 0` is a regularization parameter. More precisely, the data-fidelity term penalizes the discrepancy</span>
<span class="sd">    between the data :math:`y` and the forward operator :math:`A` applied to the variable :math:`x`, as</span>

<span class="sd">    .. math::</span>
<span class="sd">        \datafid{x}{y} = \distance{Ax}{y}</span>

<span class="sd">    where :math:`\distance{\cdot}{\cdot}` is a distance function, and where :math:`A:\xset\mapsto \yset` is the forward</span>
<span class="sd">    operator (see :class:`deepinv.physics.Physics`)</span>

<span class="sd">    Optimization algorithms for minimising the problem above can be written as fixed point algorithms,</span>
<span class="sd">    i.e. for :math:`k=1,2,...`</span>

<span class="sd">    .. math::</span>
<span class="sd">        \qquad (x_{k+1}, z_{k+1}) = \operatorname{FixedPoint}(x_k, z_k, f, g, A, y, ...)</span>


<span class="sd">    where :math:`x_k` is a variable converging to the solution of the minimization problem, and</span>
<span class="sd">    :math:`z_k` is an additional &quot;dual&quot; variable that may be required in the computation of the fixed point operator.</span>

<span class="sd">    If the algorithm is minimizing an explicit and fixed cost function :math:`F(x) =  \datafid{x}{y} + \lambda \reg{x}`,</span>
<span class="sd">    the value of the cost function is computed along the iterations and can be used for convergence criterion.</span>
<span class="sd">    Moreover, backtracking can be used to adapt the stepsize at each iteration. Backtracking consists in choosing</span>
<span class="sd">    the largest stepsize :math:`\tau` such that, at each iteration, sufficient decrease of the cost function :math:`F` is achieved.</span>
<span class="sd">    More precisely, Given :math:`\gamma \in (0,1/2)` and :math:`\eta \in (0,1)` and an initial stepsize :math:`\tau &gt; 0`,</span>
<span class="sd">    the following update rule is applied at each iteration :math:`k`:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{ while } F(x_k) - F(x_{k+1}) &lt; \frac{\gamma}{\tau} || x_{k-1} - x_k ||^2, \,\, \text{ do } \tau \leftarrow \eta \tau</span>

<span class="sd">    .. note::</span>
<span class="sd">        To use backtracking, the optimized function (i.e., both the the data-fidelity and prior) must be explicit and provide a computable cost for the current iterate.</span>
<span class="sd">        If the prior is not explicit (e.g. a denoiser) or if the argument ``has_cost`` is set to ``False``, backtracking is automatically disabled.</span>

<span class="sd">    The variable ``params_algo`` is a dictionary containing all the relevant parameters for running the algorithm.</span>
<span class="sd">    If the value associated with the key is a float, the algorithm will use the same parameter across all iterations.</span>
<span class="sd">    If the value is list of length max_iter, the algorithm will use the corresponding parameter at each iteration.</span>

<span class="sd">    By default, the intial iterates are initialized with the adjoint applied to the measurement :math:`A^{\top}y`, when the adjoint is defined, and with the observation :math:`y` if the adjoint is not defined.</span>
<span class="sd">    Custom initialization can be defined with the ``custom_init`` class argument or via ``init`` argument in the ``forward`` method.</span>

<span class="sd">    The variable ``data_fidelity`` is a list of instances of :class:`deepinv.optim.DataFidelity` (or a single instance).</span>
<span class="sd">    If a single instance, the same data-fidelity is used at each iteration. If a list, the data-fidelity can change at each iteration.</span>
<span class="sd">    The same holds for the variable ``prior`` which is a list of instances of :class:`deepinv.optim.Prior` (or a single instance).</span>

<span class="sd">    Setting ``unfold`` to ``True`` enables to turn this iterative optimization algorithm into an unfolded algorithm, i.e. an algorithm</span>
<span class="sd">    that can be trained end-to-end, with learnable parameters. These learnable parameters encompass the trainable parameters of the algorithm which</span>
<span class="sd">    can be chosen with the ``trainable_params`` argument</span>
<span class="sd">    (e.g. ``stepsize`` :math:`\gamma`, regularization parameter ``lambda_reg`` :math:`\lambda`, prior parameter (``g_param`` or ``sigma_denoiser``) :math:`\sigma` ...)</span>
<span class="sd">    but also the trainable priors (e.g. a deep denoiser) or forward models.</span>

<span class="sd">    If ``DEQ`` is set to ``True``, the algorithm is unfolded as a Deep Equilibrium model, i.e. the algorithm is virtually unrolled infinitely, leveraging the implicit function theorem.</span>
<span class="sd">    The backward pass is then performed using fixed point iterations to find solutions of the fixed-point equation</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{equation}</span>
<span class="sd">        v = \left(\frac{\partial \operatorname{FixedPoint}(x^\star)}{\partial x^\star} \right )^{\top} v + u.</span>
<span class="sd">        \end{equation}</span>

<span class="sd">    where :math:`u` is the incoming gradient from the backward pass,</span>
<span class="sd">    and :math:`x^\star` is the equilibrium point of the forward pass. See `this tutorial &lt;http://implicit-layers-tutorial.org/deep_equilibrium_models/&gt;`_ for more details.</span>

<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are tranable by default.</span>

<span class="sd">    .. note::</span>

<span class="sd">        For now DEQ is only possible with PGD, HQS and GD optimization algorithms.</span>
<span class="sd">        If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    .. doctest::</span>

<span class="sd">        &gt;&gt;&gt; import deepinv as dinv</span>
<span class="sd">        &gt;&gt;&gt; # This minimal example shows how to use the BaseOptim class to solve the problem</span>
<span class="sd">        &gt;&gt;&gt; #                min_x 0.5  ||Ax-y||_2^2 + \lambda ||x||_1</span>
<span class="sd">        &gt;&gt;&gt; # with the PGD algorithm, where A is the identity operator, lambda = 1 and y = [2, 2].</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create the measurement operator A</span>
<span class="sd">        &gt;&gt;&gt; A = torch.tensor([[1, 0], [0, 1]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; A_forward = lambda v: A @ v</span>
<span class="sd">        &gt;&gt;&gt; A_adjoint = lambda v: A.transpose(0, 1) @ v</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the physics model associated to this operator</span>
<span class="sd">        &gt;&gt;&gt; physics = dinv.physics.LinearPhysics(A=A_forward, A_adjoint=A_adjoint)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the measurement y</span>
<span class="sd">        &gt;&gt;&gt; y = torch.tensor([2, 2], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the data fidelity term</span>
<span class="sd">        &gt;&gt;&gt; data_fidelity = dinv.optim.data_fidelity.L2()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the prior</span>
<span class="sd">        &gt;&gt;&gt; prior = dinv.optim.Prior(g = lambda x, *args: torch.linalg.vector_norm(x, ord=1, dim=tuple(range(1, x.ndim))))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the parameters of the algorithm</span>
<span class="sd">        &gt;&gt;&gt; params_algo = {&quot;stepsize&quot;: 0.5, &quot;lambda&quot;: 1.0}</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the fixed-point iterator</span>
<span class="sd">        &gt;&gt;&gt; iterator = dinv.optim.optim_iterators.PGDIteration()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Define the optimization algorithm</span>
<span class="sd">        &gt;&gt;&gt; optimalgo = dinv.optim.BaseOptim(iterator,</span>
<span class="sd">        ...                     data_fidelity=data_fidelity,</span>
<span class="sd">        ...                     params_algo=params_algo,</span>
<span class="sd">        ...                     prior=prior)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Run the optimization algorithm</span>
<span class="sd">        &gt;&gt;&gt; with torch.no_grad(): xhat = optimalgo(y, physics)</span>
<span class="sd">        &gt;&gt;&gt; print(xhat)</span>
<span class="sd">        tensor([1., 1.], dtype=torch.float64)</span>


<span class="sd">    :param deepinv.optim.OptimIterator iterator: Fixed-point iterator of the optimization algorithm of interest.</span>
<span class="sd">    :param dict params_algo: dictionary containing all the relevant parameters for running the algorithm,</span>
<span class="sd">        e.g. the stepsize, regularization parameter, denoising standard deviation.</span>
<span class="sd">        Each value of the dictionary can be either Iterable (distinct value for each iteration) or</span>
<span class="sd">        a single float (same value for each iteration).</span>
<span class="sd">        Default: ``{&quot;stepsize&quot;: 1.0, &quot;lambda&quot;: 1.0}``. See :any:`optim-params` for more details.</span>
<span class="sd">    :param list, deepinv.optim.DataFidelity: data-fidelity term.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior: regularization prior.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: 100.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: value of the threshold for claiming convergence. Default: ``1e-05``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm once the convergence criterion is reached. Default: ``True``.</span>
<span class="sd">    :param bool has_cost: whether the algorithm has an explicit cost function or not. Default: `False`.</span>
<span class="sd">        If the prior is not explicit (e.g. a denoiser) ``prior.explicit_prior = False``, then ``has_cost`` is automatically set to ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary containing custom metrics to be computed at each iteration.</span>
<span class="sd">    :param BacktrackingConfig, bool backtracking: configuration for using a backtracking line-search strategy for automatic stepsize adaptation.</span>
<span class="sd">        If ``None`` (default) or ``False``, stepsize backtracking is disabled. Otherwise, ``backtracking`` must be an instance of :class:`deepinv.optim.BacktrackingConfig`, which defines the parameters for backtracking line-search.</span>
<span class="sd">        If ``True``, the default ``BacktrackingConfig`` is used.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>

<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>

<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method.</span>

<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param Callable get_output:  Custom output of the algorithm.</span>
<span class="sd">        The callable function ``get_output(X)`` takes as input the dictionary ``X`` containing the primal and auxiliary variables and returns the desired output. Default : ``X[&#39;est&#39;][0]``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm and make the model parameters trainable. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of the algorithmic parameters to be made trainable (must be chosen among the keys of the dictionary ``params_algo``).</span>
<span class="sd">        Default: ``None``, which means that all parameters in params_algo are trainable. For no trainable parameters, set to an empty list ``[]``.</span>
<span class="sd">    :param DEQConfig, bool DEQ: Configuration for a Deep Equilibrium (DEQ) unfolding strategy.</span>
<span class="sd">        DEQ algorithms are virtually unrolled infinitely, leveraging the implicit function theorem.</span>
<span class="sd">        If ``None`` (default) or ``False``, DEQ is disabled and the algorithm runs a standard finite number of iterations.</span>
<span class="sd">        Otherwise, ``DEQ`` must be an instance of :class:`deepinv.optim.DEQConfig`, which defines the parameters</span>
<span class="sd">        for forward and backward equilibrium-based implicit differentiation.</span>
<span class="sd">        If ``True``, the default ``DEQConfig`` is used.</span>
<span class="sd">    :param AndersonAccelerationConfig, bool anderson_acceleration: Configuration of Anderson acceleration for the fixed-point iterations.</span>
<span class="sd">        If ``None`` (default) or ``False``, Anderson acceleration is disabled.</span>
<span class="sd">        Otherwise, ``anderson_acceleration`` must be an instance of :class:`deepinv.optim.AndersonAccelerationConfig`, which defines the parameters for Anderson acceleration.</span>
<span class="sd">        If ``True``, the default ``AndersonAccelerationConfig`` is used.</span>
<span class="sd">    :param bool verbose: whether to print relevant information of the algorithm during its run,</span>
<span class="sd">        such as convergence criterion at each iterate. Default: ``False``.</span>
<span class="sd">    :param bool show_progress_bar: show progress bar during optimization.</span>
<span class="sd">    :return: a torch model that solves the optimization problem.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">iterator</span><span class="p">:</span> <span class="n">OptimIterator</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingProxyType</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span>
        <span class="p">),</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">has_cost</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">backtracking</span><span class="p">:</span> <span class="n">BacktrackingConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">get_output</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">DEQ</span><span class="p">:</span> <span class="n">DEQConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">anderson_acceleration</span><span class="p">:</span> <span class="n">AndersonAccelerationConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">early_stop</span> <span class="o">=</span> <span class="n">early_stop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crit_conv</span> <span class="o">=</span> <span class="n">crit_conv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">show_progress_bar</span> <span class="o">=</span> <span class="n">show_progress_bar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backtracking</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span> <span class="o">=</span> <span class="n">backtracking</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking_config</span> <span class="o">=</span> <span class="n">BacktrackingConfig</span><span class="p">()</span> <span class="k">if</span> <span class="n">backtracking</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span> <span class="o">=</span> <span class="n">backtracking</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking_config</span> <span class="o">=</span> <span class="n">backtracking</span> <span class="ow">or</span> <span class="n">BacktrackingConfig</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_converged</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thres_conv</span> <span class="o">=</span> <span class="n">thres_conv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_metrics</span> <span class="o">=</span> <span class="n">custom_metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_init</span> <span class="o">=</span> <span class="n">custom_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span> <span class="o">=</span> <span class="n">get_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span> <span class="o">=</span> <span class="n">has_cost</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unfold</span> <span class="o">=</span> <span class="n">unfold</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">DEQ</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">DEQ</span> <span class="o">=</span> <span class="n">DEQ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span> <span class="o">=</span> <span class="n">DEQConfig</span><span class="p">()</span> <span class="k">if</span> <span class="n">DEQ</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">DEQ</span> <span class="o">=</span> <span class="n">DEQ</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span> <span class="o">=</span> <span class="n">DEQ</span> <span class="ow">or</span> <span class="n">DEQConfig</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">anderson_acceleration</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">anderson_acceleration_config</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">AndersonAccelerationConfig</span><span class="p">()</span> <span class="k">if</span> <span class="n">anderson_acceleration</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">anderson_acceleration_config</span> <span class="o">=</span> <span class="n">anderson_acceleration</span>

        <span class="c1"># By default, ``self.prior`` should be a list of elements of the class :meth:`deepinv.optim.Prior`. The user could want the prior to change at each iteration. If no prior is given, we set it to a zero prior.</span>
        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="n">ZeroPrior</span><span class="p">()]</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="n">prior</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>

        <span class="c1"># By default, ``self.data_fidelity`` should be a list of elements of the class :meth:`deepinv.optim.DataFidelity`. The user could want the data-fidelity to change at each iteration.</span>
        <span class="k">if</span> <span class="n">data_fidelity</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span> <span class="o">=</span> <span class="p">[</span><span class="n">ZeroFidelity</span><span class="p">()]</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_fidelity</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_fidelity</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span> <span class="o">=</span> <span class="n">data_fidelity</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">explicit_prior</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">explicit_prior</span>
        <span class="p">)</span>
        <span class="n">iterator</span><span class="o">.</span><span class="n">has_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params_algo</span><span class="p">,</span> <span class="n">MappingProxyType</span><span class="p">):</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># By default ``params_algo`` should contain a prior parameter named ``g_param`` or ``sigma_denoiser``, set by default to ``None``.</span>
        <span class="k">if</span> <span class="s2">&quot;g_param&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;sigma_denoiser&quot;</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;g_param&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sigma_denoiser&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;g_param&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># By default ``params_algo`` should contain a regularization parameter ``lambda`` parameter, which multiplies the prior term ``g``. It is set by default to ``1``.</span>
        <span class="k">if</span> <span class="s2">&quot;lambda&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;lambda_reg&quot;</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lambda_reg&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1"># By default ``params_algo`` should contain a relaxation ``beta`` parameter, set by default to 1..</span>
        <span class="k">if</span> <span class="s2">&quot;beta&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1"># By default, each parameter in ``params_algo` is a list.</span>
        <span class="c1"># If given as a single number, we convert it to a list of 1 element.</span>
        <span class="c1"># If given as a list of more than 1 element, it should have lenght ``max_iter``.</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
                <span class="n">params_algo</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_algo</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_algo</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The number of elements in the parameter </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is inferior to max_iter.&quot;</span>
                    <span class="p">)</span>
        <span class="c1"># If ``stepsize`` is a list of more than 1 element, backtracking is impossible.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="s2">&quot;stepsize&quot;</span> <span class="ow">in</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;stepsize&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Backtracking impossible when stepsize is predefined as a list. Setting backtracking to False.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># If no cost function, backtracking is impossible.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Backtracking impossible when no cost function is given. Setting backtracking to False.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># keep track of initial parameters in case they are changed during optimization (e.g. backtracking)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span> <span class="o">=</span> <span class="n">params_algo</span>

        <span class="c1"># set trainable parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unfold</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">trainable_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;lambda_reg&quot;</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
                    <span class="n">trainable_params</span><span class="p">[</span><span class="n">trainable_params</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;lambda_reg&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="s2">&quot;lambda&quot;</span>
                <span class="k">if</span> <span class="s2">&quot;sigma_denoiser&quot;</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
                    <span class="n">trainable_params</span><span class="p">[</span><span class="n">trainable_params</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;sigma_denoiser&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;g_param&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">trainable_params</span> <span class="o">=</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">param_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span><span class="p">[</span><span class="n">param_key</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span><span class="p">[</span><span class="n">param_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="p">(</span>
                                <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">el</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">el</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                                <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">el</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
                            <span class="p">)</span>
                            <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">param_value</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="c1"># The prior (list of instances of :class:`deepinv.optim.Prior`), data_fidelity and bremgna_potentials are converted to a `nn.ModuleList` to be trainable.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># Initialize the fixed-point module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_point</span> <span class="o">=</span> <span class="n">FixedPoint</span><span class="p">(</span>
            <span class="n">iterator</span><span class="o">=</span><span class="n">iterator</span><span class="p">,</span>
            <span class="n">update_params_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_params_fn</span><span class="p">,</span>
            <span class="n">update_data_fidelity_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_data_fidelity_fn</span><span class="p">,</span>
            <span class="n">update_prior_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_prior_fn</span><span class="p">,</span>
            <span class="n">backtracking_check_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backtracking_check_fn</span><span class="p">,</span>
            <span class="n">check_conv_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">check_conv_fn</span><span class="p">,</span>
            <span class="n">init_metrics_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">init_metrics_fn</span><span class="p">,</span>
            <span class="n">init_iterate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">init_iterate_fn</span><span class="p">,</span>
            <span class="n">update_metrics_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">update_metrics_fn</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">anderson_acceleration_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">anderson_acceleration_config</span><span class="p">,</span>
            <span class="n">backtracking_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backtracking_config</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">show_progress_bar</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">show_progress_bar</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">deepinv.loss.metric.distortion</span><span class="w"> </span><span class="kn">import</span> <span class="n">PSNR</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">psnr</span> <span class="o">=</span> <span class="n">PSNR</span><span class="p">()</span>

<div class="viewcode-block" id="BaseOptim.update_params_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.update_params_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_params_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">it</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For each parameter ``params_algo``, selects the parameter value for iteration ``it``</span>
<span class="sd">        (if this parameter depends on the iteration number).</span>

<span class="sd">        :param int it: iteration number.</span>
<span class="sd">        :return: a dictionary containing the parameters at iteration ``it``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cur_params_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">cur_params_dict</span></div>


<div class="viewcode-block" id="BaseOptim.update_prior_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.update_prior_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_prior_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">it</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Prior</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For each prior function in `prior`, selects the prior value for iteration ``it``</span>
<span class="sd">        (if this prior depends on the iteration number).</span>

<span class="sd">        :param int it: iteration number.</span>
<span class="sd">        :return: the prior at iteration ``it``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cur_prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">cur_prior</span></div>


<div class="viewcode-block" id="BaseOptim.update_data_fidelity_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.update_data_fidelity_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_data_fidelity_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">it</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFidelity</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For each data_fidelity function in `data_fidelity`, selects the data_fidelity value for iteration ``it``</span>
<span class="sd">        (if this data_fidelity depends on the iteration number).</span>

<span class="sd">        :param int it: iteration number.</span>
<span class="sd">        :return: the data_fidelity at iteration ``it``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cur_data_fidelity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span><span class="p">[</span><span class="n">it</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_fidelity</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">cur_data_fidelity</span></div>


<div class="viewcode-block" id="BaseOptim.init_iterate_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.init_iterate_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_iterate_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">physics</span><span class="p">:</span> <span class="n">Physics</span><span class="p">,</span>
        <span class="n">init</span><span class="p">:</span> <span class="p">(</span>
            <span class="n">Callable</span><span class="p">[</span>
                <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span>
            <span class="p">]</span>
            <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
            <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="o">|</span> <span class="nb">dict</span>
        <span class="p">)</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the iterate of the algorithm.</span>
<span class="sd">        The first iterate is stored in a dictionary of the form ``X = {&#39;est&#39;: (x_0, u_0), &#39;cost&#39;: F_0}`` where:</span>

<span class="sd">            * ``est`` is a tuple containing the first primal and auxiliary iterates.</span>
<span class="sd">            * ``cost`` is the value of the cost function at the first iterate.</span>

<span class="sd">        By default, the first (primal and dual) iterate of the algorithm is chosen as :math:`A^{\top}y` when the adjoint is defined, and with the observation `y` if the adjoint is not defined.</span>
<span class="sd">        A custom initialization is possible via the ``custom_init`` class argument or via the ``init`` argument.</span>

<span class="sd">        :param torch.Tensor y: measurement vector.</span>
<span class="sd">        :param deepinv.physics: physics of the problem.</span>
<span class="sd">        :param Callable, torch.Tensor, tuple, dict init:  initialization of the algorithm.</span>
<span class="sd">            Either a Callable function of the form ``init(y, physics)`` or a fixed torch.Tensor initialization.</span>
<span class="sd">            The output of the function or the fixed initialization can be either:</span>

<span class="sd">            - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">            - a :class:`torch.Tensor` :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">            - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>

<span class="sd">        :param Callable cost_fn:  function that computes the cost function.</span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input</span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`),</span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">        :return: a dictionary containing the first iterate of the algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># reset params to initial values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_params_algo</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">init</span> <span class="o">=</span> <span class="n">init</span> <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_init</span>
        <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">init</span><span class="p">):</span>
                <span class="n">init</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">init_X</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">init</span><span class="p">,)}</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">init_X</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="n">init</span><span class="p">}</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">init_X</span> <span class="o">=</span> <span class="n">init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Custom initial iterate must be a torch.Tensor, a tuple, or a dict. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">custom_init</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_init</span><span class="p">,</span> <span class="n">z_init</span> <span class="o">=</span> <span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">init_X</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">z_init</span><span class="p">)}</span>
        <span class="n">F</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">cost_fn</span><span class="p">(</span>
                <span class="n">init_X</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_data_fidelity_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_prior_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_params_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">physics</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span> <span class="ow">and</span> <span class="n">cost_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">init_X</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span>
        <span class="k">return</span> <span class="n">init_X</span></div>


<div class="viewcode-block" id="BaseOptim.init_metrics_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.init_metrics_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_metrics_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X_init</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">x_gt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the metrics.</span>

<span class="sd">        Metrics are computed for each batch and for each iteration.</span>
<span class="sd">        They are represented by a list of list, and ``metrics[metric_name][i,j]`` contains the metric ``metric_name``</span>
<span class="sd">        computed for batch i, at iteration j.</span>

<span class="sd">        :param dict X_init: dictionary containing the primal and auxiliary initial iterates.</span>
<span class="sd">        :param torch.Tensor x_gt: ground truth image, required for PSNR computation. Default: ``None``.</span>
<span class="sd">        :return dict: A dictionary containing the metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">x_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X_init</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">x_init</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">x_gt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">psnr</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">psnr</span><span class="p">(</span><span class="n">x_init</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">x_gt</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">psnr</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="n">init</span><span class="p">[</span><span class="s2">&quot;psnr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">psnr</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span><span class="p">:</span>
            <span class="n">init</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="n">init</span><span class="p">[</span><span class="s2">&quot;residual&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">custom_metric_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">init</span><span class="p">[</span><span class="n">custom_metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">init</span></div>


<div class="viewcode-block" id="BaseOptim.update_metrics_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.update_metrics_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_metrics_fn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span>
        <span class="n">X_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">X</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">x_gt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that compute all the metrics, across all batches, for the current iteration.</span>

<span class="sd">        :param dict metrics: dictionary containing the metrics. Each metric is computed for each batch.</span>
<span class="sd">        :param dict X_prev: dictionary containing the primal and dual previous iterates.</span>
<span class="sd">        :param dict X: dictionary containing the current primal and dual iterates.</span>
<span class="sd">        :param torch.Tensor x_gt: ground truth image, required for PSNR computation. Default: None.</span>
<span class="sd">        :return dict: a dictionary containing the updated metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X_prev</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="n">residual</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="p">((</span><span class="n">x_prev</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-06</span><span class="p">))</span>
                    <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                    <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                    <span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;residual&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">x_gt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">psnr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">psnr</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">x_gt</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;psnr&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">psnr</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span><span class="p">:</span>
                    <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">for</span> <span class="p">(</span>
                        <span class="n">custom_metric_name</span><span class="p">,</span>
                        <span class="n">custom_metric_fn</span><span class="p">,</span>
                    <span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="n">metrics</span><span class="p">[</span><span class="n">custom_metric_name</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">custom_metric_fn</span><span class="p">(</span>
                                <span class="n">metrics</span><span class="p">[</span><span class="n">custom_metric_name</span><span class="p">],</span> <span class="n">x_prev</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span></div>


<div class="viewcode-block" id="BaseOptim.backtracking_check_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.backtracking_check_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">backtracking_check_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs stepsize backtracking if the sufficient decrease condition is not verified.</span>

<span class="sd">        :param dict X_prev: dictionary containing the primal and dual previous iterates.</span>
<span class="sd">        :param dict X: dictionary containing the current primal and dual iterates.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">backtracking</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_cost</span> <span class="ow">and</span> <span class="n">X_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x_prev</span> <span class="o">=</span> <span class="n">X_prev</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">x_prev</span> <span class="o">=</span> <span class="n">x_prev</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">F_prev</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="n">X_prev</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
            <span class="n">diff_F</span><span class="p">,</span> <span class="n">diff_x</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">F_prev</span> <span class="o">-</span> <span class="n">F</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_prev</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="n">stepsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;stepsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">diff_F</span> <span class="o">&lt;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backtracking_config</span><span class="o">.</span><span class="n">gamma</span> <span class="o">/</span> <span class="n">stepsize</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff_x</span><span class="p">:</span>
                <span class="n">backtracking_check</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;stepsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">backtracking_config</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">stepsize</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;Backtracking : new stepsize = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">params_algo</span><span class="p">[</span><span class="s2">&quot;stepsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">&#39;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">backtracking_check</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">backtracking_check</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="BaseOptim.check_conv_fn">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.check_conv_fn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">check_conv_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">it</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">X_prev</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks the convergence of the algorithm.</span>

<span class="sd">        :param int it: iteration number.</span>
<span class="sd">        :param dict X_prev: dictionary containing the primal and dual previous iterates.</span>
<span class="sd">        :param dict X: dictionary containing the current primal and dual iterates.</span>
<span class="sd">        :return bool: ``True`` if the algorithm has converged, ``False`` otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crit_conv</span> <span class="o">==</span> <span class="s2">&quot;residual&quot;</span><span class="p">:</span>
            <span class="n">x_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X_prev</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">x_prev</span> <span class="o">=</span> <span class="n">x_prev</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">crit_cur</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">x_prev</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-06</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">crit_conv</span> <span class="o">==</span> <span class="s2">&quot;cost&quot;</span><span class="p">:</span>
            <span class="n">F_prev</span> <span class="o">=</span> <span class="n">X_prev</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
            <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
            <span class="n">crit_cur</span> <span class="o">=</span> <span class="p">((</span><span class="n">F_prev</span> <span class="o">-</span> <span class="n">F</span><span class="p">)</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-06</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;convergence criteria not implemented&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">crit_cur</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">thres_conv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">has_converged</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s2">, current converge crit. = </span><span class="si">{</span><span class="n">crit_cur</span><span class="si">:</span><span class="s2">.2E</span><span class="si">}</span><span class="s2">, objective = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">thres_conv</span><span class="si">:</span><span class="s2">.2E</span><span class="si">}</span><span class="s2"> </span><span class="se">\r</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="BaseOptim.DEQ_additional_step">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.DEQ_additional_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">DEQ_additional_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">physics</span><span class="p">:</span> <span class="n">Physics</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For Deep Equilibrium models, performs an additional step at the equilibrium point</span>
<span class="sd">        to compute the gradient of the fixed point operator with respect to the input.</span>

<span class="sd">        :param dict X: dictionary defining the current update at the equilibrium point.</span>
<span class="sd">        :param torch.Tensor y: measurement vector.</span>
<span class="sd">        :param deepinv.physics.Physics physics: physics of the problem for the acquisition of ``y``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Once, at the equilibrium point, performs one additional iteration with gradient tracking.</span>
        <span class="n">cur_data_fidelity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_data_fidelity_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_data_fidelity_fn</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">cur_prior</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_prior_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_prior_fn</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">cur_params</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_params_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_params_fn</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_point</span><span class="o">.</span><span class="n">iterator</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">cur_data_fidelity</span><span class="p">,</span> <span class="n">cur_prior</span><span class="p">,</span> <span class="n">cur_params</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">jacobian_free</span><span class="p">:</span>
            <span class="c1"># Another iteration for jacobian computation via automatic differentiation.</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
            <span class="n">f0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_point</span><span class="o">.</span><span class="n">iterator</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">x0</span><span class="p">,)},</span>
                <span class="n">cur_data_fidelity</span><span class="p">,</span>
                <span class="n">cur_prior</span><span class="p">,</span>
                <span class="n">cur_params</span><span class="p">,</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">physics</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Add a backwards hook that takes the incoming backward gradient `X[&quot;est&quot;][0]` and solves the fixed point equation</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">backward_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
                <span class="k">class</span><span class="w"> </span><span class="nc">backward_iterator</span><span class="p">(</span><span class="n">OptimIterator</span><span class="p">):</span>
                    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

                    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                        <span class="k">return</span> <span class="p">{</span>
                            <span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span>
                                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                                    <span class="n">f0</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
                                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                                <span class="o">+</span> <span class="n">grad</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">}</span>

                <span class="c1"># Use the :class:`deepinv.optim.fixed_point.FixedPoint` class to solve the fixed point equation</span>
                <span class="k">def</span><span class="w"> </span><span class="nf">init_iterate_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
                    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">grad</span><span class="p">,)}</span>  <span class="c1"># initialize the fixed point algorithm.</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">anderson_acceleration_backward</span><span class="p">:</span>
                    <span class="n">anderson_acceleration_config</span> <span class="o">=</span> <span class="n">AndersonAccelerationConfig</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">history_size_backward</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">beta_backward</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">eps_backward</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">anderson_acceleration_config</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="n">backward_FP</span> <span class="o">=</span> <span class="n">FixedPoint</span><span class="p">(</span>
                    <span class="n">backward_iterator</span><span class="p">(),</span>
                    <span class="n">init_iterate_fn</span><span class="o">=</span><span class="n">init_iterate_fn</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">DEQ_config</span><span class="o">.</span><span class="n">max_iter_backward</span><span class="p">,</span>
                    <span class="n">check_conv_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">check_conv_fn</span><span class="p">,</span>
                    <span class="n">anderson_acceleration_config</span><span class="o">=</span><span class="n">anderson_acceleration_config</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">g</span> <span class="o">=</span> <span class="n">backward_FP</span><span class="p">({</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">grad</span><span class="p">,)},</span> <span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;est&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">g</span>

            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">x</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">backward_hook</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="BaseOptim.forward">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.BaseOptim.html#deepinv.optim.BaseOptim.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">physics</span><span class="p">:</span> <span class="n">Physics</span><span class="p">,</span>
        <span class="n">init</span><span class="p">:</span> <span class="p">(</span>
            <span class="n">Callable</span><span class="p">[</span>
                <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">dict</span>
            <span class="p">]</span>
            <span class="o">|</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
            <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="o">|</span> <span class="nb">dict</span>
        <span class="p">)</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">x_gt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">compute_metrics</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the fixed-point iteration algorithm for solving :ref:`(1) &lt;optim&gt;`.</span>

<span class="sd">        :param torch.Tensor y: measurement vector.</span>
<span class="sd">        :param deepinv.physics.Physics physics: physics of the problem for the acquisition of ``y``.</span>
<span class="sd">        :param Callable, torch.Tensor, tuple, dict init:  initialization of the algorithm. Default: ``None``.</span>
<span class="sd">            if ``None`` (and the class ``custom_init``argument is ``None``), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined, and with the observation `y` if the adjoint is not defined.</span>
<span class="sd">            ``init`` can be either a fixed initialization or a Callable function of the form ``init(y, physics)`` that takes as input</span>
<span class="sd">            the measurement :math:`y` and the physics ``physics``. The output of the function or the fixed initialization can be either:</span>

<span class="sd">            - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">            - a :class:`torch.Tensor` :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">            - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>

<span class="sd">            Note that custom initialization can also be defined via the ``custom_init`` class argument.</span>
<span class="sd">        :param torch.Tensor x_gt: (optional) ground truth image, for plotting the PSNR across optim iterations.</span>
<span class="sd">        :param bool compute_metrics: whether to compute the metrics or not. Default: ``False``.</span>
<span class="sd">        :param kwargs: optional keyword arguments for the optimization iterator (see :class:`deepinv.optim.OptimIterator`)</span>
<span class="sd">        :return: If ``compute_metrics`` is ``False``,  returns (:class:`torch.Tensor`) the output of the algorithm. Else, returns (torch.Tensor, dict) the output of the algorithm and the metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_context</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">unfold</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">train_context</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_point</span><span class="p">(</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">physics</span><span class="p">,</span>
                <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
                <span class="n">x_gt</span><span class="o">=</span><span class="n">x_gt</span><span class="p">,</span>
                <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEQ_additional_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compute_metrics</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">metrics</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span></div>
</div>



<span class="k">def</span><span class="w"> </span><span class="nf">create_iterator</span><span class="p">(</span>
    <span class="n">iteration</span><span class="p">:</span> <span class="n">OptimIterator</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">DataFidelity</span><span class="p">,</span> <span class="n">Prior</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">bregman_potential</span><span class="p">:</span> <span class="n">Bregman</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OptimIterator</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for creating an iterator, instance of the :class:`deepinv.optim.OptimIterator` class,</span>
<span class="sd">    corresponding to the chosen minimization algorithm.</span>

<span class="sd">    :param str, deepinv.optim.OptimIterator iteration: either the name of the algorithm to be used,</span>
<span class="sd">        or directly an optim iterator.</span>
<span class="sd">        If an algorithm name (string), should be either ``&quot;GD&quot;`` (gradient descent), ``&quot;PGD&quot;`` (proximal gradient descent), ``&quot;ADMM&quot;`` (ADMM),</span>
<span class="sd">        ``&quot;HQS&quot;`` (half-quadratic splitting), ``&quot;PDCP&quot;`` (Primal-Dual Chambolle-Pock),  ``&quot;DRS&quot;`` (Douglas Rachford), ``&quot;MD&quot;`` (Mirror Descent) or ``&quot;PMD&quot;`` (Proximal Mirror Descent).</span>
<span class="sd">    :param list, deepinv.optim.Prior: regularization prior.</span>
<span class="sd">                            Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">                            deepinv.optim.Prior (distinct prior for each iteration). Default: ``None``.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.</span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input</span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`),</span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the step on :math:`g` before that on :math:`f` before or not. Default: False</span>
<span class="sd">    :param deepinv.optim.Bregman bregman_potential: Bregman potential used for Bregman optimization algorithms such as Mirror Descent. Default: ``None``, uses standard Euclidean optimization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;F_fn&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">F_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;F_fn&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`F_fn` is deprecated and will be removed in a future release. &quot;</span>
            <span class="s2">&quot;Use `cost_fn` instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">cost_fn</span> <span class="o">=</span> <span class="n">F_fn</span>
    <span class="c1"># If no prior is given, we set it to a zero prior.</span>
    <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prior</span> <span class="o">=</span> <span class="n">ZeroPrior</span><span class="p">()</span>
    <span class="c1"># If no custom objective function cost_fn is given but g is explicitly given, we have an explicit objective function.</span>
    <span class="n">explicit_prior</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">prior</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">explicit_prior</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">prior</span><span class="o">.</span><span class="n">explicit_prior</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">cost_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">explicit_prior</span><span class="p">:</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">cost_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">data_fidelity</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">cur_params</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">):</span>
            <span class="n">prior_value</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cur_params</span><span class="p">[</span><span class="s2">&quot;g_param&quot;</span><span class="p">],</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prior_value</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">reg_value</span> <span class="o">=</span> <span class="n">cur_params</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">prior_value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cur_params</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">],</span> <span class="nb">float</span><span class="p">):</span>
                    <span class="n">reg_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_params</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">prior_value</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">reg_value</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">cur_params</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">prior_value</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        <span class="o">*</span> <span class="n">prior_value</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">data_fidelity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg_value</span>

        <span class="n">has_cost</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># boolean to indicate if there is a cost function to evaluate along the iterations</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">has_cost</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Create an instance of :class:`deepinv.optim.OptimIterator`.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">iteration</span><span class="p">,</span> <span class="nb">str</span>
    <span class="p">):</span>  <span class="c1"># If the name of the algorithm is given as a string, the correspondong class is automatically called.</span>
        <span class="n">iterator_fn</span> <span class="o">=</span> <span class="n">str_to_class</span><span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">iterator_fn</span><span class="p">(</span>
            <span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span>
            <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">,</span>
            <span class="n">has_cost</span><span class="o">=</span><span class="n">has_cost</span><span class="p">,</span>
            <span class="n">bregman_potential</span><span class="o">=</span><span class="n">bregman_potential</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If the iteration is directly given as an instance of OptimIterator, nothing to do</span>
        <span class="k">return</span> <span class="n">iteration</span>


<div class="viewcode-block" id="optim_builder">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.optim_builder.html#deepinv.optim.optim_builder">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">optim_builder</span><span class="p">(</span>
    <span class="n">iteration</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">OptimIterator</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">]</span> <span class="o">=</span> <span class="n">MappingProxyType</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>
    <span class="p">),</span>
    <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">DataFidelity</span><span class="p">,</span>
            <span class="n">Prior</span><span class="p">,</span>
            <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Iterable</span><span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">Physics</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">bregman_potential</span><span class="p">:</span> <span class="n">Bregman</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseOptim</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Helper function for building an instance of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    .. note::</span>

<span class="sd">         Since 0.3.6, instead of using this function, it is possible to define optimization algorithms using directly the algorithm name e.g.</span>
<span class="sd">         ``model = PGD(data_fidelity, prior, ...)``.</span>

<span class="sd">     :param str, deepinv.optim.OptimIterator iteration: either the name of the algorithm to be used, or directly an optim iterator.</span>
<span class="sd">         If an algorithm name (string), should be either ``&quot;GD&quot;`` (gradient descent), ``&quot;PGD&quot;`` (proximal gradient descent), ``&quot;ADMM&quot;`` (ADMM),</span>
<span class="sd">         ``&quot;HQS&quot;`` (half-quadratic splitting), ``&quot;PDCP&quot;`` (Primal-Dual Chambolle-Pock),  ``&quot;DRS&quot;`` (Douglas Rachford), ``&quot;MD&quot;`` (Mirror Descent) or ``&quot;PMD&quot;`` (Proximal Mirror Descent).</span>
<span class="sd">     :param int max_iter: maximum number of iterations of the optimization algorithm. Default: 100.</span>
<span class="sd">     :param dict params_algo: dictionary containing all the relevant parameters for running the algorithm,</span>
<span class="sd">                             e.g. the stepsize, regularization parameter, denoising standard deviation.</span>
<span class="sd">                             Each value of the dictionary can be either Iterable (distinct value for each iteration) or</span>
<span class="sd">                             a single float (same value for each iteration). See :any:`optim-params` for more details.</span>
<span class="sd">                             Default: ``{&quot;stepsize&quot;: 1.0, &quot;lambda&quot;: 1.0}``.</span>
<span class="sd">     :param list, deepinv.optim.DataFidelity: data-fidelity term.</span>
<span class="sd">                             Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">                             :class:`deepinv.optim.DataFidelity` (distinct data-fidelity for each iteration). Default: ``None``.</span>
<span class="sd">     :param list, deepinv.optim.Prior prior: regularization prior.</span>
<span class="sd">                             Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">                             deepinv.optim.Prior (distinct prior for each iteration). Default: ``None``.</span>
<span class="sd">     :param Callable cost_fn: Custom user input cost function.</span>
<span class="sd">             ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input</span>
<span class="sd">             the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`),</span>
<span class="sd">             the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">             Default: ``None``.</span>
<span class="sd">     :param bool g_first: whether to perform the step on :math:`g` before that on :math:`f` before or not. Default: `False`</span>
<span class="sd">     :param deepinv.optim.Bregman bregman_potential: Bregman potential used for Bregman optimization algorithms such as Mirror Descent. Default: ``None``, uses standard Euclidean optimization.</span>
<span class="sd">     :param kwargs: additional arguments to be passed to the :class:`deepinv.optim.BaseOptim` class.</span>
<span class="sd">     :return: an instance of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;F_fn&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">F_fn</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;F_fn&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`F_fn` is deprecated and will be removed in a future release. &quot;</span>
            <span class="s2">&quot;Use `cost_fn` instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">cost_fn</span> <span class="o">=</span> <span class="n">F_fn</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params_algo</span><span class="p">,</span> <span class="n">MappingProxyType</span><span class="p">):</span>
        <span class="n">params_algo</span> <span class="o">=</span> <span class="n">params_algo</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="n">iterator</span> <span class="o">=</span> <span class="n">create_iterator</span><span class="p">(</span>
        <span class="n">iteration</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">,</span>
        <span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span>
        <span class="n">bregman_potential</span><span class="o">=</span><span class="n">bregman_potential</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">BaseOptim</span><span class="p">(</span>
        <span class="n">iterator</span><span class="p">,</span>
        <span class="n">has_cost</span><span class="o">=</span><span class="n">iterator</span><span class="o">.</span><span class="n">has_cost</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">str_to_class</span><span class="p">(</span><span class="n">classname</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_optim_iterators</span><span class="p">,</span> <span class="n">classname</span><span class="p">)</span>


<div class="viewcode-block" id="ADMM">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.ADMM.html#deepinv.optim.ADMM">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ADMM</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ADMM module for solving the problem</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>

<span class="sd">    where :math:`\datafid{x}{y}` is the data-fidelity term, :math:`\reg{x}` is the regularization term.</span>
<span class="sd">    If the attribute ``g_first`` is set to False (by default), the ADMM iterations write (see :footcite:t:`boyd2011distributed` for more details):</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k+1} &amp;= \operatorname{prox}_{\gamma f}(x_k - z_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox}_{\gamma \lambda \regname}(u_{k+1} + z_k) \\</span>
<span class="sd">        z_{k+1} &amp;= z_k + \beta (u_{k+1} - x_{k+1})</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    where :math:`\gamma&gt;0` is a stepsize and :math:`\beta&gt;0` is a relaxation parameter.  If the attribute ``g_first`` is set to ``True``, the functions :math:`f` and :math:`\regname` are</span>
<span class="sd">    inverted in the previous iterations. The ADMM iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.ADMMIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the algorithmic parameters (stepsize, regularization parameter, etc.) of the algorithm are trainable. </span>
<span class="sd">    By default (if the attribute ``unfold`` is set to ``True``) all the algorithm parameters are trainable: the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter and the relaxation parameter :math:`\beta`.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float beta: ADMM relaxation parameter :math:`\beta`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of ADMM parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;, &quot;beta&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the ADMM parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg`, `g_param` and `beta`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
                <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ADMM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">ADMMIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="DRS">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.DRS.html#deepinv.optim.DRS">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DRS</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DRS module for solving the problem</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>

<span class="sd">    where :math:`\datafid{x}{y}` is the data-fidelity term, :math:`\reg{x}` is the regularization term.</span>
<span class="sd">     If the attribute ``g_first`` is set to False (by default), the DRS iterations are given by</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k+1} &amp;= \operatorname{prox}_{\gamma f}(z_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox}_{\gamma \lambda \regname}(2*u_{k+1}-z_k) \\</span>
<span class="sd">        z_{k+1} &amp;= z_k + \beta (x_{k+1} - u_{k+1})</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    where :math:`\gamma&gt;0` is a stepsize and :math:`\beta&gt;0` is a relaxation parameter. If the attribute ``g_first`` is set to True, the functions :math:`f` and :math:`\regname` are inverted in the previous iteration.</span>
<span class="sd">    The DRS iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.DRSIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter and the relaxation parameter :math:`\beta`.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float beta: DRS relaxation parameter :math:`\beta`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of DRS parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;, &quot;beta&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the DRS parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg`, `g_param` and `beta`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
                <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DRS</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">DRSIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="GD">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.GD.html#deepinv.optim.GD">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GD</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient Descent (GD) module for solving the problem</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>

<span class="sd">    where :math:`\datafid{x}{y}` is the data-fidelity term, :math:`\reg{x}` is the regularization term.</span>

<span class="sd">    The Gradient Descent iterations are given by</span>

<span class="sd">    .. math::</span>
<span class="sd">        x_{k+1} = x_k - \gamma \nabla f(x_k) - \gamma \lambda \nabla \regname(x_k)</span>

<span class="sd">    where :math:`\gamma&gt;0` is a stepsize. The Gradient Descent iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.GDIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default.</span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">        :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param deepinv.optim.BacktrackingConfig, bool backtracking: configuration for using a backtracking line-search strategy for automatic stepsize adaptation.</span>
<span class="sd">        If None (default), stepsize backtracking is disabled. Otherwise, ``backtracking`` must be an instance of :class:`deepinv.optim.BacktrackingConfig`, which defines the parameters for backtracking line-search.</span>
<span class="sd">        By default, backtracking is disabled (i.e., ``backtracking=None``), and as soon as ``backtracking`` is not ``None``, the default ``BacktrackingConfig`` is used.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>

<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>

<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method.</span>

<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of GD parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param deepinv.optim.DEQConfig, bool DEQ: Configuration for a Deep Equilibrium (DEQ) unfolding strategy.</span>
<span class="sd">        DEQ algorithms are virtually unrolled infinitely, leveraging the implicit function theorem.</span>
<span class="sd">        If ``None`` (default) or ``False``, DEQ is disabled and the algorithm runs a standard finite number of iterations.</span>
<span class="sd">        Otherwise, ``DEQ`` must be an instance of :class:`deepinv.optim.DEQConfig`, which defines the parameters</span>
<span class="sd">        for forward and backward equilibrium-based implicit differentiation.</span>
<span class="sd">        If ``True``, the default ``DEQConfig`` is used.</span>
<span class="sd">    :param bool anderson_acceleration: Configure Anderson acceleration for the fixed-point iterations.</span>
<span class="sd">        If ``None`` (default) or ``False``, Anderson acceleration is disabled.</span>
<span class="sd">        Otherwise, ``anderson_acceleration`` must be an instance of :class:`deepinv.optim.AndersonAccelerationConfig`, which defines the parameters for Anderson acceleration.</span>
<span class="sd">        If ``True``, the default ``AndersonAccelerationConfig`` is used.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.</span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input</span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`),</span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the GD parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg` and `g_param`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">backtracking</span><span class="p">:</span> <span class="n">BacktrackingConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">DEQ</span><span class="p">:</span> <span class="n">DEQConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">anderson_acceleration</span><span class="p">:</span> <span class="n">AndersonAccelerationConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">GDIteration</span><span class="p">(</span><span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">backtracking</span><span class="o">=</span><span class="n">backtracking</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="n">DEQ</span><span class="o">=</span><span class="n">DEQ</span><span class="p">,</span>
            <span class="n">anderson_acceleration</span><span class="o">=</span><span class="n">anderson_acceleration</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="HQS">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.HQS.html#deepinv.optim.HQS">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">HQS</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Half-Quadratic Splitting (HQS) module for solving the problem</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\datafid{x}{y}` is the data-fidelity term, :math:`\reg{x}` is the regularization term.</span>
<span class="sd">    If the attribute ``g_first`` is set to False (by default), the HQS iterations are given by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k} &amp;= \operatorname{prox}_{\gamma f}(x_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox}_{\sigma \lambda \regname}(u_k).</span>
<span class="sd">        \end{aligned}</span>
<span class="sd">    </span>
<span class="sd">    If the attribute ``g_first`` is set to True, the functions :math:`f` and :math:`\regname` are inverted in the previous iteration.</span>
<span class="sd">    The HQS iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.HQSIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of HQS parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param deepinv.optim.DEQConfig, bool DEQ: Configuration for a Deep Equilibrium (DEQ) unfolding strategy.</span>
<span class="sd">        DEQ algorithms are virtually unrolled infinitely, leveraging the implicit function theorem.</span>
<span class="sd">        If ``None`` (default) or ``False``, DEQ is disabled and the algorithm runs a standard finite number of iterations.</span>
<span class="sd">        Otherwise, ``DEQ`` must be an instance of :class:`deepinv.optim.DEQConfig`, which defines the parameters</span>
<span class="sd">        for forward and backward equilibrium-based implicit differentiation.</span>
<span class="sd">        If ``True``, the default ``DEQConfig`` is used.</span>
<span class="sd">    :param bool anderson_acceleration: Configure Anderson acceleration for the fixed-point iterations.</span>
<span class="sd">        If ``None`` (default) or ``False``, Anderson acceleration is disabled.</span>
<span class="sd">        Otherwise, ``anderson_acceleration`` must be an instance of :class:`deepinv.optim.AndersonAccelerationConfig`, which defines the parameters for Anderson acceleration.</span>
<span class="sd">        If ``True``, the default ``AndersonAccelerationConfig`` is used.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the HQS parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg` and `g_param`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">DEQ</span><span class="p">:</span> <span class="n">DEQConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">anderson_acceleration</span><span class="p">:</span> <span class="n">AndersonAccelerationConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HQS</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">HQSIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="n">DEQ</span><span class="o">=</span><span class="n">DEQ</span><span class="p">,</span>
            <span class="n">anderson_acceleration</span><span class="o">=</span><span class="n">anderson_acceleration</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="PGD">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.PGD.html#deepinv.optim.PGD">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PGD</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Proximal Gradient Descent (PGD) module for solving the problem</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{equation}</span>
<span class="sd">        \label{eq:min_prob}</span>
<span class="sd">        \tag{1}</span>
<span class="sd">        \underset{x}{\arg\min} \quad  \datafid{x}{y} + \lambda \reg{x},</span>
<span class="sd">        \end{equation}</span>

<span class="sd">    where :math:`\datafid{x}{y}` is the data-fidelity term, :math:`\reg{x}` is the regularization term.</span>
<span class="sd">    If the attribute ``g_first`` is set to False (by default), the PGD iterations are given by</span>

<span class="sd">    .. math::</span>
<span class="sd">        x_{k+1} = \operatorname{prox}_{\gamma \lambda \regname}(x_k - \gamma \nabla f(x_k)).</span>

<span class="sd">    If the attribute ``g_first`` is set to True, the functions :math:`f` and :math:`\regname` are inverted in the previous iteration.</span>
<span class="sd">    The PGD iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.PGDIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default.</span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param deepinv.optim.BacktrackingConfig, bool backtracking: configuration for using a backtracking line-search strategy for automatic stepsize adaptation.</span>
<span class="sd">        If None (default), stepsize backtracking is disabled. Otherwise, ``backtracking`` must be an instance of :class:`deepinv.optim.BacktrackingConfig`, which defines the parameters for backtracking line-search.</span>
<span class="sd">        By default, backtracking is disabled (i.e., ``backtracking=None``), and as soon as ``backtracking`` is not ``None``, the default ``BacktrackingConfig`` is used.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>

<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>

<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method.</span>

<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of PGD parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param deepinv.optim.DEQConfig, bool DEQ: Configuration for a Deep Equilibrium (DEQ) unfolding strategy.</span>
<span class="sd">        DEQ algorithms are virtually unrolled infinitely, leveraging the implicit function theorem.</span>
<span class="sd">        If ``None`` (default) or ``False``, DEQ is disabled and the algorithm runs a standard finite number of iterations.</span>
<span class="sd">        Otherwise, ``DEQ`` must be an instance of :class:`deepinv.optim.DEQConfig`, which defines the parameters</span>
<span class="sd">        for forward and backward equilibrium-based implicit differentiation.</span>
<span class="sd">        If ``True``, the default ``DEQConfig`` is used.</span>
<span class="sd">    :param bool anderson_acceleration: Configure Anderson acceleration for the fixed-point iterations.</span>
<span class="sd">        If ``None`` (default) or ``False``, Anderson acceleration is disabled.</span>
<span class="sd">        Otherwise, ``anderson_acceleration`` must be an instance of :class:`deepinv.optim.AndersonAccelerationConfig`, which defines the parameters for Anderson acceleration.</span>
<span class="sd">        If ``True``, the default ``AndersonAccelerationConfig`` is used.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.</span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input</span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`),</span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the PGD parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg` and `g_param`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">backtracking</span><span class="p">:</span> <span class="n">BacktrackingConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">DEQ</span><span class="p">:</span> <span class="n">DEQConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">anderson_acceleration</span><span class="p">:</span> <span class="n">AndersonAccelerationConfig</span> <span class="o">|</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PGD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">PGDIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">backtracking</span><span class="o">=</span><span class="n">backtracking</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="n">DEQ</span><span class="o">=</span><span class="n">DEQ</span><span class="p">,</span>
            <span class="n">anderson_acceleration</span><span class="o">=</span><span class="n">anderson_acceleration</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="FISTA">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.FISTA.html#deepinv.optim.FISTA">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">FISTA</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FISTA module for acceleration of the Proximal Gradient Descent algorithm.</span>
<span class="sd">    If the attribute ``g_first`` is set to False (by default), the FISTA iterations are given by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k} &amp;= z_k -  \gamma \nabla f(z_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox}_{\gamma \lambda \regname}(u_k) \\</span>
<span class="sd">        z_{k+1} &amp;= x_{k+1} + \alpha_k (x_{k+1} - x_k),</span>
<span class="sd">        \end{aligned}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\gamma` is a stepsize that should satisfy :math:`\gamma \leq 1/\operatorname{Lip}(\|\nabla f\|)` and</span>
<span class="sd">    :math:`\alpha_k = (k+a-1)/(k+a)`,  with :math:`a` a parameter that should be strictly greater than 2.</span>
<span class="sd">    </span>
<span class="sd">    If the attribute ``g_first`` is set to True, the functions :math:`f` and :math:`\regname` are inverted in the previous iteration.</span>
<span class="sd">    The FISTA iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.FISTAIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter, and the parameter :math:`a` of the FISTA algorithm.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param int a: parameter of the FISTA algorithm, should be strictly greater than 2. Default: ``3``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of FISTA parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;, &quot;a&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the FISTA parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg`, `g_param` and `a`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">a</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
                <span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FISTA</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">FISTAIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="MD">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.MD.html#deepinv.optim.MD">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MD</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mirror Descent (MD) or Bregman variant of the Gradient Descent algorithm. For a given convex potential :math:`h`, the iterations are given by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        v_{k} &amp;= \nabla f(x_k) + \lambda \nabla g(x_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \nabla h^*(\nabla h(x_k) - \gamma v_{k})</span>
<span class="sd">        \end{aligned}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\gamma&gt;0` is a stepsize and :math:`h^*` is the convex conjugate of :math:`h`.</span>
<span class="sd">    The Mirror Descent iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.MDIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    :param deepinv.optim.Bregman bregman_potential: Bregman potential used for Bregman optimization algorithms such as Mirror Descent. Default: ``BregmanL2()``.</span>
<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of MD parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the MD parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg` and `g_param`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bregman_potential</span><span class="o">=</span><span class="n">BregmanL2</span><span class="p">(),</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>

        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">MDIteration</span><span class="p">(</span><span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">,</span> <span class="n">bregman_potential</span><span class="o">=</span><span class="n">bregman_potential</span><span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="PMD">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.PMD.html#deepinv.optim.PMD">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PMD</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    Proximal Mirror Descent (PMD) or Bregman variant of the Proximal Gradient Descent algorithm. For a given convex potential :math:`h`, the iterations are given by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k} &amp;= \nabla h^*(\nabla h(x_k) - \gamma \nabla f(x_k)) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox^h}_{\gamma \lambda \regname}(u_k)</span>
<span class="sd">        \end{aligned}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\gamma` is a stepsize that should satisfy :math:`\gamma \leq 2/L` with :math:`L` verifying :math:`Lh-f` is convex. </span>
<span class="sd">    :math:`\operatorname{prox^h}_{\gamma \lambda \regname}` is the Bregman proximal operator, detailed in the method :meth:`deepinv.optim.Potential.bregman_prox`.</span>
<span class="sd">    The Proximal Mirror Descent iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.PMDIteration`.</span>
<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, all the algorithm parameters are trainable : the stepsize :math:`\gamma`, the regularization parameter :math:`\lambda`, the prior parameter.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>
<span class="sd">    </span>
<span class="sd">    :param deepinv.optim.Bregman bregman_potential: Bregman potential used for Bregman optimization algorithms such as Proximal Mirror Descent. Default: ``BregmanL2()``.</span>
<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">          Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">          :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">          Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">          :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\gamma`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of PMD parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;g_param&quot;]``. Default: None, which means that all parameters are trainable if ``unfold`` is True. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the PMD parameters in a dictionary. This will overwrite the parameters in the arguments `stepsize`, `lambda_reg` and `g_param`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bregman_potential</span><span class="o">=</span><span class="n">BregmanL2</span><span class="p">(),</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>
        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PMD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">PMDIteration</span><span class="p">(</span>
                <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">,</span> <span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">bregman_potential</span><span class="o">=</span><span class="n">bregman_potential</span>
            <span class="p">),</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="PDCP">
<a class="viewcode-back" href="../../../api/stubs/deepinv.optim.PDCP.html#deepinv.optim.PDCP">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PDCP</span><span class="p">(</span><span class="n">BaseOptim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; Primal Dual Chambolle-Pock optimization module.</span>
<span class="sd">    </span>
<span class="sd">    Implementation of the `Primal-Dual Chambolle-Pock (PDCP) &lt;https://hal.science/hal-00490826/document&gt;`_ </span>
<span class="sd">    algorithm for minimising :math:`F(Kx) + \lambda G(x)` or :math:`\lambda F(x) + G(Kx)` for generic functions :math:`F` and :math:`G`.</span>
<span class="sd">    Our implementation corresponds to Algorithm 1 of `&lt;https://hal.science/hal-00490826/document&gt;`_.</span>

<span class="sd">    If the attribute ``g_first`` is set to ``False`` (by default), a single iteration is given by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        u_{k+1} &amp;= \operatorname{prox}_{\sigma F^*}(u_k + \sigma K z_k) \\</span>
<span class="sd">        x_{k+1} &amp;= \operatorname{prox}_{\tau \lambda G}(x_k-\tau K^\top u_{k+1}) \\</span>
<span class="sd">        z_{k+1} &amp;= x_{k+1} + \beta(x_{k+1}-x_k) \\</span>
<span class="sd">        \end{aligned}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`F^*` is the Fenchel-Legendre conjugate of :math:`F`, :math:`\beta&gt;0` is a relaxation parameter, and :math:`\sigma` and :math:`\tau` are step-sizes that should</span>
<span class="sd">    satisfy :math:`\sigma \tau \|K\|^2 \leq 1`. </span>

<span class="sd">    If the attribute ``g_first`` is set to ``True``, the functions :math:`F` and :math:`G` are inverted in the previous iteration.</span>
<span class="sd">    In particular, setting :math:`F = \distancename`, :math:`K = A` and :math:`G = \regname`, the above algorithms solves</span>

<span class="sd">    .. math::</span>
<span class="sd">        \underset{x}{\operatorname{min}} \,\,  \distancename(Ax, y) + \lambda \regname(x)</span>
<span class="sd">    </span>
<span class="sd">    with a splitting on :math:`\distancename`.</span>

<span class="sd">    Note that the algorithm requires an intiliazation of the three variables :math:`x_0`, :math:`z_0` and :math:`u_0`.</span>

<span class="sd">    For using early stopping or stepsize backtracking, see the documentation of the :class:`deepinv.optim.BaseOptim` class.</span>

<span class="sd">    If the attribute ``unfold`` is set to ``True``, the algorithm is unfolded and the parameters of the algorithm are trainable.</span>
<span class="sd">    By default, the trainable parameters are : the stepsize :math:`\sigma`, the stepsize :math:`\tau`, the regularization parameter :math:`\lambda`, the prior parameter and the relaxation parameter :math:`\beta`.</span>
<span class="sd">    Use the ``trainable_params`` argument to adjust the list of trainable parameters.</span>
<span class="sd">    Note also that by default, if the prior has trainable parameters (e.g. a neural network denoiser), these parameters are learnable by default. </span>
<span class="sd">    If the model is used for inference only, use the ``with torch.no_grad():`` context when calling the model in order to avoid unnecessary gradient computations.</span>

<span class="sd">    The Primal Dual CP iterations are defined in the iterator class :class:`deepinv.optim.optim_iterators.CPIteration`.</span>

<span class="sd">    :param Callable K: linear operator :math:`K` in the primal problem. Default: identity function.</span>
<span class="sd">    :param Callable K_adjoint: adjoint linear operator :math:`K^\top` in the primal problem. Default: identity function.</span>
<span class="sd">    :param list, deepinv.optim.DataFidelity data_fidelity: data-fidelity term :math:`\datafid{x}{y}`.</span>
<span class="sd">        Either a single instance (same data-fidelity for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.DataFidelity` (distinct data fidelity for each iteration). Default: ``None`` corresponding to :math:`\datafid{x}{y} = 0`.</span>
<span class="sd">    :param list, deepinv.optim.Prior prior: regularization prior :math:`\reg{x}`.</span>
<span class="sd">        Either a single instance (same prior for each iteration) or a list of instances of</span>
<span class="sd">        :class:`deepinv.optim.Prior` (distinct prior for each iteration). Default: ``None`` corresponding to :math:`\reg{x} = 0`.</span>
<span class="sd">    :param float lambda_reg: regularization parameter :math:`\lambda`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize: stepsize parameter :math:`\tau`. Default: ``1.0``.</span>
<span class="sd">    :param float stepsize_dual: stepsize parameter :math:`\sigma`. Default: ``1.0``.</span>
<span class="sd">    :param float beta: PD relaxation parameter :math:`\beta`. Default: ``1.0``.</span>
<span class="sd">    :param float g_param: parameter of the prior function. For example the noise level for a denoising prior. Default: ``None``.</span>
<span class="sd">    :param float sigma_denoiser: same as ``g_param``. If both ``g_param`` and ``sigma_denoiser`` are provided, ``g_param`` is used. Default: ``None``.</span>
<span class="sd">    :param int max_iter: maximum number of iterations of the optimization algorithm. Default: ``100``.</span>
<span class="sd">    :param str crit_conv: convergence criterion to be used for claiming convergence, either ``&quot;residual&quot;`` (residual</span>
<span class="sd">        of the iterate norm) or ``&quot;cost&quot;`` (on the cost function). Default: ``&quot;residual&quot;``</span>
<span class="sd">    :param float thres_conv: convergence threshold for the chosen convergence criterion. Default: ``1e-5``.</span>
<span class="sd">    :param bool early_stop: whether to stop the algorithm as soon as the convergence criterion is met. Default: ``False``.</span>
<span class="sd">    :param dict custom_metrics: dictionary of custom metric functions to be computed along the iterations. The keys of the dictionary are the names of the metrics, and the values are functions that take as input the current and previous iterates, and return a scalar value. Default: ``None``.</span>
<span class="sd">    :param Callable custom_init:  Custom initialization of the algorithm.</span>
<span class="sd">        The callable function ``custom_init(y, physics)`` takes as input the measurement :math:`y` and the physics ``physics`` and returns the initialization in the form of either:</span>
<span class="sd">        </span>
<span class="sd">        - a tuple :math:`(x_0, z_0)` (where ``x_0`` and ``z_0`` are the initial primal and dual variables),</span>
<span class="sd">        - a torch.Tensor :math:`x_0` (if no dual variables :math:`z_0` are used), or</span>
<span class="sd">        - a dictionary of the form ``X = {&#39;est&#39;: (x_0, z_0)}``.</span>
<span class="sd">        </span>
<span class="sd">        Note that custom initialization can also be directly defined via the ``init`` argument in the ``forward`` method. </span>
<span class="sd">        </span>
<span class="sd">        If ``None`` (default value), the algorithm is initialized with the adjoint :math:`A^{\top}y` when the adjoint is defined,</span>
<span class="sd">        and with the observation `y` if the adjoint is not defined. Default: ``None``.</span>
<span class="sd">    :param bool g_first: whether to perform the proximal step on :math:`\reg{x}` before that on :math:`\datafid{x}{y}`, or the opposite. Default: ``False``.</span>
<span class="sd">    :param bool unfold: whether to unfold the algorithm or not. Default: ``False``.</span>
<span class="sd">    :param list trainable_params: list of PD parameters to be trained if ``unfold`` is True. To choose between ``[&quot;lambda&quot;, &quot;stepsize&quot;, &quot;stepsize_dual&quot;, &quot;g_param&quot;, &quot;beta&quot;]``. For no trainable parameters, set to an empty list.</span>
<span class="sd">    :param Callable cost_fn: Custom user input cost function.    </span>
<span class="sd">            ``cost_fn(x, data_fidelity, prior, cur_params, y, physics)`` takes as input </span>
<span class="sd">            the current primal variable (:class:`torch.Tensor`), the current data-fidelity (:class:`deepinv.optim.DataFidelity`), </span>
<span class="sd">            the current prior (:class:`deepinv.optim.Prior`), the current parameters (dict), and the measurement (:class:`torch.Tensor`).</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">    :param dict params_algo: optionally, directly provide the PD parameters in a dictionary. This will overwrite the parameters in the arguments `K`, `K_adjoint`, `stepsize`, `lambda_reg`, `stepsize_dual`, `g_param`, `beta`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">K</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">K_adjoint</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">data_fidelity</span><span class="p">:</span> <span class="n">DataFidelity</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">DataFidelity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">:</span> <span class="n">Prior</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prior</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_reg</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">stepsize_dual</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">g_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma_denoiser</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">crit_conv</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;residual&quot;</span><span class="p">,</span>
        <span class="n">thres_conv</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_init</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Physics</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">g_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">unfold</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cost_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">DataFidelity</span><span class="p">,</span>
                <span class="n">Prior</span><span class="p">,</span>
                <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">Physics</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_algo</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">g_param</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sigma_denoiser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">g_param</span> <span class="o">=</span> <span class="n">sigma_denoiser</span>
        <span class="k">if</span> <span class="n">params_algo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params_algo</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">:</span> <span class="n">lambda_reg</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">:</span> <span class="n">stepsize</span><span class="p">,</span>
                <span class="s2">&quot;stepsize_dual&quot;</span><span class="p">:</span> <span class="n">stepsize_dual</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">:</span> <span class="n">g_param</span><span class="p">,</span>
                <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
                <span class="s2">&quot;K&quot;</span><span class="p">:</span> <span class="n">K</span><span class="p">,</span>
                <span class="s2">&quot;K_adjoint&quot;</span><span class="p">:</span> <span class="n">K_adjoint</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">if</span> <span class="n">trainable_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s2">&quot;lambda&quot;</span><span class="p">,</span>
                <span class="s2">&quot;stepsize&quot;</span><span class="p">,</span>
                <span class="s2">&quot;stepsize_dual&quot;</span><span class="p">,</span>
                <span class="s2">&quot;g_param&quot;</span><span class="p">,</span>
                <span class="s2">&quot;beta&quot;</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="k">if</span> <span class="n">custom_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">custom_init</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">physics</span><span class="p">):</span>
                <span class="n">x_init</span> <span class="o">=</span> <span class="n">physics</span><span class="o">.</span><span class="n">A_adjoint</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
                <span class="n">u_init</span> <span class="o">=</span> <span class="n">y</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;est&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">x_init</span><span class="p">,</span> <span class="n">u_init</span><span class="p">)}</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">PDCP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">CPIteration</span><span class="p">(</span><span class="n">g_first</span><span class="o">=</span><span class="n">g_first</span><span class="p">,</span> <span class="n">cost_fn</span><span class="o">=</span><span class="n">cost_fn</span><span class="p">),</span>
            <span class="n">custom_init</span><span class="o">=</span><span class="n">custom_init</span><span class="p">,</span>
            <span class="n">data_fidelity</span><span class="o">=</span><span class="n">data_fidelity</span><span class="p">,</span>
            <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
            <span class="n">params_algo</span><span class="o">=</span><span class="n">params_algo</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">crit_conv</span><span class="o">=</span><span class="n">crit_conv</span><span class="p">,</span>
            <span class="n">thres_conv</span><span class="o">=</span><span class="n">thres_conv</span><span class="p">,</span>
            <span class="n">early_stop</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
            <span class="n">custom_metrics</span><span class="o">=</span><span class="n">custom_metrics</span><span class="p">,</span>
            <span class="n">unfold</span><span class="o">=</span><span class="n">unfold</span><span class="p">,</span>
            <span class="n">trainable_params</span><span class="o">=</span><span class="n">trainable_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      Â© Copyright deepinverse contributors 2025.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 9.0.4.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>