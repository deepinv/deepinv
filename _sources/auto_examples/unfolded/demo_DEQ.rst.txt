
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/unfolded/demo_DEQ.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        New to DeepInverse? Get started with the basics with the
        :ref:`5 minute quickstart tutorial <sphx_glr_auto_examples_basics_demo_quickstart.py>`.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_unfolded_demo_DEQ.py:


Deep Equilibrium (DEQ) algorithms for image deblurring
====================================================================================================

This a toy example to show you how to use DEQ to solve a deblurring problem.
Note that this is a small dataset for training. For optimal results, use a larger dataset.

For now DEQ is only possible with PGD, HQS and GD optimization algorithms.

.. GENERATED FROM PYTHON SOURCE LINES 11-23

.. code-block:: Python


    import deepinv as dinv
    from pathlib import Path
    import torch
    from torch.utils.data import DataLoader
    from deepinv.optim.data_fidelity import L2
    from deepinv.optim.prior import PnP
    from deepinv.unfolded import DEQ_builder
    from torchvision import transforms
    from deepinv.utils import load_dataset, load_degradation









.. GENERATED FROM PYTHON SOURCE LINES 24-27

Setup paths for data loading and results.
----------------------------------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 27-39

.. code-block:: Python


    BASE_DIR = Path(".")
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    CKPT_DIR = BASE_DIR / "ckpts"
    DEG_DIR = BASE_DIR / "degradations"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 40-43

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------------
In this example, we use the CBSD500 dataset and the Set3C dataset for testing.

.. GENERATED FROM PYTHON SOURCE LINES 43-62

.. code-block:: Python


    img_size = 32
    n_channels = 3  # 3 for color images, 1 for gray-scale images
    operation = "deblurring"
    # For simplicity, we use a small dataset for training.
    # To be replaced for optimal results. For example, you can use the larger "drunet" dataset.
    train_dataset_name = "CBSD500"
    test_dataset_name = "set3c"
    # Generate training and evaluation datasets in HDF5 folders and load them.
    test_transform = transforms.Compose(
        [transforms.CenterCrop(img_size), transforms.ToTensor()]
    )
    train_transform = transforms.Compose(
        [transforms.RandomCrop(img_size), transforms.ToTensor()]
    )
    train_base_dataset = load_dataset(train_dataset_name, transform=train_transform)
    test_base_dataset = load_dataset(test_dataset_name, transform=test_transform)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/CBSD500.zip
      0%|          | 0.00/71.0M [00:00<?, ?iB/s]     13%|█▎        | 9.14M/71.0M [00:00<00:00, 91.4MiB/s]     28%|██▊       | 19.9M/71.0M [00:00<00:00, 101MiB/s]      43%|████▎     | 30.5M/71.0M [00:00<00:00, 103MiB/s]     58%|█████▊    | 41.3M/71.0M [00:00<00:00, 105MiB/s]     74%|███████▎  | 52.3M/71.0M [00:00<00:00, 107MiB/s]     89%|████████▉ | 63.4M/71.0M [00:00<00:00, 108MiB/s]    100%|██████████| 71.0M/71.0M [00:00<00:00, 106MiB/s]
    CBSD500 dataset downloaded in datasets
    Downloading datasets/set3c.zip
      0%|          | 0.00/385k [00:00<?, ?iB/s]    100%|██████████| 385k/385k [00:00<00:00, 21.3MiB/s]
    set3c dataset downloaded in datasets




.. GENERATED FROM PYTHON SOURCE LINES 63-66

Generate a dataset of low resolution images and load it.
----------------------------------------------------------------------------------------
We use the Downsampling class from the physics module to generate a dataset of low resolution images.

.. GENERATED FROM PYTHON SOURCE LINES 66-109

.. code-block:: Python



    # Use parallel dataloader if using a GPU to speed up training, otherwise, as all computes are on CPU, use synchronous
    # dataloading.
    num_workers = 4 if torch.cuda.is_available() else 0

    # Degradation parameters
    noise_level_img = 0.03

    # Generate a motion blur operator.
    kernel_index = 1  # which kernel to chose among the 8 motion kernels from 'Levin09.mat'
    kernel_torch = load_degradation("Levin09.npy", DEG_DIR / "kernels", index=kernel_index)
    kernel_torch = kernel_torch.unsqueeze(0).unsqueeze(
        0
    )  # add batch and channel dimensions

    # Generate the gaussian blur downsampling operator.
    physics = dinv.physics.BlurFFT(
        img_size=(n_channels, img_size, img_size),
        filter=kernel_torch,
        device=device,
        noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),
    )

    my_dataset_name = "demo_DEQ"
    n_images_max = (
        1000 if torch.cuda.is_available() else 10
    )  # maximal number of images used for training
    measurement_dir = DATA_DIR / train_dataset_name / operation
    generated_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_base_dataset,
        test_dataset=test_base_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Levin09.npy degradation downloaded in degradations/kernels
    Dataset has been saved at measurements/CBSD500/deblurring/demo_DEQ0.h5




.. GENERATED FROM PYTHON SOURCE LINES 110-116

Define the  DEQ algorithm.
----------------------------------------------------------------------------------------
We use the helper function :func:`deepinv.unfolded.DEQ_builder` to defined the DEQ architecture.
The chosen algorithm is here HQS (Half Quadratic Splitting).
Note for DEQ, the prior and regularization parameters should be common for all iterations
to keep a constant fixed-point operator.

.. GENERATED FROM PYTHON SOURCE LINES 116-155

.. code-block:: Python



    # Select the data fidelity term
    data_fidelity = L2()

    # Set up the trainable denoising prior. Here the prior model is common for all iterations. We use here a pretrained denoiser.
    prior = PnP(denoiser=dinv.models.DnCNN(depth=20, pretrained="download").to(device))

    # Unrolled optimization algorithm parameters
    max_iter = 20 if torch.cuda.is_available() else 10
    stepsize = [1.0]  # stepsize of the algorithm
    sigma_denoiser = [0.03]  # noise level parameter of the denoiser
    jacobian_free = False  # does not perform Jacobian inversion.

    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
    }
    trainable_params = [
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = DEQ_builder(
        iteration="PGD",  # For now DEQ is only possible with PGD, HQS and GD optimization algorithms.
        params_algo=params_algo.copy(),
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
        anderson_acceleration=True,
        anderson_acceleration_backward=True,
        history_size_backward=3,
        history_size=3,
        max_iter_backward=20,
        jacobian_free=jacobian_free,
    )








.. GENERATED FROM PYTHON SOURCE LINES 156-159

Define the training parameters.
-------------------------------
We use the Adam optimizer and the StepLR scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 159-185

.. code-block:: Python



    # training parameters
    epochs = 10 if torch.cuda.is_available() else 2
    learning_rate = 1e-4
    train_batch_size = 32 if torch.cuda.is_available() else 1
    test_batch_size = 3


    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))

    # choose supervised training loss
    losses = [dinv.loss.SupLoss(metric=dinv.metric.MSE())]

    # Logging parameters
    verbose = True

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 186-189

Train the network
-----------------
We train the network using the library's train function.

.. GENERATED FROM PYTHON SOURCE LINES 189-208

.. code-block:: Python


    trainer = dinv.Trainer(
        model=model,
        physics=physics,
        epochs=epochs,
        scheduler=scheduler,
        device=device,
        losses=losses,
        optimizer=optimizer,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        show_progress_bar=True,  # disable progress bar for better vis in sphinx gallery.
    )

    trainer.train()
    model = trainer.load_best_model()  # load model with best validation PSNR





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 668229 trainable parameters
    /home/runner/work/deepinv/deepinv/deepinv/training/trainer.py:516: UserWarning: Update progress bar frequency of 1 may slow down training on GPU. Consider setting freq_update_progress_bar > 1.
      warnings.warn(
      0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 1/2:   0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 1/2:   0%|          | 0/10 [00:00<?, ?it/s, TotalLoss=9.15e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:00<00:03,  2.66it/s, TotalLoss=9.15e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:00<00:03,  2.66it/s, TotalLoss=9.15e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:00<00:03,  2.66it/s, TotalLoss=0.000929, PSNR=34]     Train epoch 1/2:  20%|██        | 2/10 [00:00<00:03,  2.67it/s, TotalLoss=0.000929, PSNR=34]    Train epoch 1/2:  20%|██        | 2/10 [00:00<00:03,  2.67it/s, TotalLoss=0.000929, PSNR=34]    Train epoch 1/2:  20%|██        | 2/10 [00:01<00:03,  2.67it/s, TotalLoss=0.000793, PSNR=33.6]    Train epoch 1/2:  30%|███       | 3/10 [00:01<00:02,  2.66it/s, TotalLoss=0.000793, PSNR=33.6]    Train epoch 1/2:  30%|███       | 3/10 [00:01<00:02,  2.66it/s, TotalLoss=0.000793, PSNR=33.6]    Train epoch 1/2:  30%|███       | 3/10 [00:01<00:02,  2.66it/s, TotalLoss=0.0019, PSNR=30.9]      Train epoch 1/2:  40%|████      | 4/10 [00:01<00:02,  2.70it/s, TotalLoss=0.0019, PSNR=30.9]    Train epoch 1/2:  40%|████      | 4/10 [00:01<00:02,  2.70it/s, TotalLoss=0.0019, PSNR=30.9]    Train epoch 1/2:  40%|████      | 4/10 [00:01<00:02,  2.70it/s, TotalLoss=0.00268, PSNR=29.2]    Train epoch 1/2:  50%|█████     | 5/10 [00:01<00:01,  2.71it/s, TotalLoss=0.00268, PSNR=29.2]    Train epoch 1/2:  50%|█████     | 5/10 [00:01<00:01,  2.71it/s, TotalLoss=0.00268, PSNR=29.2]    Train epoch 1/2:  50%|█████     | 5/10 [00:02<00:01,  2.71it/s, TotalLoss=0.00302, PSNR=28.2]    Train epoch 1/2:  60%|██████    | 6/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00302, PSNR=28.2]    Train epoch 1/2:  60%|██████    | 6/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00302, PSNR=28.2]    Train epoch 1/2:  60%|██████    | 6/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00308, PSNR=27.7]    Train epoch 1/2:  70%|███████   | 7/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00308, PSNR=27.7]    Train epoch 1/2:  70%|███████   | 7/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00308, PSNR=27.7]    Train epoch 1/2:  70%|███████   | 7/10 [00:02<00:01,  2.73it/s, TotalLoss=0.00315, PSNR=27.3]    Train epoch 1/2:  80%|████████  | 8/10 [00:02<00:00,  2.74it/s, TotalLoss=0.00315, PSNR=27.3]    Train epoch 1/2:  80%|████████  | 8/10 [00:02<00:00,  2.74it/s, TotalLoss=0.00315, PSNR=27.3]    Train epoch 1/2:  80%|████████  | 8/10 [00:03<00:00,  2.74it/s, TotalLoss=0.00402, PSNR=26.4]    Train epoch 1/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.00402, PSNR=26.4]    Train epoch 1/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.00402, PSNR=26.4]    Train epoch 1/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.00394, PSNR=26.3]    Train epoch 1/2: 100%|██████████| 10/10 [00:03<00:00,  2.75it/s, TotalLoss=0.00394, PSNR=26.3]    Train epoch 1/2: 100%|██████████| 10/10 [00:03<00:00,  2.73it/s, TotalLoss=0.00394, PSNR=26.3]
      0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 1/2:   0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 1/2:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=21.8]    Eval epoch 1/2: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s, PSNR=21.8]    Eval epoch 1/2: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s, PSNR=21.8]
    Best model saved at epoch 1
      0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 2/2:   0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 2/2:   0%|          | 0/10 [00:00<?, ?it/s, TotalLoss=0.00354, PSNR=24.5]    Train epoch 2/2:  10%|█         | 1/10 [00:00<00:03,  2.72it/s, TotalLoss=0.00354, PSNR=24.5]    Train epoch 2/2:  10%|█         | 1/10 [00:00<00:03,  2.72it/s, TotalLoss=0.00354, PSNR=24.5]    Train epoch 2/2:  10%|█         | 1/10 [00:00<00:03,  2.72it/s, TotalLoss=0.00238, PSNR=26.8]    Train epoch 2/2:  20%|██        | 2/10 [00:00<00:02,  2.71it/s, TotalLoss=0.00238, PSNR=26.8]    Train epoch 2/2:  20%|██        | 2/10 [00:00<00:02,  2.71it/s, TotalLoss=0.00238, PSNR=26.8]    Train epoch 2/2:  20%|██        | 2/10 [00:01<00:02,  2.71it/s, TotalLoss=0.00326, PSNR=25.5]    Train epoch 2/2:  30%|███       | 3/10 [00:01<00:02,  2.72it/s, TotalLoss=0.00326, PSNR=25.5]    Train epoch 2/2:  30%|███       | 3/10 [00:01<00:02,  2.72it/s, TotalLoss=0.00326, PSNR=25.5]    Train epoch 2/2:  30%|███       | 3/10 [00:01<00:02,  2.72it/s, TotalLoss=0.00263, PSNR=27]      Train epoch 2/2:  40%|████      | 4/10 [00:01<00:02,  2.73it/s, TotalLoss=0.00263, PSNR=27]    Train epoch 2/2:  40%|████      | 4/10 [00:01<00:02,  2.73it/s, TotalLoss=0.00263, PSNR=27]    Train epoch 2/2:  40%|████      | 4/10 [00:01<00:02,  2.73it/s, TotalLoss=0.00269, PSNR=26.6]    Train epoch 2/2:  50%|█████     | 5/10 [00:01<00:01,  2.74it/s, TotalLoss=0.00269, PSNR=26.6]    Train epoch 2/2:  50%|█████     | 5/10 [00:01<00:01,  2.74it/s, TotalLoss=0.00269, PSNR=26.6]    Train epoch 2/2:  50%|█████     | 5/10 [00:02<00:01,  2.74it/s, TotalLoss=0.00311, PSNR=26]      Train epoch 2/2:  60%|██████    | 6/10 [00:02<00:01,  2.75it/s, TotalLoss=0.00311, PSNR=26]    Train epoch 2/2:  60%|██████    | 6/10 [00:02<00:01,  2.75it/s, TotalLoss=0.00311, PSNR=26]    Train epoch 2/2:  60%|██████    | 6/10 [00:02<00:01,  2.75it/s, TotalLoss=0.0031, PSNR=25.9]    Train epoch 2/2:  70%|███████   | 7/10 [00:02<00:01,  2.76it/s, TotalLoss=0.0031, PSNR=25.9]    Train epoch 2/2:  70%|███████   | 7/10 [00:02<00:01,  2.76it/s, TotalLoss=0.0031, PSNR=25.9]    Train epoch 2/2:  70%|███████   | 7/10 [00:02<00:01,  2.76it/s, TotalLoss=0.00401, PSNR=25.1]    Train epoch 2/2:  80%|████████  | 8/10 [00:02<00:00,  2.75it/s, TotalLoss=0.00401, PSNR=25.1]    Train epoch 2/2:  80%|████████  | 8/10 [00:02<00:00,  2.75it/s, TotalLoss=0.00401, PSNR=25.1]    Train epoch 2/2:  80%|████████  | 8/10 [00:03<00:00,  2.75it/s, TotalLoss=0.0036, PSNR=26.2]     Train epoch 2/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.0036, PSNR=26.2]    Train epoch 2/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.0036, PSNR=26.2]    Train epoch 2/2:  90%|█████████ | 9/10 [00:03<00:00,  2.75it/s, TotalLoss=0.00368, PSNR=25.9]    Train epoch 2/2: 100%|██████████| 10/10 [00:03<00:00,  2.74it/s, TotalLoss=0.00368, PSNR=25.9]    Train epoch 2/2: 100%|██████████| 10/10 [00:03<00:00,  2.74it/s, TotalLoss=0.00368, PSNR=25.9]
      0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 2/2:   0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 2/2:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=22.1]    Eval epoch 2/2: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s, PSNR=22.1]    Eval epoch 2/2: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s, PSNR=22.1]
    Best model saved at epoch 2




.. GENERATED FROM PYTHON SOURCE LINES 209-213

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 213-232

.. code-block:: Python


    trainer.test(test_dataloader)

    test_sample, _ = next(iter(test_dataloader))
    model.eval()
    test_sample = test_sample.to(device)

    # Get the measurements and the ground truth
    y = physics(test_sample)
    with torch.no_grad():
        rec = model(y, physics=physics)

    backprojected = physics.A_adjoint(y)

    dinv.utils.plot(
        [backprojected, rec, test_sample],
        titles=["Linear", "Reconstruction", "Ground truth"],
        suptitle="Reconstruction results",
    )



.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_DEQ_001.png
   :alt: Reconstruction results, Linear, Reconstruction, Ground truth
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_DEQ_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/1 [00:00<?, ?it/s]    Test:   0%|          | 0/1 [00:00<?, ?it/s]    Test:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=22.1, PSNR no learning=17]    Test: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s, PSNR=22.1, PSNR no learning=17]    Test: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s, PSNR=22.1, PSNR no learning=17]
    Test results:
    PSNR no learning: 16.952 +- 0.651
    PSNR: 22.115 +- 1.307
    /home/runner/work/deepinv/deepinv/deepinv/utils/plotting.py:379: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.
      fig.subplots_adjust(top=0.75)





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 10.509 seconds)


.. _sphx_glr_download_auto_examples_unfolded_demo_DEQ.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_DEQ.ipynb <demo_DEQ.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_DEQ.py <demo_DEQ.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_DEQ.zip <demo_DEQ.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
