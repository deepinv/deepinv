
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/blind-inverse-problems/demo_optimizing_physics_parameter.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        New to DeepInverse? Get started with the basics with the
        :ref:`5 minute quickstart tutorial <sphx_glr_auto_examples_basics_demo_quickstart.py>`..

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_blind-inverse-problems_demo_optimizing_physics_parameter.py:


Calibrating physics operators
=============================

This demo shows you how to use
:class:`deepinv.physics.Physics` together with automatic differentiation to estimate unknown parameters of your forward operator.

Consider the forward model

.. math::
    y = \noise{\forw{x, \theta}}

where :math:`N` is the noise model, :math:`\forw{\cdot, \theta}` is the forward operator, and the goal is to learn the parameter :math:`\theta` (e.g., the filter in :class:`deepinv.physics.Blur`).

In a typical blind inverse problem, given a measurement :math:`y`, we would like to recover both the underlying image :math:`x` and the operator parameter :math:`\theta`,
resulting in a highly ill-posed inverse problem.

In this example, we only focus on a much more simpler calibration problem: given the measurement :math:`y` and the ground truth :math:`x`, find the parameter :math:`\theta`.
This can be reformulated as the following optimization problem:

.. math::
    \min_{\theta} \frac{1}{2} \|\forw{x, \theta} - y \|^2

This problem can be addressed by first-order optimization if we can compute the gradient of the above function with respect to :math:`\theta`.
The dependence between the operator :math:`A` and the parameter :math:`\theta` can be complicated.
DeepInverse provides a wide range of physics operators, implemented as differentiable classes.
We can leverage the automatic differentiation engine provided in Pytorch to compute the gradient of the above loss function w.r.t. the physics parameters :math:`\theta`.

The purpose of this demo is to show how to use the physics classes in DeepInverse to estimate the physics parameters, together with the automatic differentiation.
We show 3 different ways to do this: manually implementing the projected gradient descent algorithm, using a Pytorch optimizer and optimizing the physics as a usual neural network.

.. GENERATED FROM PYTHON SOURCE LINES 34-36

Import required packages


.. GENERATED FROM PYTHON SOURCE LINES 36-44

.. code-block:: Python

    import deepinv as dinv
    import torch
    from tqdm import tqdm
    import matplotlib.pyplot as plt

    device = dinv.utils.get_device()
    dtype = torch.float32





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Selected GPU 0 with 4094.25 MiB free memory




.. GENERATED FROM PYTHON SOURCE LINES 45-50

Define the physics
------------------

In this first example, we use the convolution operator, defined in the :class:`deepinv.physics.Blur` class.
We also generate a random convolution kernel of motion blur

.. GENERATED FROM PYTHON SOURCE LINES 50-57

.. code-block:: Python


    generator = dinv.physics.generator.MotionBlurGenerator(
        psf_size=(25, 25), rng=torch.Generator(device), device=device
    )
    true_kernel = generator.step(1, seed=123)["filter"]
    physics = dinv.physics.Blur(noise_model=dinv.physics.GaussianNoise(0.02), device=device)








.. GENERATED FROM PYTHON SOURCE LINES 58-68

.. code-block:: Python

    x = dinv.utils.load_url_image(
        dinv.utils.get_image_url("celeba_example.jpg"),
        img_size=256,
        resize_mode="resize",
    ).to(device)

    y = physics(x, filter=true_kernel)

    dinv.utils.plot([x, y, true_kernel], titles=["Sharp", "Blurry", "True kernel"])




.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_001.png
   :alt: Sharp, Blurry, True kernel
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 69-79

Define an optimization algorithm
--------------------------------

The convolution kernel lives in the simplex, ie the kernel must have positive entries summing to 1.
We can use a simple optimization algorithm - Projected Gradient Descent - to enforce this constraint.
The following function allows one to compute the orthogonal projection onto the simplex, by a sorting algorithm
(Reference: `Large-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex
-- Mathieu Blondel, Akinori Fujino, and Naonori Ueda
<https://ieeexplore.ieee.org/document/6976941>`_)


.. GENERATED FROM PYTHON SOURCE LINES 79-102

.. code-block:: Python



    @torch.no_grad()
    def projection_simplex_sort(v: torch.Tensor) -> torch.Tensor:
        r"""
        Projects a tensor onto the simplex using a sorting algorithm.
        """
        shape = v.shape
        B = shape[0]
        v = v.view(B, -1)
        n_features = v.size(1)
        u = torch.sort(v, descending=True, dim=-1).values
        cssv = torch.cumsum(u, dim=-1) - 1.0
        ind = torch.arange(n_features, device=v.device)[None, :].expand(B, -1) + 1.0
        cond = u - cssv / ind > 0
        rho = ind[cond][-1]
        theta = cssv[cond][-1] / rho
        w = torch.maximum(v - theta, torch.zeros_like(v))
        return w.reshape(shape)


    # We also define a data fidelity term
    data_fidelity = dinv.optim.L2()







.. GENERATED FROM PYTHON SOURCE LINES 103-106

Run the algorithm

Initialize a constant kernel

.. GENERATED FROM PYTHON SOURCE LINES 107-137

.. code-block:: Python

    kernel_init = torch.zeros_like(true_kernel)
    kernel_init[..., 5:-5, 5:-5] = 1.0
    kernel_init = projection_simplex_sort(kernel_init)
    n_iter = 1000
    stepsize = 0.7

    kernel_hat = kernel_init
    losses = []
    for i in tqdm(range(n_iter)):
        # compute the gradient
        with torch.enable_grad():
            kernel_hat.requires_grad_(True)
            physics.update(filter=kernel_hat)
            loss = data_fidelity(y=y, x=x, physics=physics) / y.numel()
            loss.backward()
        grad = kernel_hat.grad

        # gradient step and projection step
        with torch.no_grad():
            kernel_hat = kernel_hat - stepsize * grad
            kernel_hat = projection_simplex_sort(kernel_hat)

        losses.append(loss.item())

    dinv.utils.plot(
        [true_kernel, kernel_init, kernel_hat],
        titles=["True kernel", "Init. kernel", "Estimated kernel"],
        suptitle="Result with Projected Gradient Descent",
    )




.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_002.png
   :alt: Result with Projected Gradient Descent, True kernel, Init. kernel, Estimated kernel
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/1000 [00:00<?, ?it/s]      3%|▎         | 31/1000 [00:00<00:03, 305.42it/s]      6%|▋         | 65/1000 [00:00<00:02, 323.91it/s]     10%|█         | 100/1000 [00:00<00:02, 334.31it/s]     14%|█▎        | 135/1000 [00:00<00:02, 339.17it/s]     17%|█▋        | 170/1000 [00:00<00:02, 342.33it/s]     20%|██        | 205/1000 [00:00<00:02, 343.62it/s]     24%|██▍       | 240/1000 [00:00<00:02, 344.92it/s]     28%|██▊       | 275/1000 [00:00<00:02, 344.60it/s]     31%|███       | 310/1000 [00:00<00:02, 343.81it/s]     34%|███▍      | 345/1000 [00:01<00:01, 343.85it/s]     38%|███▊      | 380/1000 [00:01<00:01, 344.80it/s]     42%|████▏     | 415/1000 [00:01<00:01, 345.73it/s]     45%|████▌     | 450/1000 [00:01<00:01, 344.50it/s]     48%|████▊     | 485/1000 [00:01<00:01, 345.78it/s]     52%|█████▏    | 520/1000 [00:01<00:01, 345.90it/s]     56%|█████▌    | 555/1000 [00:01<00:01, 345.69it/s]     59%|█████▉    | 590/1000 [00:01<00:01, 345.85it/s]     63%|██████▎   | 628/1000 [00:01<00:01, 355.57it/s]     66%|██████▋   | 664/1000 [00:01<00:00, 354.20it/s]     70%|███████   | 703/1000 [00:02<00:00, 363.35it/s]     74%|███████▍  | 740/1000 [00:02<00:00, 357.37it/s]     78%|███████▊  | 776/1000 [00:02<00:00, 354.43it/s]     81%|████████  | 812/1000 [00:02<00:00, 352.06it/s]     85%|████████▍ | 848/1000 [00:02<00:00, 350.94it/s]     88%|████████▊ | 884/1000 [00:02<00:00, 350.23it/s]     92%|█████████▏| 920/1000 [00:02<00:00, 350.55it/s]     96%|█████████▌| 956/1000 [00:02<00:00, 349.51it/s]     99%|█████████▉| 991/1000 [00:02<00:00, 348.64it/s]    100%|██████████| 1000/1000 [00:02<00:00, 347.27it/s]
    /local/jtachell/deepinv/deepinv/deepinv/utils/plotting.py:408: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.
      fig.subplots_adjust(top=0.75)




.. GENERATED FROM PYTHON SOURCE LINES 138-140

We can plot the loss to make sure that it decreases


.. GENERATED FROM PYTHON SOURCE LINES 141-149

.. code-block:: Python

    plt.figure()
    plt.plot(range(n_iter), losses)
    plt.title("Loss evolution")
    plt.yscale("log")
    plt.xlabel("Iteration")
    plt.tight_layout()
    plt.show()




.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_003.png
   :alt: Loss evolution
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 150-155

Combine with arbitrary optimizer
--------------------------------

Pytorch provides a wide range of optimizers for training neural networks.
We can also pick one of those to optimizer our parameter

.. GENERATED FROM PYTHON SOURCE LINES 155-189

.. code-block:: Python


    kernel_init = torch.zeros_like(true_kernel)
    kernel_init[..., 5:-5, 5:-5] = 1.0
    kernel_init = projection_simplex_sort(kernel_init)

    kernel_hat = kernel_init.clone()
    optimizer = torch.optim.Adam([kernel_hat], lr=0.1)

    # We will alternate a gradient step and a projection step
    losses = []
    n_iter = 200
    for i in tqdm(range(n_iter)):
        optimizer.zero_grad()
        # compute the gradient, this will directly change the gradient of `kernel_hat`
        with torch.enable_grad():
            kernel_hat.requires_grad_(True)
            physics.update(filter=kernel_hat)
            loss = data_fidelity(y=y, x=x, physics=physics) / y.numel()
            loss.backward()

        # a gradient step
        optimizer.step()
        # projection step, when doing additional steps, it's important to change only
        # the tensor data to avoid breaking the gradient computation
        kernel_hat.data = projection_simplex_sort(kernel_hat.data)
        # loss
        losses.append(loss.item())

    dinv.utils.plot(
        [true_kernel, kernel_init, kernel_hat],
        titles=["True kernel", "Init. kernel", "Estimated kernel"],
        suptitle="Result with ADAM",
    )




.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_004.png
   :alt: Result with ADAM, True kernel, Init. kernel, Estimated kernel
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/200 [00:00<?, ?it/s]     20%|██        | 40/200 [00:00<00:00, 397.51it/s]     40%|████      | 80/200 [00:00<00:00, 362.10it/s]     58%|█████▊    | 117/200 [00:00<00:00, 353.53it/s]     76%|███████▋  | 153/200 [00:00<00:00, 350.16it/s]     94%|█████████▍| 189/200 [00:00<00:00, 336.79it/s]    100%|██████████| 200/200 [00:00<00:00, 345.82it/s]
    /local/jtachell/deepinv/deepinv/deepinv/utils/plotting.py:408: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.
      fig.subplots_adjust(top=0.75)




.. GENERATED FROM PYTHON SOURCE LINES 190-192

We can plot the loss to make sure that it decreases


.. GENERATED FROM PYTHON SOURCE LINES 193-201

.. code-block:: Python

    plt.figure()
    plt.semilogy(range(n_iter), losses)
    plt.title("Loss evolution")
    plt.xlabel("Iteration")
    plt.tight_layout()
    plt.show()





.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_005.png
   :alt: Loss evolution
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 202-206

Optimizing the physics as a usual neural network
------------------------------------------------

Below we show another way to optimize the parameter of the physics, as we usually do for neural networks

.. GENERATED FROM PYTHON SOURCE LINES 207-254

.. code-block:: Python


    kernel_init = torch.zeros_like(true_kernel)
    kernel_init[..., 5:-5, 5:-5] = 1.0
    kernel_init = projection_simplex_sort(kernel_init)

    # The gradient is off by default, we need to enable the gradient of the parameter
    physics = dinv.physics.Blur(
        filter=kernel_init.clone().requires_grad_(True), device=device
    )

    # Set up the optimizer by giving the parameter to an optimizer
    # Try to change your favorite optimizer
    optimizer = torch.optim.AdamW([physics.filter], lr=0.1)


    # Try to change another loss function
    # loss_fn = torch.nn.MSELoss()
    loss_fn = torch.nn.L1Loss()

    # We will alternate a gradient step and a projection step
    losses = []
    n_iter = 100
    for i in tqdm(range(n_iter)):
        # update the gradient
        optimizer.zero_grad()
        y_hat = physics.A(x)
        loss = loss_fn(y_hat, y)
        loss.backward()

        # a gradient step
        optimizer.step()

        # projection step.
        # Note: when doing additional steps, it's important to change only
        # the tensor data to avoid breaking the gradient computation
        physics.filter.data = projection_simplex_sort(physics.filter.data)

        # loss
        losses.append(loss.item())

    kernel_hat = physics.filter.data
    dinv.utils.plot(
        [true_kernel, kernel_init, kernel_hat],
        titles=["True kernel", "Init. kernel", "Estimated kernel"],
        suptitle="Result with AdamW and L1 Loss",
    )




.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_006.png
   :alt: Result with AdamW and L1 Loss, True kernel, Init. kernel, Estimated kernel
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/100 [00:00<?, ?it/s]     74%|███████▍  | 74/100 [00:00<00:00, 737.26it/s]    100%|██████████| 100/100 [00:00<00:00, 736.27it/s]
    /local/jtachell/deepinv/deepinv/deepinv/utils/plotting.py:408: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.
      fig.subplots_adjust(top=0.75)




.. GENERATED FROM PYTHON SOURCE LINES 255-257

We can plot the loss to make sure that it decreases


.. GENERATED FROM PYTHON SOURCE LINES 258-264

.. code-block:: Python

    plt.figure()
    plt.semilogy(range(n_iter), losses)
    plt.title("Loss evolution")
    plt.xlabel("Iteration")
    plt.tight_layout()
    plt.show()



.. image-sg:: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_007.png
   :alt: Loss evolution
   :srcset: /auto_examples/blind-inverse-problems/images/sphx_glr_demo_optimizing_physics_parameter_007.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.971 seconds)


.. _sphx_glr_download_auto_examples_blind-inverse-problems_demo_optimizing_physics_parameter.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_optimizing_physics_parameter.ipynb <demo_optimizing_physics_parameter.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_optimizing_physics_parameter.py <demo_optimizing_physics_parameter.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_optimizing_physics_parameter.zip <demo_optimizing_physics_parameter.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
