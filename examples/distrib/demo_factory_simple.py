"""
Distributed Framework Factory API Demo
======================================

This example demonstrates the simplified factory API for DeepInverse's distributed framework.
The factory API reduces boilerplate code by using configuration objects and builder functions.

**Usage:**

.. code-block:: bash

    # Single process
    python examples/distrib/demo_factory_simple.py

    # Multi-process with torchrun
    torchrun --nproc_per_node=2 examples/distrib/demo_factory_simple.py

**Key Concepts:**

- **FactoryConfig**: Simplified configuration for physics, measurements, and data fidelity
- **TilingConfig**: Configuration for spatial tiling strategies
- **make_distrib_core**: Builder function that creates all distributed components
- **DistributedBundle**: Container for all distributed objects
"""

import torch
import deepinv as dinv
from pathlib import Path

# Import physics and loss components
from deepinv.physics import GaussianNoise, LinearPhysics
from deepinv.physics.blur import Blur
from deepinv.optim.data_fidelity import L2
from deepinv.optim.prior import PnP
from deepinv.loss.metric import PSNR

# Import the distributed framework
from deepinv.distrib.distrib_framework import DistributedContext

# Import the new factory API
from deepinv.distrib.utils import (
    FactoryConfig,
    TilingConfig,
    make_distrib_core,
    DistributedBundle,
)


def create_simple_physics_and_measurements(device: torch.device, image_size: tuple = (256, 256)):
    """
    Create simple physics operators and synthetic measurements for demonstration.
    
    :param torch.device device: Device to create operators on
    :param tuple image_size: Size of the test image (H, W)
    
    :returns: Tuple of (physics_list, measurements_list)
    :rtype: Tuple[List, List[torch.Tensor]]
    """
    # Create a simple synthetic image
    C, H, W = 1, *image_size
    clean_image = torch.randn(1, C, H, W, device=device)
    
    # Create simple blur kernels
    kernels = []
    
    # Gaussian blur
    gaussian_kernel = torch.zeros((1, 1, 5, 5), device=device)
    center = 2
    sigma = 1.0
    for i in range(5):
        for j in range(5):
            gaussian_kernel[0, 0, i, j] = torch.exp(
                torch.tensor(-((i - center) ** 2 + (j - center) ** 2) / (2 * sigma**2), device=device)
            )
    gaussian_kernel = gaussian_kernel / gaussian_kernel.sum()
    kernels.append(gaussian_kernel)
    
    # Motion blur
    motion_kernel = torch.zeros((1, 1, 5, 5), device=device)
    motion_kernel[0, 0, 2, :] = 1.0 / 5.0
    kernels.append(motion_kernel)
    
    # Create physics operators and measurements
    physics_list = []
    measurements_list = []
    
    for i, kernel in enumerate(kernels):
        # Create blur operator
        blur_op = Blur(filter=kernel, padding="circular")
        blur_op = blur_op.to(device)  # Ensure blur op is on correct device
        
        # Add noise
        noise_level = 0.05 * (1.0 + 0.2 * i)
        noise_op = GaussianNoise(sigma=noise_level)
        noise_op = noise_op.to(device)  # Ensure noise op is on correct device
        
        # Combine operators: blur followed by noise
        # We'll create a composite physics operator
        class CompositePhysics(LinearPhysics):
            def __init__(self, blur_op, noise_op):
                super().__init__()
                self.blur_op = blur_op
                self.noise_op = noise_op
            
            def A(self, x, **kwargs):
                return self.blur_op.A(x, **kwargs)
            
            def A_adjoint(self, y, **kwargs):
                return self.blur_op.A_adjoint(y, **kwargs)
                
            def forward(self, x, **kwargs):
                blurred = self.blur_op(x, **kwargs)
                return self.noise_op(blurred, **kwargs)
        
        physics = CompositePhysics(blur_op, noise_op)
        physics_list.append(physics)
        
        # Generate measurement
        measurement = physics(clean_image)
        measurements_list.append(measurement)
    
    return physics_list, measurements_list, clean_image


def run_distributed_pnp_factory_demo():
    """
    Demonstrate the distributed PnP algorithm using the Factory API.
    """
    
    print("=" * 70)
    print("üöÄ Distributed Framework Factory API Demo")
    print("=" * 70)
    
    # Configuration
    num_iterations = 10
    lr = 0.05
    denoiser_sigma = 0.02
    patch_size = 128
    receptive_field_radius = 32
    
    # Initialize Distributed Context
    with DistributedContext(sharding="round_robin", seed=42) as ctx:
        
        print(f"\nüìä Process Information:")
        print(f"  ‚Ä¢ World size: {ctx.world_size} process(es)")
        print(f"  ‚Ä¢ Current rank: {ctx.rank}")
        print(f"  ‚Ä¢ Device: {ctx.device}")
        print(f"  ‚Ä¢ Distributed: {ctx.is_dist}")
        
        # Create physics operators and measurements
        physics_list, measurements_list, clean_image = create_simple_physics_and_measurements(
            ctx.device, image_size=(128, 128)
        )
        B, C, H, W = clean_image.shape
        
        if ctx.rank == 0:
            print(f"\nüî¨ Created {len(physics_list)} physics operators")
            print(f"  ‚Ä¢ Image shape: {clean_image.shape}")
        
        # ==========================================
        # Factory API Configuration and Usage
        # ==========================================
        
        if ctx.rank == 0:
            print(f"\nüîß Setting up Factory API configuration...")
        
        # Create factory configuration
        factory_config = FactoryConfig(
            physics=physics_list,                    # List of physics operators
            measurements=measurements_list,          # List of measurements  
            data_fidelity=None,                     # Use default L2 data fidelity
        )

        # Create tiling configuration for the prior
        tiling_config = TilingConfig(
            patch_size=patch_size,
            receptive_field_radius=receptive_field_radius,
            overlap=True,
            strategy="smart_tiling",
        )
        
        # Create denoiser (simplified - just identity for demo)
        class IdentityDenoiser(torch.nn.Module):
            def forward(self, x, sigma=None):
                return x  # Identity denoiser for simplicity
        
        identity_prior = PnP(denoiser=IdentityDenoiser())
        
        if ctx.rank == 0:
            print(f"  ‚úÖ Factory configuration created")
            print(f"  ‚úÖ Tiling configuration: {patch_size}x{patch_size} patches")
        
        # ==========================================
        # Single Builder Call - This is the key benefit!
        # ==========================================
        
        if ctx.rank == 0:
            print(f"\nüèóÔ∏è Building all distributed components with single call...")
        
        # This single call creates ALL distributed components!
        distributed_bundle = make_distrib_core(
            ctx,
            factory_config=factory_config,
            signal_shape=(B, C, H, W),
            reduction="mean",
            prior=identity_prior,
            tiling=tiling_config,
        )
        
        # Extract components from bundle
        distributed_physics = distributed_bundle.physics
        distributed_measurements = distributed_bundle.measurements  
        distributed_df = distributed_bundle.df
        distributed_signal = distributed_bundle.signal
        distributed_prior = distributed_bundle.prior
        
        if ctx.rank == 0:
            print(f"  ‚úÖ Distributed physics: {len(distributed_physics.local_idx)} local operators")
            print(f"  ‚úÖ Distributed measurements: {len(distributed_measurements.local)} local measurements")
            print(f"  ‚úÖ Distributed signal: shape {distributed_signal.shape}")
            print(f"  ‚úÖ Distributed data fidelity: ready")
            print(f"  ‚úÖ Distributed prior: ready")
        
        # ==========================================
        # Run Distributed PnP Algorithm
        # ==========================================
        
        if ctx.rank == 0:
            print(f"\nüîÑ Running distributed PnP algorithm ({num_iterations} iterations)...")
        
        psnr_metric = PSNR()
        psnr_history = []
        
        # Initialize signal with zeros
        distributed_signal.update_(torch.zeros_like(clean_image))
        
        with torch.no_grad():
            for it in range(num_iterations):
                # Data fidelity gradient step (distributed across processes)
                grad = distributed_df.grad(distributed_signal)
                
                # Gradient step
                new_data = distributed_signal.data - lr * grad
                distributed_signal.update_(new_data)
                
                # Denoising step (if prior is available)
                if distributed_prior is not None:
                    denoised = distributed_prior.prox(
                        distributed_signal, sigma_denoiser=denoiser_sigma
                    )
                    distributed_signal.update_(denoised)
                
                # Compute PSNR (only on rank 0)
                if ctx.rank == 0:
                    psnr_val = psnr_metric(
                        distributed_signal.data, clean_image
                    ).item()
                    psnr_history.append(psnr_val)
                    
                    if it == 0 or (it + 1) % 2 == 0:
                        print(f"  Iteration {it+1}/{num_iterations}, PSNR: {psnr_val:.2f} dB")
        
        # ==========================================
        # Results Summary  
        # ==========================================
        
        if ctx.rank == 0:
            print(f"\nüìä Results Summary:")
            print(f"  ‚Ä¢ Final PSNR: {psnr_history[-1]:.2f} dB")
            print(f"  ‚Ä¢ Reconstruction completed successfully!")
            
            print(f"\nüéØ Factory API Benefits Demonstrated:")
            print(f"  ‚Ä¢ ‚úÖ Single builder call instead of manual factory functions")
            print(f"  ‚Ä¢ ‚úÖ Configuration-driven approach")  
            print(f"  ‚Ä¢ ‚úÖ Type-safe configuration objects")
            print(f"  ‚Ä¢ ‚úÖ Reduced boilerplate code")
            print(f"  ‚Ä¢ ‚úÖ Easy to modify and reuse configurations")
            
            print(f"\n‚úÖ Factory API demo completed successfully!")
            
            # Show comparison with manual approach
            print(f"\nüìù Comparison with Manual Approach:")
            print(f"  Manual: ~20+ lines of factory function definitions")
            print(f"  Factory API: ~5 lines of configuration + 1 builder call")
            print(f"  Code reduction: ~75% fewer lines!")


if __name__ == "__main__":
    run_distributed_pnp_factory_demo()
    
    print(f"\n" + "="*70)
    print(f"üéâ Factory API Demo Complete!")
    print(f"="*70)
    print(f"""
Key Takeaways:

1. **FactoryConfig**: Define physics, measurements, and data fidelity
2. **TilingConfig**: Configure spatial processing strategies  
3. **make_distrib_core**: Single call to create all distributed objects
4. **DistributedBundle**: Convenient container for all components

Next steps:
- Try your own physics operators and priors
- Experiment with different tiling configurations
- Scale to multiple processes with torchrun
""")