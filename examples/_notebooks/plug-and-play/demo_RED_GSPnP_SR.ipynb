{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afcdde0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Regularization by Denoising (RED) for Super-Resolution.\n",
    "====================================================================================================\n",
    "\n",
    "Implementation of :footcite:t:`romano2017little` using as plug-in denoiser the Gradient-Step Denoiser (GSPnP) :footcite:t:`hurault2021gradient` which provides an explicit prior.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import deepinv as dinv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.optim.prior import RED\n",
    "from deepinv.optim.optimizers import optim_builder\n",
    "from deepinv.training import test\n",
    "from torchvision import transforms\n",
    "from deepinv.utils.parameters import get_GSPnP_params\n",
    "from deepinv.utils import load_dataset, load_degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da39e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths for data loading and results.\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"measurements\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "DEG_DIR = BASE_DIR / \"degradations\"\n",
    "\n",
    "# Set the global random seed from pytorch to ensure\n",
    "# the reproducibility of the example.\n",
    "torch.manual_seed(0)\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base image datasets and degradation operators.\n",
    "# --------------------------------------------------------------------------------\n",
    "# In this example, we use the Set3C dataset and a motion blur kernel from :footcite:t:`levin2009understanding`.\n",
    "\n",
    "dataset_name = \"set3c\"\n",
    "img_size = 256 if torch.cuda.is_available() else 32\n",
    "operation = \"super-resolution\"\n",
    "val_transform = transforms.Compose(\n",
    "    [transforms.CenterCrop(img_size), transforms.ToTensor()]\n",
    ")\n",
    "dataset = load_dataset(dataset_name, transform=val_transform)\n",
    "\n",
    "# Generate the degradation operator.\n",
    "kernel_index = 1\n",
    "kernel_torch = load_degradation(\n",
    "    \"kernels_12.npy\", DEG_DIR / \"kernels\", index=kernel_index\n",
    ")\n",
    "kernel_torch = kernel_torch.unsqueeze(0).unsqueeze(\n",
    "    0\n",
    ")  # add batch and channel dimensions\n",
    "\n",
    "# Use parallel dataloader if using a GPU to speed up training, otherwise, as all computes are on CPU, use synchronous dataloading.\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "factor = 2  # down-sampling factor\n",
    "n_channels = 3  # 3 for color images, 1 for gray-scale images\n",
    "n_images_max = 3  # Maximal number of images to restore from the input dataset\n",
    "noise_level_img = 0.03  # Gaussian Noise standard deviation for the degradation\n",
    "p = dinv.physics.Downsampling(\n",
    "    img_size=(n_channels, img_size, img_size),\n",
    "    factor=factor,\n",
    "    filter=kernel_torch,\n",
    "    device=device,\n",
    "    noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),\n",
    ")\n",
    "\n",
    "# Generate a dataset in a HDF5 folder in \"{dir}/dinv_dataset0.h5'\" and load it.\n",
    "measurement_dir = DATA_DIR / dataset_name / operation\n",
    "dinv_dataset_path = dinv.datasets.generate_dataset(\n",
    "    train_dataset=dataset,\n",
    "    test_dataset=None,\n",
    "    physics=p,\n",
    "    device=device,\n",
    "    save_dir=measurement_dir,\n",
    "    train_datapoints=n_images_max,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "dataset = dinv.datasets.HDF5Dataset(path=dinv_dataset_path, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6902cfd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Setup the PnP algorithm. This involves in particular the definition of a custom prior class.\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# We use the proximal gradient algorithm to solve the super-resolution problem with GSPnP.\n",
    "\n",
    "# Parameters of the algorithm to solve the inverse problem\n",
    "early_stop = True  # Stop algorithm when convergence criteria is reached\n",
    "crit_conv = \"cost\"  # Convergence is reached when the difference of cost function between consecutive iterates is\n",
    "# smaller than thres_conv\n",
    "thres_conv = 1e-5\n",
    "backtracking = True\n",
    "use_bicubic_init = False  # Use bicubic interpolation to initialize the algorithm\n",
    "batch_size = 1  # batch size for evaluation is necessarily 1 for early stopping and backtracking to work.\n",
    "\n",
    "# load specific parameters for GSPnP\n",
    "lamb, sigma_denoiser, stepsize, max_iter = get_GSPnP_params(operation, noise_level_img)\n",
    "\n",
    "params_algo = {\n",
    "    \"stepsize\": stepsize,\n",
    "    \"g_param\": sigma_denoiser,\n",
    "    \"lambda\": lamb,\n",
    "}\n",
    "\n",
    "# Select the data fidelity term\n",
    "data_fidelity = L2()\n",
    "\n",
    "\n",
    "# The GSPnP prior corresponds to a RED prior with an explicit `g`.\n",
    "# We thus write a class that inherits from RED for this custom prior.\n",
    "class GSPnP(RED):\n",
    "    r\"\"\"\n",
    "    Gradient-Step Denoiser prior.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.explicit_prior = True\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        r\"\"\"\n",
    "        Computes the prior :math:`g(x)`.\n",
    "\n",
    "        :param torch.tensor x: Variable :math:`x` at which the prior is computed.\n",
    "        :return: (torch.tensor) prior :math:`g(x)`.\n",
    "        \"\"\"\n",
    "        return self.denoiser.potential(x, *args, **kwargs)\n",
    "\n",
    "\n",
    "method = \"GSPnP\"\n",
    "denoiser_name = \"gsdrunet\"\n",
    "# Specify the Denoising prior\n",
    "prior = GSPnP(denoiser=dinv.models.GSDRUNet(pretrained=\"download\").to(device))\n",
    "\n",
    "\n",
    "# we want to output the intermediate PGD update to finish with a denoising step.\n",
    "def custom_output(X):\n",
    "    return X[\"est\"][1]\n",
    "\n",
    "\n",
    "# instantiate the algorithm class to solve the IP problem.\n",
    "model = optim_builder(\n",
    "    iteration=\"PGD\",\n",
    "    prior=prior,\n",
    "    g_first=True,\n",
    "    data_fidelity=data_fidelity,\n",
    "    params_algo=params_algo,\n",
    "    early_stop=early_stop,\n",
    "    max_iter=max_iter,\n",
    "    crit_conv=crit_conv,\n",
    "    thres_conv=thres_conv,\n",
    "    backtracking=backtracking,\n",
    "    get_output=custom_output,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode. We do not require training here.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the problem.\n",
    "# ----------------------------------------------------\n",
    "# We evaluate the PnP algorithm on the test dataset, compute the PSNR metrics and plot reconstruction results.\n",
    "\n",
    "save_folder = RESULTS_DIR / method / operation / dataset_name\n",
    "plot_convergence_metrics = True  # plot metrics. Metrics are saved in save_folder.\n",
    "plot_images = True  # plot images. Images are saved in save_folder.\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
    ")\n",
    "\n",
    "test(\n",
    "    model=model,\n",
    "    test_dataloader=dataloader,\n",
    "    physics=p,\n",
    "    device=device,\n",
    "    plot_images=plot_images,\n",
    "    save_folder=RESULTS_DIR / method / operation / dataset_name,\n",
    "    plot_convergence_metrics=plot_convergence_metrics,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
