{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Learned Iterative Soft-Thresholding Algorithm (LISTA) for compressed sensing\n",
    "====================================================================================================\n",
    "\n",
    "This example shows how to implement the LISTA algorithm :footcite:t:`gregor2010learning`,\n",
    "for a compressed sensing problem. In a nutshell, LISTA is an unfolded proximal gradient algorithm involving a\n",
    "soft-thresholding proximal operator with learnable thresholding parameters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import deepinv as dinv\n",
    "from torch.utils.data import DataLoader\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.unfolded import unfolded_builder\n",
    "from deepinv.utils import get_data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f156aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths for data loading and results.\n",
    "# -----------------------------------------\n",
    "#\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"measurements\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "CKPT_DIR = BASE_DIR / \"ckpts\"\n",
    "ORIGINAL_DATA_DIR = get_data_home()\n",
    "\n",
    "# Set the global random seed from pytorch to ensure reproducibility of the example.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1e4c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load base image datasets and degradation operators.\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# In this example, we use MNIST as the base dataset.\n",
    "\n",
    "img_size = 28\n",
    "n_channels = 1\n",
    "operation = \"compressed-sensing\"\n",
    "train_dataset_name = \"MNIST_train\"\n",
    "\n",
    "# Generate training and evaluation datasets in HDF5 folders and load them.\n",
    "train_test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_base_dataset = datasets.MNIST(\n",
    "    root=ORIGINAL_DATA_DIR, train=True, transform=train_test_transform, download=True\n",
    ")\n",
    "test_base_dataset = datasets.MNIST(\n",
    "    root=ORIGINAL_DATA_DIR, train=False, transform=train_test_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a00c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of compressed measurements and load it.\n",
    "# ----------------------------------------------------------------------------\n",
    "# We use the compressed sensing class from the physics module to generate a dataset of highly-compressed measurements\n",
    "# (10% of the total number of pixels).\n",
    "#\n",
    "# The forward operator is defined as :math:`y = Ax`\n",
    "# where :math:`A` is a (normalized) random Gaussian matrix.\n",
    "\n",
    "\n",
    "# Use parallel dataloader if using a GPU to speed up training, otherwise, as all computes are on CPU, use synchronous\n",
    "# data loading.\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Generate the compressed sensing measurement operator with 10x under-sampling factor.\n",
    "physics = dinv.physics.CompressedSensing(\n",
    "    m=78, img_size=(n_channels, img_size, img_size), fast=True, device=device\n",
    ")\n",
    "my_dataset_name = \"demo_LISTA\"\n",
    "n_images_max = (\n",
    "    1000 if torch.cuda.is_available() else 200\n",
    ")  # maximal number of images used for training\n",
    "measurement_dir = DATA_DIR / train_dataset_name / operation\n",
    "generated_datasets_path = dinv.datasets.generate_dataset(\n",
    "    train_dataset=train_base_dataset,\n",
    "    test_dataset=test_base_dataset,\n",
    "    physics=physics,\n",
    "    device=device,\n",
    "    save_dir=measurement_dir,\n",
    "    train_datapoints=n_images_max,\n",
    "    test_datapoints=8,\n",
    "    num_workers=num_workers,\n",
    "    dataset_filename=str(my_dataset_name),\n",
    ")\n",
    "\n",
    "train_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)\n",
    "test_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c47c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unfolded Proximal Gradient algorithm.\n",
    "# ------------------------------------------------------------------------\n",
    "# In this example, following the original LISTA algorithm :footcite:t:`gregor2010learning`\n",
    "# the backbone algorithm we unfold is the proximal gradient algorithm which minimizes the following objective function\n",
    "#\n",
    "# .. math::\n",
    "#          \\begin{equation}\n",
    "#          \\tag{1}\n",
    "#          \\min_x \\frac{1}{2} \\|y - Ax\\|_2^2 + \\lambda \\|Wx\\|_1\n",
    "#          \\end{equation}\n",
    "#\n",
    "# where :math:`\\lambda` is the regularization parameter.\n",
    "# The proximal gradient iteration (see also :class:`deepinv.optim.optim_iterators.PGDIteration`) is defined as\n",
    "#\n",
    "#   .. math::\n",
    "#           x_{k+1} = \\text{prox}_{\\gamma \\lambda g}(x_k - \\gamma A^T (Ax_k - y))\n",
    "#\n",
    "# where :math:`\\gamma` is the stepsize and :math:`\\text{prox}_{g}` is the proximity operator of :math:`g(x) = \\|Wx\\|_1`\n",
    "# which corresponds to soft-thresholding with a wavelet basis (see :class:`deepinv.optim.WaveletPrior`).\n",
    "#\n",
    "# We use :func:`deepinv.unfolded.unfolded_builder` to define the unfolded algorithm\n",
    "# and set both the stepsizes of the LISTA algorithm :math:`\\gamma` (``stepsize``) and the soft\n",
    "# thresholding parameters :math:`\\lambda` as learnable parameters.\n",
    "# These parameters are initialized with a table of length max_iter,\n",
    "# yielding a distinct ``stepsize`` value for each iteration of the algorithm.\n",
    "\n",
    "# Select the data fidelity term\n",
    "data_fidelity = L2()\n",
    "max_iter = 30 if torch.cuda.is_available() else 10  # Number of unrolled iterations\n",
    "stepsize = [torch.ones(1, device=device)] * max_iter  # initialization of the stepsizes.\n",
    "# A distinct stepsize is trained for each iteration.\n",
    "\n",
    "# Set up the trainable denoising prior; here, the soft-threshold in a wavelet basis.\n",
    "# If the prior is initialized with a list of length max_iter,\n",
    "# then a distinct weight is trained for each PGD iteration.\n",
    "# For fixed trained model prior across iterations, initialize with a single model.\n",
    "level = 3\n",
    "prior = [\n",
    "    dinv.optim.WaveletPrior(wv=\"db8\", level=level, device=device)\n",
    "    for i in range(max_iter)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da9595",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#\n",
    "# In practice, it is common to apply a different thresholding parameter for each wavelet sub-band. This means that\n",
    "# the thresholding parameter is a tensor of shape (n_levels, n_wavelet_subbands) and the associated problem (1) is\n",
    "# reformulated as\n",
    "#\n",
    "# .. math::\n",
    "#          \\begin{equation}\n",
    "#          \\min_x \\frac{1}{2} \\|y - Ax\\|_2^2 +  \\sum_{i, j} \\lambda_{i, j} \\|\\left(Wx\\right)_{i, j}\\|_1\n",
    "#          \\end{equation}\n",
    "#\n",
    "# where :math:`\\lambda_{i, j}` is the thresholding parameter for the wavelet sub-band :math:`j` at level :math:`i`.\n",
    "# Note that in this case, the prior is a list of elements containing the terms :math:`\\|\\left(Wx\\right)_{i, j}\\|_1=g_{i, j}(x)`,\n",
    "# and that it is necessary that the dimension of the thresholding parameter matches that of :math:`g_{i, j}`.\n",
    "\n",
    "# Unrolled optimization algorithm parameters\n",
    "lamb = [\n",
    "    torch.ones(1, 3, 3, device=device)\n",
    "    * 0.01  # initialization of the regularization parameter. One thresholding parameter per wavelet sub-band and level.\n",
    "] * max_iter  # A distinct lamb is trained for each iteration.\n",
    "\n",
    "\n",
    "params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary\n",
    "    \"stepsize\": stepsize,\n",
    "    \"lambda\": lamb,\n",
    "}\n",
    "\n",
    "trainable_params = [\n",
    "    \"stepsize\",\n",
    "    \"lambda\",\n",
    "]  # define which parameters from 'params_algo' are trainable\n",
    "\n",
    "# Define the unfolded trainable model.\n",
    "model = unfolded_builder(\n",
    "    iteration=\"PGD\",\n",
    "    params_algo=params_algo.copy(),\n",
    "    trainable_params=trainable_params,\n",
    "    data_fidelity=data_fidelity,\n",
    "    max_iter=max_iter,\n",
    "    prior=prior,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters.\n",
    "# -------------------------------\n",
    "#\n",
    "# We now define training-related parameters,\n",
    "# number of epochs, optimizer (Adam) and its hyperparameters, and the train and test batch sizes.\n",
    "#\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5 if torch.cuda.is_available() else 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Choose optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Choose supervised training loss\n",
    "losses = [dinv.loss.SupLoss(metric=dinv.metric.MSE())]\n",
    "\n",
    "# Logging parameters\n",
    "verbose = True\n",
    "\n",
    "# Batch sizes and data loaders\n",
    "train_batch_size = 64 if torch.cuda.is_available() else 1\n",
    "test_batch_size = 64 if torch.cuda.is_available() else 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network.\n",
    "# -------------------------------------------\n",
    "#\n",
    "# We train the network using the :class:`deepinv.Trainer` class.\n",
    "#\n",
    "\n",
    "trainer = dinv.Trainer(\n",
    "    model,\n",
    "    physics=physics,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    epochs=epochs,\n",
    "    losses=losses,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    save_path=str(CKPT_DIR / operation),\n",
    "    verbose=verbose,\n",
    "    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n",
    ")\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network.\n",
    "# ---------------------------\n",
    "#\n",
    "# We now test the learned unrolled network on the test dataset. In the plotted results, the `Linear` column shows the\n",
    "# measurements back-projected in the image domain, the `Recons` column shows the output of our LISTA network,\n",
    "# and `GT` shows the ground truth.\n",
    "#\n",
    "\n",
    "\n",
    "trainer.test(test_dataloader)\n",
    "\n",
    "test_sample, _ = next(iter(test_dataloader))\n",
    "model.eval()\n",
    "test_sample = test_sample.to(device)\n",
    "\n",
    "# Get the measurements and the ground truth\n",
    "y = physics(test_sample)\n",
    "with torch.no_grad():  # it is important to disable gradient computation during testing.\n",
    "    rec = model(y, physics=physics)\n",
    "\n",
    "backprojected = physics.A_adjoint(y)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    [backprojected, rec, test_sample],\n",
    "    titles=[\"Linear\", \"Reconstruction\", \"Ground truth\"],\n",
    "    suptitle=\"Reconstruction results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learned parameters.\n",
    "# ------------------------------------\n",
    "dinv.utils.plotting.plot_parameters(\n",
    "    model, init_params=params_algo, save_dir=RESULTS_DIR / \"unfolded_pgd\" / operation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f067e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
