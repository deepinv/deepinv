{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b805d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Reducing the memory and computational complexity of unfolded network training\n",
    "====================================================================================================\n",
    "\n",
    "Some unfolded architectures rely on a :func:`least-squares solver <deepinv.optim.utils.least_squares>` to compute the proximal step w.r.t. the data-fidelity term (e.g., :class:`ADMM <deepinv.optim.optim_iterators.ADMMIteration>` or :class:`HQS <deepinv.optim.optim_iterators.HQSIteration>`):  \n",
    "\n",
    ".. math::  \n",
    "\n",
    "     \\operatorname{prox}_{\\gamma f}(z)  = \\underset{x}{\\arg\\min} \\; \\frac{\\gamma}{2}\\|A_\\theta x-y\\|^2 + \\frac{1}{2}\\|x-z\\|^2  \n",
    "\n",
    "During backpropagation, a naive implementation requires storing the gradients of every intermediate step of the least squares solver (which is an iterative method), resulting in significant memory and computational costs which are proportional to number of iterations done by the solver.  \n",
    "\n",
    "The library provides a memory-efficient back-propagation strategy that reduces the memory footprint during training, by computing the gradients of the proximal step in closed-form, without storing any intermediate steps. This closed-form computation requires evaluating the least-squares solver one additional time during the gradient computation.  \n",
    "\n",
    "Let :math:`h(z, y, \\theta, \\gamma) = \\operatorname{prox}_{\\gamma f}(z)` be the proximal operator. During the backward pass, we need to compute the vector-Jacobian products (VJPs), w.r.t the input variables :math:`(z, y, \\theta, \\gamma)` required for backpropagation:\n",
    "\n",
    ".. math::\n",
    "\n",
    "    \\left( \\frac{\\partial h}{\\partial z} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial y} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial \\theta} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial \\gamma} \\right)^{\\top} v\n",
    "\n",
    "and :math:`v` is the upstream gradient. The VJPs can be computed in closed-form by solving a second least-squares problem, as shown in the following. \n",
    "When the forward least-squares solver converges to the exact minimizer, we have the following closed-form expressions for :math:`h(z, y, \\theta, \\gamma)`:\n",
    "\n",
    ".. math::\n",
    "\n",
    "    h(z, y, \\theta, \\gamma) = \\left( A_\\theta^{\\top} A_\\theta + \\frac{1}{\\gamma} I \\right)^{-1} \\left( A_\\theta^{\\top} y + \\frac{1}{\\gamma} z \\right)\n",
    "\n",
    "Let :math:`M` denote the inverse :math:`\\left( A_\\theta^T A_\\theta + \\frac{1}{\\gamma} I \\right)^{-1}`. The VJPs can be computed as follows:\n",
    "\n",
    ".. math::\n",
    "\n",
    "    \\left( \\frac{\\partial h}{\\partial z} \\right)^{\\top} v               &= \\frac{1}{\\gamma} M v \\\\\n",
    "    \\left( \\frac{\\partial h}{\\partial y} \\right)^{\\top} v               &= A_\\theta M v \\\\\n",
    "    \\left( \\frac{\\partial h}{\\partial \\gamma} \\right)^{\\top} v          &=   (h - z)^\\top M  v / \\gamma^2 \\\\\n",
    "    \\left( \\frac{\\partial h}{\\partial \\theta} \\right)^{\\top} v          &= \\frac{\\partial p}{\\partial \\theta} \n",
    "    \n",
    "where :math:`p =  (y - A_\\theta h)^{\\top} A_\\theta M v` and :math:`\\frac{\\partial p}{\\partial \\theta}` can be computed using the standard backpropagation mechanism (autograd).\n",
    "\n",
    ".. note::\n",
    "\n",
    "    Linear forward operators that have a :class:`closed-form singular value decomposition <deepinv.physics.DecomposablePhysics>` benefit from a :func:`closed-form formula <deepinv.physics.DecomposablePhysics.prox_l2>` for computing the proximal step, and thus we shouldn't expect speed-ups in these specific cases.  \n",
    "\n",
    "\n",
    "This example shows how to train an unfolded neural network with a memory complexity that is independent of the number of iterations in least squares solver (O(1) memory complexity) used for computing the data-fidelity proximal step.\n",
    "\n",
    ".. note::\n",
    "\n",
    "    By default, this example is run on CPU so we should not expect significant speed-ups and we can not trace the memory usage precisely. For a better experience, we recommend running the example on a machine with a GPU. In such a case, we can expect significant speed-ups (20-50%) and a significant reduction in memory usage (2x-3x reduction factor). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4e236",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.optim.prior import PnP\n",
    "from deepinv.unfolded import unfolded_builder\n",
    "from torchvision import transforms\n",
    "from deepinv.utils.demo import load_dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    dinv.utils.get_freer_gpu() if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "dtype = torch.float32\n",
    "img_size = 64 if torch.cuda.is_available() else 32\n",
    "num_images = 480 if torch.cuda.is_available() else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe876e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the degradation operator and the dataset.\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# We use the Blur class with `valid` padding from the physics module. ITs proximal operator does not\n",
    "# have a closed-form solution, and thus requires using a least-squares solver.\n",
    "\n",
    "# In this example, we use the CBSD500 dataset\n",
    "train_dataset_name = \"CBSD500\"\n",
    "\n",
    "# Specify the transforms to be applied to the input images.\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomCrop(img_size), transforms.ToTensor()]\n",
    ")\n",
    "# Define the base train dataset of clean images.\n",
    "train_dataset = load_dataset(train_dataset_name, transform=train_transform)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(num_images))\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32 if torch.cuda.is_available() else 8,\n",
    "    num_workers=4 if torch.cuda.is_available() else 0,\n",
    "    shuffle=True,\n",
    ")\n",
    "physics = dinv.physics.Blur(\n",
    "    filter=dinv.physics.blur.gaussian_blur(sigma=(2.5, 2.5)),\n",
    "    padding=\"valid\",\n",
    "    device=device,\n",
    "    noise_model=dinv.physics.GaussianNoise(sigma=0.1),\n",
    "    max_iter=100,\n",
    "    tol=1e-8,\n",
    "    implicit_backward_solver=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb068bc2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the unfolded parameters.\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# Unrolled optimization algorithm parameters\n",
    "max_iter = 5  # number of unfolded layers\n",
    "\n",
    "# Select the data fidelity term\n",
    "data_fidelity = L2()\n",
    "stepsize = [1] * max_iter  # stepsize of the algorithm\n",
    "sigma_denoiser = [0.01] * max_iter  # noise level parameter of the denoiser\n",
    "params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary\n",
    "    \"stepsize\": stepsize,\n",
    "    \"g_param\": sigma_denoiser,\n",
    "}\n",
    "trainable_params = [\n",
    "    \"g_param\",\n",
    "    \"stepsize\",\n",
    "]  # define which parameters from 'params_algo' are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Here we write explicitly the training loop to show how implicit differentiation can be used to avoid out-of-memory issues and sometimes accelerate training. But you can also use the :class:`deepinv.Trainer` class as shown in other examples.\n",
    "\n",
    "\n",
    "# Some helper functions for measuring memory usage\n",
    "use_cuda = device.type == \"cuda\" and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def sync():\n",
    "    if use_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def reset_memory():\n",
    "    if use_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "def peak_memory():\n",
    "    if use_cuda:\n",
    "        peak_bytes = int(torch.cuda.max_memory_allocated(device=device))\n",
    "    else:  # Stats on CPU is not very accurate\n",
    "        peak_bytes = 0\n",
    "    return peak_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22c7f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We first train the model will full backpropagation to compare the memory usage.\n",
    "# Define the unfolded trainable model.\n",
    "torch.manual_seed(42)  # Make sure that we have the same initialization for both runs\n",
    "prior = PnP(denoiser=dinv.models.DnCNN(depth=7, pretrained=None).to(device))\n",
    "model = unfolded_builder(\n",
    "    iteration=\"HQS\",\n",
    "    params_algo=params_algo.copy(),\n",
    "    trainable_params=trainable_params,\n",
    "    data_fidelity=data_fidelity,\n",
    "    max_iter=max_iter,\n",
    "    prior=prior,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-8)\n",
    "model.train()\n",
    "\n",
    "# Setting this parameter to False to use full backpropagation\n",
    "physics.implicit_backward_solver = False\n",
    "\n",
    "reset_memory()\n",
    "sync()\n",
    "start = time.perf_counter()\n",
    "auto_losses = []\n",
    "for x in train_loader:\n",
    "    x = x.to(device)\n",
    "    y = physics(x)\n",
    "    optimizer.zero_grad()\n",
    "    x_hat = model(physics=physics, y=y)\n",
    "    loss = torch.nn.functional.mse_loss(x_hat, x)\n",
    "    auto_losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "sync()\n",
    "end = time.perf_counter()\n",
    "auto_peak_memory_mb = peak_memory() / (10**6)\n",
    "auto_time_per_iter = (end - start) / len(train_loader)\n",
    "auto_avg_loss = np.array(auto_losses)\n",
    "auto_avg_loss = np.cumsum(auto_avg_loss) / (np.arange(len(auto_avg_loss)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now train the model using the closed-form gradients of the proximal step.\n",
    "# We can do this by setting `implicit_backward_solver` to `True`.\n",
    "#\n",
    "\n",
    "physics.implicit_backward_solver = True\n",
    "\n",
    "# Define the unfolded trainable model.\n",
    "torch.manual_seed(42)  # Make sure that we have the same initialization for both runs\n",
    "prior = PnP(denoiser=dinv.models.DnCNN(depth=7, pretrained=None).to(device))\n",
    "model = unfolded_builder(\n",
    "    iteration=\"HQS\",\n",
    "    params_algo=params_algo.copy(),\n",
    "    trainable_params=trainable_params,\n",
    "    data_fidelity=data_fidelity,\n",
    "    max_iter=max_iter,\n",
    "    prior=prior,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-8)\n",
    "model.train()\n",
    "\n",
    "reset_memory()\n",
    "sync()\n",
    "start = time.perf_counter()\n",
    "implicit_losses = []\n",
    "for x in train_loader:\n",
    "    x = x.to(device)\n",
    "    y = physics(x)\n",
    "    optimizer.zero_grad()\n",
    "    x_hat = model(physics=physics, y=y)\n",
    "    loss = torch.nn.functional.mse_loss(x_hat, x)\n",
    "    implicit_losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "sync()\n",
    "end = time.perf_counter()\n",
    "implicit_peak_memory_mb = peak_memory() / (10**6)\n",
    "implicit_time_per_iter = (end - start) / len(train_loader)\n",
    "implicit_avg_loss = np.array(implicit_losses)\n",
    "implicit_avg_loss = np.cumsum(implicit_avg_loss) / (\n",
    "    np.arange(len(implicit_avg_loss)) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da688a17",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Compare the memory usage\n",
    "# ----------------------------------------------------------------------------------------\n",
    "print(f\"Full backpropagation: time per iteration: {auto_time_per_iter:.2f} s. \")\n",
    "print(f\"Implicit differentiation: time per iteration: {implicit_time_per_iter:.2f} s.\")\n",
    "\n",
    "# Compare the memory usage\n",
    "if use_cuda:\n",
    "    print(f\"Full backpropagation: peak memory usage: {auto_peak_memory_mb:.1f} MB\")\n",
    "    print(\n",
    "        f\"Implicit differentiation: peak memory usage: {implicit_peak_memory_mb:.1f} MB\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Memory reduction factor: {auto_peak_memory_mb/implicit_peak_memory_mb:.1f}x\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Compare the training loss\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(auto_avg_loss, label=\"Full backpropagation\", linestyle=\"--\", linewidth=2)\n",
    "plt.plot(\n",
    "    implicit_avg_loss, label=\"Implicit differentiation\", linestyle=\"-.\", linewidth=1.5\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.ylabel(\"Training loss (MSE)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    f\"Training loss. Avg loss difference: {np.mean(np.abs(auto_avg_loss - implicit_avg_loss)):.2e}\",\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa7197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
