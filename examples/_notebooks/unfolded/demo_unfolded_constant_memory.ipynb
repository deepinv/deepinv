{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reducing the memory and computational complexity of unfolded network training\n\nSome unfolded architectures rely on a :func:`least-squares solver <deepinv.optim.utils.least_squares>` to compute the proximal step w.r.t. the data-fidelity term (e.g., :class:`ADMM <deepinv.optim.optim_iterators.ADMMIteration>` or :class:`HQS <deepinv.optim.optim_iterators.HQSIteration>`):  \n\n\\begin{align}\\operatorname{prox}_{\\gamma f}(z)  = \\underset{x}{\\arg\\min} \\; \\frac{\\gamma}{2}\\|A_\\theta x-y\\|^2 + \\frac{1}{2}\\|x-z\\|^2\\end{align}\n\nDuring backpropagation, a naive implementation requires storing the gradients of every intermediate step of the least squares solver (which is an iterative method), resulting in significant memory and computational costs which are proportional to number of iterations done by the solver.  \n\nThe library provides a memory-efficient back-propagation strategy that reduces the memory footprint during training, by computing the gradients of the proximal step in closed-form, without storing any intermediate steps. This closed-form computation requires evaluating the least-squares solver one additional time during the gradient computation.  \n\nLet $h(z, y, \\theta, \\gamma) = \\operatorname{prox}_{\\gamma f}(z)$ be the proximal operator. During the backward pass, we need to compute the vector-Jacobian products (VJPs), w.r.t the input variables $(z, y, \\theta, \\gamma)$ required for backpropagation:\n\n\\begin{align}\\left( \\frac{\\partial h}{\\partial z} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial y} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial \\theta} \\right)^{\\top} v, \\quad \\left( \\frac{\\partial h}{\\partial \\gamma} \\right)^{\\top} v\\end{align}\n\nand $v$ is the upstream gradient. The VJPs can be computed in closed-form by solving a second least-squares problem, as shown in the following. \nWhen the forward least-squares solver converges to the exact minimizer, we have the following closed-form expressions for $h(z, y, \\theta, \\gamma)$:\n\n\\begin{align}h(z, y, \\theta, \\gamma) = \\left( A_\\theta^{\\top} A_\\theta + \\frac{1}{\\gamma} I \\right)^{-1} \\left( A_\\theta^{\\top} y + \\frac{1}{\\gamma} z \\right)\\end{align}\n\nLet $M$ denote the inverse $\\left( A_\\theta^T A_\\theta + \\frac{1}{\\gamma} I \\right)^{-1}$. The VJPs can be computed as follows:\n\n\\begin{align}\\left( \\frac{\\partial h}{\\partial z} \\right)^{\\top} v               &= \\frac{1}{\\gamma} M v \\\\\n    \\left( \\frac{\\partial h}{\\partial y} \\right)^{\\top} v               &= A_\\theta M v \\\\\n    \\left( \\frac{\\partial h}{\\partial \\gamma} \\right)^{\\top} v          &=   (h - z)^\\top M  v / \\gamma^2 \\\\\n    \\left( \\frac{\\partial h}{\\partial \\theta} \\right)^{\\top} v          &= \\frac{\\partial p}{\\partial \\theta}\\end{align}\nwhere $p =  (y - A_\\theta h)^{\\top} A_\\theta M v$ and $\\frac{\\partial p}{\\partial \\theta}$ can be computed using the standard backpropagation mechanism (autograd).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Linear forward operators that have a :class:`closed-form singular value decomposition <deepinv.physics.DecomposablePhysics>` benefit from a :func:`closed-form formula <deepinv.physics.DecomposablePhysics.prox_l2>` for computing the proximal step, and thus we shouldn't expect speed-ups in these specific cases.</p></div>\n\n\nThis example shows how to train an unfolded neural network with a memory complexity that is independent of the number of iterations in least squares solver (O(1) memory complexity) used for computing the data-fidelity proximal step.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>By default, this example is run on CPU so we should not expect significant speed-ups and we can not trace the memory usage precisely. For a better experience, we recommend running the example on a machine with a GPU. In such a case, we can expect significant speed-ups (20-50%) and a significant reduction in memory usage (2x-3x reduction factor).</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch\nfrom torch.utils.data import DataLoader\nfrom deepinv.optim.data_fidelity import L2\nfrom deepinv.optim.prior import PnP\nfrom deepinv.unfolded import unfolded_builder\nfrom torchvision import transforms\nfrom deepinv.utils.demo import load_dataset\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndevice = (\n    dinv.utils.get_freer_gpu() if torch.cuda.is_available() else torch.device(\"cpu\")\n)\ndtype = torch.float32\nimg_size = 64 if torch.cuda.is_available() else 32\nnum_images = 480 if torch.cuda.is_available() else 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the degradation operator and the dataset.\nWe use the Blur class with `valid` padding from the physics module. ITs proximal operator does not\nhave a closed-form solution, and thus requires using a least-squares solver.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In this example, we use the CBSD500 dataset\ntrain_dataset_name = \"CBSD500\"\n\n# Specify the transforms to be applied to the input images.\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(img_size), transforms.ToTensor()]\n)\n# Define the base train dataset of clean images.\ntrain_dataset = load_dataset(train_dataset_name, transform=train_transform)\ntrain_dataset = torch.utils.data.Subset(train_dataset, range(num_images))\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32 if torch.cuda.is_available() else 8,\n    num_workers=4 if torch.cuda.is_available() else 0,\n    shuffle=True,\n)\nphysics = dinv.physics.Blur(\n    filter=dinv.physics.blur.gaussian_blur(sigma=(2.5, 2.5)),\n    padding=\"valid\",\n    device=device,\n    noise_model=dinv.physics.GaussianNoise(sigma=0.1),\n    max_iter=100,\n    tol=1e-8,\n    implicit_backward_solver=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the unfolded parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Unrolled optimization algorithm parameters\nmax_iter = 5  # number of unfolded layers\n\n# Select the data fidelity term\ndata_fidelity = L2()\nstepsize = [1] * max_iter  # stepsize of the algorithm\nsigma_denoiser = [0.01] * max_iter  # noise level parameter of the denoiser\nparams_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary\n    \"stepsize\": stepsize,\n    \"g_param\": sigma_denoiser,\n}\ntrainable_params = [\n    \"g_param\",\n    \"stepsize\",\n]  # define which parameters from 'params_algo' are trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\nHere we write explicitly the training loop to show how implicit differentiation can be used to avoid out-of-memory issues and sometimes accelerate training. But you can also use the :class:`deepinv.Trainer` class as shown in other examples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some helper functions for measuring memory usage\nuse_cuda = device.type == \"cuda\" and torch.cuda.is_available()\n\n\ndef sync():\n    if use_cuda:\n        torch.cuda.synchronize()\n\n\ndef reset_memory():\n    if use_cuda:\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n\ndef peak_memory():\n    if use_cuda:\n        peak_bytes = int(torch.cuda.max_memory_allocated(device=device))\n    else:  # Stats on CPU is not very accurate\n        peak_bytes = 0\n    return peak_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first train the model will full backpropagation to compare the memory usage.\nDefine the unfolded trainable model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)  # Make sure that we have the same initialization for both runs\nprior = PnP(denoiser=dinv.models.DnCNN(depth=7, pretrained=None).to(device))\nmodel = unfolded_builder(\n    iteration=\"HQS\",\n    params_algo=params_algo.copy(),\n    trainable_params=trainable_params,\n    data_fidelity=data_fidelity,\n    max_iter=max_iter,\n    prior=prior,\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-8)\nmodel.train()\n\n# Setting this parameter to False to use full backpropagation\nphysics.implicit_backward_solver = False\n\nreset_memory()\nsync()\nstart = time.perf_counter()\nauto_losses = []\nfor x in train_loader:\n    x = x.to(device)\n    y = physics(x)\n    optimizer.zero_grad()\n    x_hat = model(physics=physics, y=y)\n    loss = torch.nn.functional.mse_loss(x_hat, x)\n    auto_losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\nsync()\nend = time.perf_counter()\nauto_peak_memory_mb = peak_memory() / (10**6)\nauto_time_per_iter = (end - start) / len(train_loader)\nauto_avg_loss = np.array(auto_losses)\nauto_avg_loss = np.cumsum(auto_avg_loss) / (np.arange(len(auto_avg_loss)) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train the model using the closed-form gradients of the proximal step.\nWe can do this by setting `implicit_backward_solver` to `True`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics.implicit_backward_solver = True\n\n# Define the unfolded trainable model.\ntorch.manual_seed(42)  # Make sure that we have the same initialization for both runs\nprior = PnP(denoiser=dinv.models.DnCNN(depth=7, pretrained=None).to(device))\nmodel = unfolded_builder(\n    iteration=\"HQS\",\n    params_algo=params_algo.copy(),\n    trainable_params=trainable_params,\n    data_fidelity=data_fidelity,\n    max_iter=max_iter,\n    prior=prior,\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-8)\nmodel.train()\n\nreset_memory()\nsync()\nstart = time.perf_counter()\nimplicit_losses = []\nfor x in train_loader:\n    x = x.to(device)\n    y = physics(x)\n    optimizer.zero_grad()\n    x_hat = model(physics=physics, y=y)\n    loss = torch.nn.functional.mse_loss(x_hat, x)\n    implicit_losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\nsync()\nend = time.perf_counter()\nimplicit_peak_memory_mb = peak_memory() / (10**6)\nimplicit_time_per_iter = (end - start) / len(train_loader)\nimplicit_avg_loss = np.array(implicit_losses)\nimplicit_avg_loss = np.cumsum(implicit_avg_loss) / (\n    np.arange(len(implicit_avg_loss)) + 1\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare the memory usage\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Full backpropagation: time per iteration: {auto_time_per_iter:.2f} s. \")\nprint(f\"Implicit differentiation: time per iteration: {implicit_time_per_iter:.2f} s.\")\n\n# Compare the memory usage\nif use_cuda:\n    print(f\"Full backpropagation: peak memory usage: {auto_peak_memory_mb:.1f} MB\")\n    print(\n        f\"Implicit differentiation: peak memory usage: {implicit_peak_memory_mb:.1f} MB\"\n    )\n    print(\n        f\"Memory reduction factor: {auto_peak_memory_mb/implicit_peak_memory_mb:.1f}x\"\n    )\n\n\n# Compare the training loss\nplt.figure(figsize=(7, 4))\nplt.plot(auto_avg_loss, label=\"Full backpropagation\", linestyle=\"--\", linewidth=2)\nplt.plot(\n    implicit_avg_loss, label=\"Implicit differentiation\", linestyle=\"-.\", linewidth=1.5\n)\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\", fontsize=12)\nplt.ylabel(\"Training loss (MSE)\", fontsize=12)\nplt.legend()\nplt.title(\n    f\"Training loss. Avg loss difference: {np.mean(np.abs(auto_avg_loss - implicit_avg_loss)):.2e}\",\n    fontsize=14,\n)\nplt.grid()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}