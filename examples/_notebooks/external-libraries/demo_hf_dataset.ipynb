{
  "cells": [
    {
      "id": "18729710",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "e322cd90",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "\n# Using HuggingFace datasets\n\nThis example shows how to load and prepare properly a HuggingFace dataset\nusing the `datasets` library.\n\nAvailable datasets: https://huggingface.co/datasets?search=deepinv\nHere we use [drunet_dataset](https://github.com/samuro95/GSPnP)."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Load libraries\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset as load_dataset_hf\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom torchvision import transforms\n\nimport deepinv as dinv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Stream data from Internet\n\nStream data from HuggingFace servers: only a limited number of samples is loaded on memory at all time,\nwhich avoid having to save the dataset on disk and avoid overloading the memory capacity.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# source : https://huggingface.co/datasets/deepinv/drunet_dataset\n# type : datasets.iterable_dataset.IterableDataset\nraw_hf_train_dataset = load_dataset_hf(\n    \"deepinv/drunet_dataset\", split=\"train\", streaming=True\n)\nprint(\"Number of data files used to store raw data: \", raw_hf_train_dataset.n_shards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Shuffle data with buffer shuffling\n\n| In streaming mode, we can only read sequentially the data sample in a certain order thus we are not able to do exact shuffling.\n| An alternative way is the buffer shuffling which load a fixed number of samples in memory and let us pick randomly one sample among this fixed number of samples.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable\nraw_hf_train_dataset = raw_hf_train_dataset.shuffle(seed=42, buffer_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Apply transformation on dataset\n\nWe define transformation with ``torchvision.transforms`` module, but it can be any other function.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Function that should be applied to a PIL Image\nimg_transforms = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),  # Resize all images to 224x224\n        transforms.ToTensor(),\n    ]\n)\n\n\n# Class that apply `transform` on data samples of a datasets.iterable_dataset.IterableDataset\nclass HFDataset(IterableDataset):\n    r\"\"\"\n    Creates an iterable dataset from a HuggingFace dataset to enable streaming.\n    \"\"\"\n\n    def __init__(self, hf_dataset, transforms=None, key=\"png\"):\n        self.hf_dataset = hf_dataset\n        self.transform = transforms\n        self.key = key\n\n    def __iter__(self):\n        for sample in self.hf_dataset:\n            if self.transform:\n                out = self.transform(sample[self.key])\n            else:\n                out = sample[self.key]\n            yield out\n\n\nhf_train_dataset = HFDataset(raw_hf_train_dataset, transforms=img_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Create a dataloader\n\n| With ``datasets.iterable_dataset.IterableDataset``, data samples are stored in 1 file or in a few files.\n| In case of few files, we can use ``num_workers`` argument to load data samples in parallel.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if raw_hf_train_dataset.n_shards > 1:\n    # num_workers <= raw_hf_train_dataset.n_shards (number of data files)\n    # num_workers <= number of available cpu cores\n    num_workers = ...\n    train_dataloader = DataLoader(\n        hf_train_dataset, batch_size=2, num_workers=num_workers\n    )\nelse:\n    train_dataloader = DataLoader(hf_train_dataset, batch_size=2)\n\n# display a batch\nbatch = next(iter(train_dataloader))\ndinv.utils.plot([batch[0], batch[1]])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}