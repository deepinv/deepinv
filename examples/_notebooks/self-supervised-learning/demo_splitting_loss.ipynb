{
  "cells": [
    {
      "id": "c19eade5",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "5e770139",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "\n# Self-supervised learning with measurement splitting\n\nWe demonstrate self-supervised learning with measurement splitting, to\ntrain a denoiser network on the MNIST dataset. The physics here is noisy\ncomputed tomography, as is the case in Noise2Inverse :footcite:t:`hendriksen2020noise2inverse`. Note this example\ncan also be easily applied to undersampled multicoil MRI as is the case\nin SSDU :footcite:t:`yaman2020self`.\n\nMeasurement splitting constructs a ground-truth free loss\n$\\frac{m}{m_2}\\| y_2 - A_2 \\inversef{y_1}{A_1}\\|^2$ by splitting\nthe measurement and the forward operator using a randomly generated\nmask.\n\nSee [`deepinv.loss.SplittingLoss`](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.SplittingLoss.html) for full details."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\n\nimport deepinv as dinv\nfrom deepinv.utils import get_data_home\nfrom deepinv.models.utils import get_weights_url\n\ntorch.manual_seed(0)\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"measurements\"\nORIGINAL_DATA_HOME = get_data_home()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define loss\n\nOur implementation has multiple optional parameters that control how the\nsplitting is to be achieved. For example, you can:\n\n-  Use ``split_ratio`` to set the ratio of pixels used in the forward\n   pass vs the loss;\n-  Define custom masking methods using a ``mask_generator`` such as\n   [`deepinv.physics.generator.BernoulliSplittingMaskGenerator`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.generator.BernoulliSplittingMaskGenerator.html)\n   or [`deepinv.physics.generator.GaussianSplittingMaskGenerator`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.generator.GaussianSplittingMaskGenerator.html);\n-  Use ``eval_n_samples`` to set how many realisations of the random\n   mask is used at evaluation time;\n-  Optionally disable measurement splitting at evaluation time using\n   ``eval_split_input`` (as is the case in SSDU :footcite:t:`yaman2020self`).\n-  Average over both input and output masks at evaluation time using\n   ``eval_split_output``. See [`deepinv.loss.SplittingLoss`](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.SplittingLoss.html) for\n   details.\n\nNote that after the model has been defined, the loss must also \"adapt\"\nthe model.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = dinv.loss.SplittingLoss(split_ratio=0.6, eval_split_input=True, eval_n_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Prepare data\n\nWe use the ``torchvision`` MNIST dataset, and use noisy tomography\nphysics (with number of angles equal to the image size) for the forward\noperator.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use a subset of the whole training set to reduce the computational load of the example.\n     We recommend to use the whole set by setting ``train_datapoints=test_datapoints=None`` to get the best results.</p></div>\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = datasets.MNIST(\n    root=ORIGINAL_DATA_HOME, train=True, transform=transform, download=True\n)\ntest_dataset = datasets.MNIST(\n    root=ORIGINAL_DATA_HOME, train=False, transform=transform, download=True\n)\n\nphysics = dinv.physics.Tomography(\n    angles=28,\n    img_width=28,\n    noise_model=dinv.physics.noise.GaussianNoise(0.1),\n    device=device,\n)\n\ndeepinv_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    physics=physics,\n    device=device,\n    save_dir=DATA_DIR,\n    train_datapoints=100,\n    test_datapoints=10,\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define model\n\nWe use a simple U-Net architecture with 2 scales as the denoiser\nnetwork.\n\nTo reduce training time, we use a pretrained model. Here we demonstrate\ntraining with 100 images for 1 epoch, after having loaded a pretrained\nmodel trained that was with 1000 images for 20 epochs.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>When using the splitting loss, the model must be \"adapted\" by the loss, as its forward pass takes only a subset of the pixels, not the full image.</p></div>\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.ArtifactRemoval(\n    dinv.models.UNet(in_channels=1, out_channels=1, scales=2).to(device), pinv=True\n)\nmodel = loss.adapt_model(model)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n\n# Load pretrained model\nfile_name = \"demo_measplit_mnist_tomography.pth\"\nurl = get_weights_url(model_name=\"measplit\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=file_name\n)\n\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "### Train and test network\nTo simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\nsuch as PSNR or SSIM, which require clean ground truth images.\n\n> **Tip**\n>\n>\n> We can use the same self-supervised loss for evaluation, as it does not require clean images,\n> to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n>\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    epochs=1,\n    losses=loss,\n    optimizer=optimizer,\n    device=device,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    metrics=None,  # no supervised metrics\n    early_stop=2,  # we can use early stopping as we have a validation loss\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    plot_images=False,\n    save_path=None,\n    verbose=True,\n    show_progress_bar=False,\n    no_learning_method=\"A_dagger\",  # use pseudo-inverse as no-learning baseline\n)\n\nmodel = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Test and visualize the model outputs using a small test set. We set the\noutput to average over 5 iterations of random mask realisations. The\ntrained model improves on the no-learning reconstruction by ~7dB.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.plot_images = True\ntrainer.test(test_dataloader, metrics=dinv.metric.PSNR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Demonstrate the effect of not averaging over multiple realisations of\nthe splitting mask at evaluation time, by setting ``eval_n_samples=1``.\nWe have a worse performance:\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.eval_n_samples = 1\ntrainer.test(test_dataloader, metrics=dinv.metric.PSNR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Furthermore, we can disable measurement splitting at evaluation\naltogether by setting ``eval_split_input`` to False (this is done in\nSSDU :footcite:t:`yaman2020self`). This generally is\nworse than MC averaging:\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.eval_split_input = False\ntrainer.test(test_dataloader, metrics=dinv.metric.PSNR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": ":References:\n\n> **Footbibliography**\n>\n>\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}