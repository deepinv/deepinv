{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Self-supervised MRI reconstruction with Artifact2Artifact\n",
    "=========================================================\n",
    "\n",
    "We demonstrate the self-supervised Artifact2Artifact loss for solving an\n",
    "undersampled sequential MRI reconstruction problem without ground truth.\n",
    "\n",
    "The Artifact2Artifact loss was introduced by :footcite:t:`liu2020rare`.\n",
    "\n",
    "In our example, we use it to reconstruct **static** images, where the\n",
    "k-space measurements is a time-sequence, where each time step (phase)\n",
    "consists of sampled lines such that the whole measurement is a set of\n",
    "non-overlapping lines.\n",
    "\n",
    "For a description of how Artifact2Artifact constructs the loss, see\n",
    ":class:`deepinv.loss.mri.Artifact2ArtifactLoss`.\n",
    "\n",
    "Note in our implementation, this is a special case of the generic\n",
    "splitting loss: see :class:`deepinv.loss.SplittingLoss` for more\n",
    "details. See :class:`deepinv.loss.mri.Phase2PhaseLoss` for the related\n",
    "Phase2Phase.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import deepinv as dinv\n",
    "from deepinv.datasets import SimpleFastMRISliceDataset\n",
    "from deepinv.utils import get_data_home\n",
    "from deepinv.models.utils import get_weights_url\n",
    "from deepinv.models import MoDL\n",
    "from deepinv.physics.generator import (\n",
    "    GaussianMaskGenerator,\n",
    "    BernoulliSplittingMaskGenerator,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9001a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# ---------\n",
    "#\n",
    "\n",
    "# In this example, we use a mini demo subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_\n",
    "# as the base image dataset, consisting of knees of size 320x320, and then resized to 128x128 for speed.\n",
    "#\n",
    "# .. important::\n",
    "#\n",
    "#    By using this dataset, you confirm that you have agreed to and signed the `FastMRI data use agreement <https://fastmri.med.nyu.edu/>`_.\n",
    "#\n",
    "# .. seealso::\n",
    "#\n",
    "#   Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n",
    "#       We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n",
    "#       You can download more data on the `FastMRI site <https://fastmri.med.nyu.edu/>`_.\n",
    "#\n",
    "#\n",
    "# We use a train set of size 1 and test set of size 1 in this demo for\n",
    "# speed to fine-tune the original model. To train the original\n",
    "# model from scratch, use a larger dataset of size ~150.\n",
    "#\n",
    "\n",
    "batch_size = 1\n",
    "H = 128\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(H)])\n",
    "\n",
    "train_dataset = SimpleFastMRISliceDataset(\n",
    "    get_data_home(), transform=transform, train=True, download=True, train_percent=0.5\n",
    ")\n",
    "test_dataset = SimpleFastMRISliceDataset(\n",
    "    get_data_home(), transform=transform, train=False, train_percent=0.5\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd0516",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define physics\n",
    "# --------------\n",
    "#\n",
    "# We simulate a sequential k-space sampler, that, over the course of 4\n",
    "# phases (i.e. frames), samples 64 lines (i.e 2x total undersampling from\n",
    "# 128) with Gaussian weighting (plus a few extra for the ACS signals in\n",
    "# the center of the k-space). We use\n",
    "# :class:`deepinv.physics.SequentialMRI` to do this.\n",
    "#\n",
    "# First, we define a static 2x acceleration mask that all measurements use\n",
    "# (of shape [B,C,H,W]):\n",
    "#\n",
    "\n",
    "mask_full = GaussianMaskGenerator((2, H, H), acceleration=2, device=device).step(\n",
    "    batch_size=batch_size\n",
    ")[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47e453",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Next, we randomly share the sampled lines across 4 time-phases into a\n",
    "# time-varying mask:\n",
    "#\n",
    "\n",
    "# Split only in horizontal direction\n",
    "masks = [mask_full[..., 0, :]]\n",
    "splitter = BernoulliSplittingMaskGenerator((2, H), split_ratio=0.5, device=device)\n",
    "\n",
    "acs = 10\n",
    "\n",
    "# Split 4 times\n",
    "for _ in range(2):\n",
    "    new_masks = []\n",
    "    for m in masks:\n",
    "        m1 = splitter.step(batch_size=batch_size, input_mask=m)[\"mask\"]\n",
    "        m2 = m - m1\n",
    "        m1[..., H // 2 - acs // 2 : H // 2 + acs // 2] = 1\n",
    "        m2[..., H // 2 - acs // 2 : H // 2 + acs // 2] = 1\n",
    "        new_masks.extend([m1, m2])\n",
    "    masks = new_masks\n",
    "\n",
    "# Merge masks into time dimension\n",
    "mask = torch.stack(masks, 2)\n",
    "\n",
    "# Convert to vertical lines\n",
    "mask = torch.stack([mask] * H, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ccfc3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now define physics using this time-varying mask of shape [B,C,T,H,W]:\n",
    "#\n",
    "\n",
    "physics = dinv.physics.SequentialMRI(mask=mask, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26e721",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's visualize the sequential measurements using a sample image (run\n",
    "# this notebook yourself to display the video). We also visualize the\n",
    "# frame-by-frame no-learning zero-filled reconstruction.\n",
    "#\n",
    "\n",
    "x = next(iter(train_dataloader)).to(device)\n",
    "y = physics(x)\n",
    "dinv.utils.plot_videos(\n",
    "    [physics.repeat(x, mask), y, mask, physics.A_adjoint(y, keep_time_dim=True)],\n",
    "    titles=[\"x\", \"y\", \"mask\", \"x_init\"],\n",
    "    display=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5fc9a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Also visualize the flattened time-series, recovering the original 2x\n",
    "# undersampling mask (note the actual undersampling factor is much lower\n",
    "# due to ACS lines):\n",
    "#\n",
    "\n",
    "dinv.utils.plot(\n",
    "    [x, physics.average(y), physics.average(mask), physics.A_adjoint(y)],\n",
    "    titles=[\"x\", \"y\", \"orig mask\", \"x_init\"],\n",
    ")\n",
    "\n",
    "print(\"Total acceleration:\", (2 * 128 * 128) / mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a8dc1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "# ------------\n",
    "#\n",
    "# As a (static) reconstruction network, we use an unrolled network\n",
    "# (half-quadratic splitting) with a trainable denoising prior based on the\n",
    "# DnCNN architecture which was proposed in MoDL :footcite:t:`aggarwal2018modl`.\n",
    "# See :class:`deepinv.models.MoDL` for details.\n",
    "#\n",
    "\n",
    "model = MoDL().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e42655",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prep loss\n",
    "# ---------\n",
    "#\n",
    "# Perform loss on all collected lines by setting ``dynamic_model`` to\n",
    "# False. Then adapt model to perform Artifact2Artifact. We set\n",
    "# ``split_size=1`` to mean that each Artifact chunk containes only 1\n",
    "# frame.\n",
    "#\n",
    "\n",
    "loss = dinv.loss.mri.Artifact2ArtifactLoss(\n",
    "    (2, 4, H, H), split_size=1, dynamic_model=False, device=device\n",
    ")\n",
    "model = loss.adapt_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d926bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "# -----------\n",
    "#\n",
    "# Original model is trained for 100 epochs. We demonstrate loading the\n",
    "# pretrained model then fine-tuning with 1 epoch. Report PSNR and SSIM. To\n",
    "# train from scratch, simply comment out the model loading code and\n",
    "# increase the number of epochs.\n",
    "#\n",
    "# To simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\n",
    "# such as PSNR or SSIM, which require clean ground truth images.\n",
    "#\n",
    "# .. tip::\n",
    "#\n",
    "#       We can use the same self-supervised loss for evaluation, as it does not require clean images,\n",
    "#       to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n",
    "\n",
    "# Load pretrained model\n",
    "file_name = \"demo_artifact2artifact_mri.pth\"\n",
    "url = get_weights_url(model_name=\"measplit\", file_name=file_name)\n",
    "ckpt = torch.hub.load_state_dict_from_url(\n",
    "    url, map_location=lambda storage, loc: storage, file_name=file_name\n",
    ")\n",
    "\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = dinv.Trainer(\n",
    "    model,\n",
    "    physics=physics,\n",
    "    epochs=1,\n",
    "    losses=loss,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    compute_eval_losses=True,  # use self-supervised loss for evaluation\n",
    "    early_stop_on_losses=True,  # stop using self-supervised eval loss\n",
    "    metrics=None,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    early_stop=2,  # early stop using the self-supervised loss on the test set\n",
    "    online_measurements=True,\n",
    "    device=device,\n",
    "    save_path=None,\n",
    "    verbose=True,\n",
    "    show_progress_bar=False,\n",
    ")\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# ==============\n",
    "#\n",
    "# We now assume that we have access to a small test set of ground-truth images to evaluate the performance of the trained network.\n",
    "# and we compute the PSNR between the denoised images and the clean ground truth images.\n",
    "#\n",
    "\n",
    "trainer.plot_images = True\n",
    "trainer.test(test_dataloader, metrics=[dinv.metric.PSNR(), dinv.metric.SSIM()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff53f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
