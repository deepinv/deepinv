{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8682bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Image transformations for Equivariant Imaging\n",
    "=============================================\n",
    "\n",
    "This example demonstrates various geometric image transformations\n",
    "implemented in ``deepinv`` that can be used in Equivariant Imaging (EI)\n",
    "for self-supervised learning:\n",
    "\n",
    "-  Shift: integer pixel 2D shift;\n",
    "-  Rotate: 2D image rotation;\n",
    "-  Scale: continuous 2D image downscaling;\n",
    "-  Euclidean: includes continuous translation, rotation, and reflection,\n",
    "   forming the group :math:`\\mathbb{E}(2)`;\n",
    "-  Similarity: as above but includes scale, forming the group\n",
    "   :math:`\\text{S}(2)`;\n",
    "-  Affine: as above but includes shear effects, forming the group\n",
    "   :math:`\\text{Aff}(3)`;\n",
    "-  Homography: as above but includes perspective (i.e pan and tilt)\n",
    "   effects, forming the group :math:`\\text{PGL}(3)`;\n",
    "-  PanTiltRotate: pure 3D camera rotation i.e pan, tilt and 2D image\n",
    "   rotation.\n",
    "\n",
    "See :ref:`docs <transform>` for full list.\n",
    "\n",
    "These were proposed in the papers:\n",
    "\n",
    "-  ``Shift``, ``Rotate``: :footcite:t:`chen2021equivariant`.\n",
    "-  ``Scale``: :footcite:t:`scanvic2025scale`.\n",
    "-  ``Homography`` and the projective geometry framework: :footcite:t:`wang2024perspective`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, CenterCrop, Resize\n",
    "\n",
    "import deepinv as dinv\n",
    "from deepinv.utils import get_data_home\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ORIGINAL_DATA_DIR = get_data_home() / \"Urban100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3babf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define transforms. For the transforms that involve 3D camera rotation\n",
    "# (i.e pan or tilt), we limit ``theta_max`` for display.\n",
    "#\n",
    "\n",
    "transforms = [\n",
    "    dinv.transform.Shift(),\n",
    "    dinv.transform.Rotate(),\n",
    "    dinv.transform.Scale(),\n",
    "    dinv.transform.Homography(theta_max=10),\n",
    "    dinv.transform.projective.Euclidean(),\n",
    "    dinv.transform.projective.Similarity(),\n",
    "    dinv.transform.projective.Affine(),\n",
    "    dinv.transform.projective.PanTiltRotate(theta_max=10),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413dc533",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot transforms on a sample image. Note that, during training, we never\n",
    "# have access to these ground truth images ``x``, only partial and noisy\n",
    "# measurements ``y``.\n",
    "#\n",
    "\n",
    "x = dinv.utils.load_example(\"celeba_example.jpg\")\n",
    "dinv.utils.plot(\n",
    "    [x] + [t(x) for t in transforms],\n",
    "    [\"Orig\"] + [t.__class__.__name__ for t in transforms],\n",
    "    fontsize=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee425694",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now, we run an inpainting experiment to reconstruct images from images\n",
    "# masked with a random mask, without ground truth, using EI. For this\n",
    "# example we use the Urban100 images of natural urban scenes. As these\n",
    "# scenes are imaged with a camera free to move and rotate in the world,\n",
    "# all of the above transformations are valid invariances that we can\n",
    "# impose on the unknown image set :math:`x\\in X`.\n",
    "#\n",
    "\n",
    "dataset = dinv.datasets.Urban100HR(\n",
    "    root=ORIGINAL_DATA_DIR,\n",
    "    download=True,\n",
    "    transform=Compose([ToTensor(), Resize(256), CenterCrop(256)]),\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, (0.8, 0.2))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# Use physics to generate data online\n",
    "physics = dinv.physics.Inpainting((3, 256, 256), mask=0.6, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d4037",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# For training, use a small UNet, Adam optimizer, EI loss with homography\n",
    "# transform, and the ``deepinv.Trainer`` functionality:\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#       We only train for a single epoch in the demo, but it is recommended to train multiple epochs in practice.\n",
    "#\n",
    "# To simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\n",
    "# such as PSNR or SSIM, which require clean ground truth images.\n",
    "#\n",
    "# .. tip::\n",
    "#\n",
    "#       We can use the same self-supervised loss for evaluation, as it does not require clean images,\n",
    "#       to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n",
    "\n",
    "model = dinv.models.UNet(\n",
    "    in_channels=3, out_channels=3, scales=2, circular_padding=True, batch_norm=False\n",
    ").to(device)\n",
    "\n",
    "losses = [\n",
    "    dinv.loss.MCLoss(),\n",
    "    dinv.loss.EILoss(dinv.transform.Homography(theta_max=10, device=device)),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n",
    "\n",
    "model = dinv.Trainer(\n",
    "    model=model,\n",
    "    physics=physics,\n",
    "    online_measurements=True,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    compute_eval_losses=True,  # use self-supervised loss for evaluation\n",
    "    early_stop_on_losses=True,  # stop using self-supervised eval loss\n",
    "    epochs=1,\n",
    "    losses=losses,\n",
    "    metrics=None,  # no supervised metrics\n",
    "    early_stop=2,  # we can use early stopping as we have a validation set\n",
    "    optimizer=optimizer,\n",
    "    verbose=True,\n",
    "    show_progress_bar=False,\n",
    "    save_path=None,\n",
    "    device=device,\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba32f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results of a pretrained model trained using a larger UNet for 40\n",
    "# epochs:\n",
    "#\n",
    "\n",
    "model = dinv.models.UNet(\n",
    "    in_channels=3, out_channels=3, scales=3, circular_padding=True, batch_norm=False\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.hub.load_state_dict_from_url(\n",
    "    dinv.models.utils.get_weights_url(\"ei\", \"Urban100_inpainting_homography_model.pth\"),\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "x = next(iter(train_dataloader))\n",
    "x = x.to(device)\n",
    "y = physics(x)\n",
    "x_hat = model(y)\n",
    "\n",
    "dinv.utils.plot([x, y, x_hat], [\"x\", \"y\", \"reconstruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
