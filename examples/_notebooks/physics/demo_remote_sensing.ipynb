{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Remote sensing with satellite images\n",
    "====================================\n",
    "\n",
    "In this example we demonstrate remote sensing inverse problems for multispectral satellite imaging.\n",
    "\n",
    "These have important applications for image restoration in environmental monitoring, urban planning, disaster recovery etc.\n",
    "\n",
    "We will demonstrate pan-sharpening, i.e., recovering high-resolution multispectral images from measurement pairs of\n",
    "low-resolution multispectral images and high-resolution panchromatic (single-band) images with the forward\n",
    "operator :class:`deepinv.physics.Pansharpen`.\n",
    "\n",
    "We will also demonstrate other inverse problems including compressive spectral imaging and hyperspectral unmixing.\n",
    "\n",
    "We provide a convenient satellite image dataset for pan-sharpening :class:`deepinv.datasets.NBUDataset` provided in the paper :footcite:t:`meng2020large`.\n",
    "which includes data from several satellites such as WorldView satellites.\n",
    "\n",
    ".. tip::\n",
    "\n",
    "    For remote sensing experiments, DeepInverse provides the following classes:\n",
    "\n",
    "    - :class:`Pan-sharpening <deepinv.physics.Pansharpen>`\n",
    "    - :class:`Compressive spectral imaging <deepinv.physics.CompressiveSpectralImaging>`\n",
    "    - :class:`Hyperspectral unmixing <deepinv.physics.HyperSpectralUnmixing>`\n",
    "    - :class:`Super resolution <deepinv.physics.Downsampling>`\n",
    "    - :class:`Satellite imagery dataset <deepinv.datasets.NBUDataset>`\n",
    "    - Metrics for multispectral data: :class:`QNR <deepinv.loss.metric.QNR>`, :class:`SpectralAngleMapper <deepinv.loss.metric.SpectralAngleMapper>`, :class:`ERGAS <deepinv.loss.metric.ERGAS>`\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63af3b0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "import torch\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw pan-sharpening measurements\n",
    "# ------------------------------------\n",
    "# The dataset includes raw pansharpening measurements\n",
    "# containing ``(MS, PAN)`` where ``MS`` are the low-res (4-band) multispectral and ``PAN`` are the high-res\n",
    "# panchromatic images. Note there are no ground truth images!\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#   The pan-sharpening measurements are provided as a :class:`deepinv.utils.TensorList`, since\n",
    "#   the pan-sharpening physics :class:`deepinv.physics.Pansharpen` is a stacked physics combining\n",
    "#   :class:`deepinv.physics.Downsampling` and :class:`deepinv.physics.Decolorize`.\n",
    "#   See the User Guide :ref:`physics_combining` for more information.\n",
    "#\n",
    "# Note, for plotting purposes we only plot the first 3 bands (RGB).\n",
    "#\n",
    "# Note also that the linear adjoint must assume the unknown spectral response function (SRF).\n",
    "#\n",
    "\n",
    "DATA_DIR = dinv.utils.get_data_home()\n",
    "dataset = dinv.datasets.NBUDataset(DATA_DIR, return_pan=True, download=True)\n",
    "\n",
    "y = dataset[0].unsqueeze(0).to(device)  # MS (1,4,256,256), PAN (1,1,1024,1024)\n",
    "\n",
    "physics = dinv.physics.Pansharpen((4, 1024, 1024), factor=4, device=device)\n",
    "\n",
    "# Pansharpen with classical Brovey method\n",
    "x_hat = physics.A_dagger(y)  # shape (1,4,1024,1024)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    [\n",
    "        y[0][:, :3],\n",
    "        y[1],  # Note this will be interpolated to match high-res image size\n",
    "        x_hat[:, :3],\n",
    "        physics.A_adjoint(y)[:, :3],\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Input MS\",\n",
    "        \"Input PAN\",\n",
    "        \"Pseudo-inverse \\n using \\n Brovey method\",\n",
    "        \"Linear adjoint\",\n",
    "    ],\n",
    "    dpi=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d816f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Evaluate performance - note we can only use QNR as we have no GT\n",
    "#\n",
    "\n",
    "qnr = dinv.metric.QNR()\n",
    "print(qnr(x_net=x_hat, x=None, y=y, physics=physics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate remote-sensing measurements\n",
    "# ------------------------------------\n",
    "# We can also simulate measurements from various remote sensing inverse problems so that we have pairs of\n",
    "# measurements and ground truth. Now, the dataset loads ground truth images ``x``:\n",
    "#\n",
    "\n",
    "dataset = dinv.datasets.NBUDataset(DATA_DIR, return_pan=False)\n",
    "\n",
    "x = dataset[0].unsqueeze(0).to(device)  # just MS of shape 1,4,256,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For **compressive spectral imaging**, we use the coded-aperture snapshot spectral imaging (CASSI) model,\n",
    "# which is a popular hyperspectral imaging method. See :class:`deepinv.physics.CompressiveSpectralImaging`\n",
    "#\n",
    "\n",
    "physics = dinv.physics.CompressiveSpectralImaging(x.shape[1:], mode=\"sd\", device=device)\n",
    "y = physics(x)  # 1,1,256,256\n",
    "dinv.utils.plot([x[:, :3], y], titles=[\"Image x\", \"CASSI meas. y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc28027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For **hyperspectral unmixing**, our images are the measurements and we seek to recover abundances\n",
    "# given the endmember matrix in the linear mixing model.\n",
    "# In this toy example, we perform unmixing with 2 endmembers: one purely yellow and one purely blue.\n",
    "#\n",
    "\n",
    "physics = dinv.physics.HyperSpectralUnmixing(\n",
    "    M=torch.tensor([[0.5, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]), device=device\n",
    ")\n",
    "abundance = physics.A_adjoint(x)  # 1,2,256,256\n",
    "dinv.utils.plot(\n",
    "    [x[:, :3], abundance[:, [0]], abundance[:, [1]]],\n",
    "    titles=[\"Mixed image\", \"Yellow abudance\", \"Blue abundance\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the **pansharpening** physics, we assume a flat spectral response function,\n",
    "# but this can also be jointly learned. We simulate Gaussian noise on the panchromatic images.\n",
    "#\n",
    "\n",
    "physics = dinv.physics.Pansharpen((4, 256, 256), factor=4, srf=\"flat\", device=device)\n",
    "\n",
    "y = physics(x)\n",
    "\n",
    "# Pansharpen with classical Brovey method\n",
    "x_hat = physics.A_dagger(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving pan-sharpening with neural networks\n",
    "# -------------------------------------------\n",
    "# The pan-sharpening physics is compatible with the rest of the DeepInverse library\n",
    "# so we can solve the inverse problem using any method provided in the library.\n",
    "# For example, we use here the PanNet :footcite:t:`yang2017pannet` model.\n",
    "#\n",
    "# This model can be trained using losses such as supervised learning using :class:`deepinv.loss.SupLoss`\n",
    "# or self-supervised learning using Equivariant Imaging :class:`deepinv.loss.EILoss`, which was applied to\n",
    "# pan-sharpening in :footcite:t:`wang2024perspective`.\n",
    "#\n",
    "# For evaluation, we use the standard full-reference metrics (ERGAS, SAM) and no-reference (QNR).\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#   This is a tiny example using 5 images. We demonstrate training for 1 epoch for speed, but you can train from scratch using 50 epochs.\n",
    "#\n",
    "\n",
    "model = dinv.models.PanNet(hrms_shape=(4, 256, 256), device=device)\n",
    "x_net = model(y, physics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b930110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loss using measurement consistency on the multispectral images\n",
    "# and Stein's Unbiased Risk Estimate on the panchromatic images.\n",
    "# For metrics, we use standard full-reference and no-reference multispectral pan-sharpening metrics,\n",
    "# since ground-truth is now available.\n",
    "\n",
    "loss = dinv.loss.StackedPhysicsLoss(\n",
    "    [dinv.loss.MCLoss(), dinv.loss.SureGaussianLoss(0.05)]\n",
    ")\n",
    "\n",
    "sam = dinv.metric.distortion.SpectralAngleMapper()\n",
    "ergas = dinv.metric.distortion.ERGAS(factor=4)\n",
    "qnr = dinv.metric.QNR()\n",
    "print(sam(x_hat, x), ergas(x_hat, x), qnr(x_hat, x=None, y=y, physics=physics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, we first load optimizer and pretrained model,\n",
    "# then train using the deepinv Trainer.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "from deepinv.models.utils import get_weights_url\n",
    "\n",
    "file_name = \"demo_nbu_pansharpen.pth\"\n",
    "url = get_weights_url(model_name=\"demo\", file_name=file_name)\n",
    "ckpt = torch.hub.load_state_dict_from_url(\n",
    "    url, map_location=lambda storage, loc: storage, file_name=file_name\n",
    ")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainer = dinv.Trainer(\n",
    "    model=model,\n",
    "    physics=physics,\n",
    "    optimizer=optimizer,\n",
    "    losses=loss,\n",
    "    metrics=[sam, ergas],\n",
    "    train_dataloader=DataLoader(dataset),\n",
    "    epochs=1,\n",
    "    online_measurements=True,\n",
    "    plot_images=False,\n",
    "    compare_no_learning=True,\n",
    "    no_learning_method=\"A_dagger\",\n",
    "    show_progress_bar=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.test(DataLoader(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ab54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample results:\n",
    "dinv.utils.plot(\n",
    "    [\n",
    "        x[:, :3],\n",
    "        y[0][:, :3],\n",
    "        y[1],\n",
    "        x_hat[:, :3],\n",
    "        x_net[:, :3],\n",
    "    ],\n",
    "    titles=[\"x HRMS\", \"y LRMS\", \"y PAN\", \"Estimate (classical)\", \"Estimate (PanNet)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
