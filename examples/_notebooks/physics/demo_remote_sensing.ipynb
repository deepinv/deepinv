{
  "cells": [
    {
      "id": "c4721a2c",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "9ef0ba8d",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "\n# Remote sensing with satellite images\n\nIn this example we demonstrate remote sensing inverse problems for multispectral satellite imaging.\n\nThese have important applications for image restoration in environmental monitoring, urban planning, disaster recovery etc.\n\nWe will demonstrate pan-sharpening, i.e., recovering high-resolution multispectral images from measurement pairs of\nlow-resolution multispectral images and high-resolution panchromatic (single-band) images with the forward\noperator [`deepinv.physics.Pansharpen`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Pansharpen.html).\n\nWe will also demonstrate other inverse problems including compressive spectral imaging and hyperspectral unmixing.\n\nWe provide a convenient satellite image dataset for pan-sharpening [`deepinv.datasets.NBUDataset`](https://deepinv.github.io/deepinv/api/stubs/deepinv.datasets.NBUDataset.html) provided in the paper :footcite:t:`meng2020large`.\nwhich includes data from several satellites such as WorldView satellites.\n\n> **Tip**\n>\n>\n> For remote sensing experiments, DeepInverse provides the following classes:\n>\n> - [Pan-sharpening](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Pansharpen.html)\n> - [Compressive spectral imaging](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.CompressiveSpectralImaging.html)\n> - [Hyperspectral unmixing](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.HyperSpectralUnmixing.html)\n> - [Super resolution](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Downsampling.html)\n> - [Satellite imagery dataset](https://deepinv.github.io/deepinv/api/stubs/deepinv.datasets.NBUDataset.html)\n> - Metrics for multispectral data: [QNR](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.metric.QNR.html), [SpectralAngleMapper](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.metric.SpectralAngleMapper.html), [ERGAS](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.metric.ERGAS.html)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Load raw pan-sharpening measurements\nThe dataset includes raw pansharpening measurements\ncontaining ``(MS, PAN)`` where ``MS`` are the low-res (4-band) multispectral and ``PAN`` are the high-res\npanchromatic images. Note there are no ground truth images!\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The pan-sharpening measurements are provided as a [`deepinv.utils.TensorList`](https://deepinv.github.io/deepinv/api/stubs/deepinv.utils.TensorList.html), since\n  the pan-sharpening physics [`deepinv.physics.Pansharpen`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Pansharpen.html) is a stacked physics combining\n  [`deepinv.physics.Downsampling`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Downsampling.html) and [`deepinv.physics.Decolorize`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.Decolorize.html).\n  See the User Guide `physics_combining` for more information.</p></div>\n\nNote, for plotting purposes we only plot the first 3 bands (RGB).\n\nNote also that the linear adjoint must assume the unknown spectral response function (SRF).\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DATA_DIR = dinv.utils.get_data_home()\ndataset = dinv.datasets.NBUDataset(DATA_DIR, return_pan=True, download=True)\n\ny = dataset[0].unsqueeze(0).to(device)  # MS (1,4,256,256), PAN (1,1,1024,1024)\n\nphysics = dinv.physics.Pansharpen((4, 1024, 1024), factor=4, device=device)\n\n# Pansharpen with classical Brovey method\nx_hat = physics.A_dagger(y)  # shape (1,4,1024,1024)\n\ndinv.utils.plot(\n    [\n        y[0][:, :3],\n        y[1],  # Note this will be interpolated to match high-res image size\n        x_hat[:, :3],\n        physics.A_adjoint(y)[:, :3],\n    ],\n    titles=[\n        \"Input MS\",\n        \"Input PAN\",\n        \"Pseudo-inverse \\n using \\n Brovey method\",\n        \"Linear adjoint\",\n    ],\n    dpi=1200,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Evaluate performance - note we can only use QNR as we have no GT\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qnr = dinv.metric.QNR()\nprint(qnr(x_net=x_hat, x=None, y=y, physics=physics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Simulate remote-sensing measurements\nWe can also simulate measurements from various remote sensing inverse problems so that we have pairs of\nmeasurements and ground truth. Now, the dataset loads ground truth images ``x``:\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = dinv.datasets.NBUDataset(DATA_DIR, return_pan=False)\n\nx = dataset[0].unsqueeze(0).to(device)  # just MS of shape 1,4,256,256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "For **compressive spectral imaging**, we use the coded-aperture snapshot spectral imaging (CASSI) model,\nwhich is a popular hyperspectral imaging method. See [`deepinv.physics.CompressiveSpectralImaging`](https://deepinv.github.io/deepinv/api/stubs/deepinv.physics.CompressiveSpectralImaging.html)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.CompressiveSpectralImaging(x.shape[1:], mode=\"sd\", device=device)\ny = physics(x)  # 1,1,256,256\ndinv.utils.plot([x[:, :3], y], titles=[\"Image x\", \"CASSI meas. y\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "For **hyperspectral unmixing**, our images are the measurements and we seek to recover abundances\ngiven the endmember matrix in the linear mixing model.\nIn this toy example, we perform unmixing with 2 endmembers: one purely yellow and one purely blue.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.HyperSpectralUnmixing(\n    M=torch.tensor([[0.5, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]), device=device\n)\nabundance = physics.A_adjoint(x)  # 1,2,256,256\ndinv.utils.plot(\n    [x[:, :3], abundance[:, [0]], abundance[:, [1]]],\n    titles=[\"Mixed image\", \"Yellow abudance\", \"Blue abundance\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "For the **pansharpening** physics, we assume a flat spectral response function,\nbut this can also be jointly learned. We simulate Gaussian noise on the panchromatic images.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.Pansharpen((4, 256, 256), factor=4, srf=\"flat\", device=device)\n\ny = physics(x)\n\n# Pansharpen with classical Brovey method\nx_hat = physics.A_dagger(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Solving pan-sharpening with neural networks\nThe pan-sharpening physics is compatible with the rest of the DeepInverse library\nso we can solve the inverse problem using any method provided in the library.\nFor example, we use here the PanNet :footcite:t:`yang2017pannet` model.\n\nThis model can be trained using losses such as supervised learning using [`deepinv.loss.SupLoss`](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.SupLoss.html)\nor self-supervised learning using Equivariant Imaging [`deepinv.loss.EILoss`](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.EILoss.html), which was applied to\npan-sharpening in :footcite:t:`wang2024perspective`.\n\nFor evaluation, we use the standard full-reference metrics (ERGAS, SAM) and no-reference (QNR).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is a tiny example using 5 images. We demonstrate training for 1 epoch for speed, but you can train from scratch using 50 epochs.</p></div>\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.PanNet(hrms_shape=(4, 256, 256), device=device)\nx_net = model(y, physics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Example training loss using measurement consistency on the multispectral images\nand Stein's Unbiased Risk Estimate on the panchromatic images.\nFor metrics, we use standard full-reference and no-reference multispectral pan-sharpening metrics,\nsince ground-truth is now available.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = dinv.loss.StackedPhysicsLoss(\n    [dinv.loss.MCLoss(), dinv.loss.SureGaussianLoss(0.05)]\n)\n\nsam = dinv.metric.distortion.SpectralAngleMapper()\nergas = dinv.metric.distortion.ERGAS(factor=4)\nqnr = dinv.metric.QNR()\nprint(sam(x_hat, x), ergas(x_hat, x), qnr(x_hat, x=None, y=y, physics=physics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "For training, we first load optimizer and pretrained model,\nthen train using the deepinv Trainer.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n\nfrom deepinv.models.utils import get_weights_url\n\nfile_name = \"demo_nbu_pansharpen.pth\"\nurl = get_weights_url(model_name=\"demo\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=file_name\n)\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])\n\nfrom torch.utils.data import DataLoader\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    optimizer=optimizer,\n    losses=loss,\n    metrics=[sam, ergas],\n    train_dataloader=DataLoader(dataset),\n    epochs=1,\n    online_measurements=True,\n    plot_images=False,\n    compare_no_learning=True,\n    no_learning_method=\"A_dagger\",\n    show_progress_bar=False,\n    device=device,\n)\n\ntrainer.train()\ntrainer.test(DataLoader(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "Plot sample results:\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.utils.plot(\n    [\n        x[:, :3],\n        y[0][:, :3],\n        y[1],\n        x_hat[:, :3],\n        x_net[:, :3],\n    ],\n    titles=[\"x HRMS\", \"y LRMS\", \"y PAN\", \"Estimate (classical)\", \"Estimate (PanNet)\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": ":References:\n\n> **Footbibliography**\n>\n>\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}