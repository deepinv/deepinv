{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Tour of MRI functionality in DeepInverse\n",
    "========================================\n",
    "\n",
    "This example presents the various datasets, forward physics and models\n",
    "available in DeepInverse for Magnetic Resonance Imaging (MRI) problems:\n",
    "\n",
    "-  Physics: :class:`deepinv.physics.MRI`,\n",
    "   :class:`deepinv.physics.MultiCoilMRI`,\n",
    "   :class:`deepinv.physics.DynamicMRI`\n",
    "-  Datasets: raw kspace with the `FastMRI <https://fastmri.med.nyu.edu>`__ dataset\n",
    "   :class:`deepinv.datasets.FastMRISliceDataset` and an in-memory easy-to-use version\n",
    "   :class:`deepinv.datasets.SimpleFastMRISliceDataset`, and raw dynamic k-t-space data with the\n",
    "   `CMRxRecon <https://cmrxrecon.github.io>`__ dataset.\n",
    "-  Models: :class:`deepinv.models.VarNet`\n",
    "   (VarNet :footcite:t:`hammernik2018learning`, E2E-VarNet :footcite:t:`sriram2020end`),\n",
    "   :class:`deepinv.models.MoDL` (a simple MoDL :footcite:t:`aggarwal2018modl` unrolled model)\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. Get started with FastMRI (singlecoil + multicoil)\n",
    "2. Train an accelerated MRI with neural networks\n",
    "3. Load raw FastMRI data (singlecoil + multicoil)\n",
    "4. Train using raw data\n",
    "5. Explore 3D MRI\n",
    "6. Explore dynamic MRI\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7757c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "rng = torch.Generator(device=device).manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6152481",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 1. Get started with FastMRI\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# You can get started with our simple\n",
    "# `FastMRI <https://fastmri.med.nyu.edu>`__ mini slice subsets which provide\n",
    "# quick, easy-to-use, in-memory datasets which can be used for simulation\n",
    "# experiments.\n",
    "#\n",
    "# .. important::\n",
    "#\n",
    "#    By using this dataset, you confirm that you have agreed to and signed the `FastMRI data use agreement <https://fastmri.med.nyu.edu/>`_.\n",
    "#\n",
    "# .. seealso::\n",
    "#\n",
    "#   Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n",
    "#       We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n",
    "#       You can download more data on the `FastMRI site <https://fastmri.med.nyu.edu/>`_.\n",
    "#\n",
    "# Load mini demo knee and brain datasets (original data is 320x320 but we resize to\n",
    "# 128 for speed):\n",
    "#\n",
    "\n",
    "transform = torchvision.transforms.Resize(128)\n",
    "knee_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n",
    "    dinv.utils.get_data_home(),\n",
    "    anatomy=\"knee\",\n",
    "    transform=transform,\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "brain_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n",
    "    dinv.utils.get_data_home(),\n",
    "    anatomy=\"brain\",\n",
    "    transform=transform,\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "img_size = knee_dataset[0].shape[-2:]  # (128, 128)\n",
    "dinv.utils.plot({\"knee\": knee_dataset[0], \"brain\": brain_dataset[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cd28a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's start with single-coil MRI. We can define a constant Cartesian 4x\n",
    "# undersampling mask by sampling once from a physics generator. The mask,\n",
    "# data and measurements will all be of shape ``(B, C, H, W)`` where\n",
    "# ``C=2`` is the real and imaginary parts.\n",
    "#\n",
    "\n",
    "physics_generator = dinv.physics.generator.GaussianMaskGenerator(\n",
    "    img_size=img_size, acceleration=4, rng=rng, device=device\n",
    ")\n",
    "mask = physics_generator.step()[\"mask\"]\n",
    "\n",
    "physics = dinv.physics.MRI(mask=mask, img_size=img_size, device=device)\n",
    "\n",
    "x = next(iter(DataLoader(knee_dataset))).to(device)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"mask\": mask,\n",
    "        \"y\": physics(x).clamp(-1, 1),\n",
    "    }\n",
    ")\n",
    "print(\"Shapes:\", x.shape, physics.mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52f25f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We can next generate an accelerated single-coil MRI measurement dataset. Let's use knees\n",
    "# for training and brains for testing.\n",
    "#\n",
    "# We can also use the physics generator to randomly sample a new mask per\n",
    "# sample, and save the masks alongside the measurements.\n",
    "#\n",
    "# Note that you could alternatively train using `online_measurements`, where you can generate\n",
    "# random measurements on the fly.\n",
    "#\n",
    "\n",
    "dataset_path = dinv.datasets.generate_dataset(\n",
    "    train_dataset=knee_dataset,\n",
    "    test_dataset=brain_dataset,\n",
    "    val_dataset=None,\n",
    "    physics=physics,\n",
    "    physics_generator=physics_generator,\n",
    "    save_physics_generator_params=True,\n",
    "    overwrite_existing=False,\n",
    "    device=device,\n",
    "    save_dir=dinv.utils.get_data_home(),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "train_dataset = dinv.datasets.HDF5Dataset(\n",
    "    dataset_path, split=\"train\", load_physics_generator_params=True\n",
    ")\n",
    "test_dataset = dinv.datasets.HDF5Dataset(\n",
    "    dataset_path, split=\"test\", load_physics_generator_params=True\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "iterator = iter(train_dataloader)\n",
    "\n",
    "x0, y0, params0 = next(iterator)\n",
    "x1, y1, params1 = next(iterator)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"x0\": x0,\n",
    "        \"mask0\": params0[\"mask\"],\n",
    "        \"x1\": x1,\n",
    "        \"mask1\": params1[\"mask\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba0c06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We can also simulate multicoil MRI data. Either pass in ground-truth\n",
    "# coil maps, or pass an integer to simulate simple birdcage coil maps. The\n",
    "# measurements ``y`` are now of shape ``(B, C, N, H, W)``, where ``N`` is\n",
    "# the coil-dimension.\n",
    "#\n",
    "\n",
    "mc_physics = dinv.physics.MultiCoilMRI(img_size=img_size, coil_maps=3, device=device)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"mask\": mask,\n",
    "        \"coil_map_0\": mc_physics.coil_maps.abs()[:, 0, ...],\n",
    "        \"coil_map_1\": mc_physics.coil_maps.abs()[:, 1, ...],\n",
    "        \"coil_map_2\": mc_physics.coil_maps.abs()[:, 2, ...],\n",
    "        \"RSS\": mc_physics.A_adjoint_A(x, mask=mask, rss=True),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110d751",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2. Train an accelerated MRI problem with neural networks\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Next, we train a neural network to solve the MRI inverse problem. We provide various\n",
    "# models specifically used for MRI reconstruction. These are unrolled\n",
    "# networks which require a backbone denoiser, such as UNet or DnCNN:\n",
    "#\n",
    "\n",
    "denoiser = dinv.models.UNet(\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    scales=2,\n",
    ")\n",
    "\n",
    "denoiser = dinv.models.DnCNN(\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    pretrained=None,\n",
    "    depth=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc7f38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# These backbones can be used within specific MRI models, such as\n",
    "# VarNet :footcite:t:`hammernik2018learning`, E2E-VarNet :footcite:t:`sriram2020end` and MoDL :footcite:t:`aggarwal2018modl`,\n",
    "# for which we provide implementations:\n",
    "#\n",
    "\n",
    "model = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"varnet\").to(device)\n",
    "\n",
    "model = dinv.models.MoDL(denoiser, num_iter=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our architecture defined, we can train it with supervised or self-supervised (using Equivariant\n",
    "# Imaging) loss. We use the PSNR metric on the complex magnitude.\n",
    "#\n",
    "# For the sake of speed in this example, we only use a very small 2-layer DnCNN inside an unrolled\n",
    "# network with 2 cascades, and train with 2 images for 1 epoch.\n",
    "#\n",
    "\n",
    "loss = dinv.loss.SupLoss()\n",
    "loss = dinv.loss.EILoss(transform=dinv.transform.CPABDiffeomorphism())\n",
    "\n",
    "trainer = dinv.Trainer(\n",
    "    model=model,\n",
    "    physics=physics,\n",
    "    optimizer=torch.optim.Adam(model.parameters()),\n",
    "    train_dataloader=train_dataloader,\n",
    "    metrics=dinv.metric.PSNR(complex_abs=True),\n",
    "    epochs=1,\n",
    "    show_progress_bar=False,\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ede0e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# To improve results in the case of this very short training, we start training from a pretrained model state (trained on 900 images):\n",
    "\n",
    "url = dinv.models.utils.get_weights_url(\n",
    "    model_name=\"demo\", file_name=\"demo_tour_mri.pth\"\n",
    ")\n",
    "ckpt = torch.hub.load_state_dict_from_url(\n",
    "    url, map_location=lambda storage, loc: storage, file_name=\"demo_tour_mri.pth\"\n",
    ")\n",
    "trainer.model.load_state_dict(ckpt[\"state_dict\"])  # load the state dict\n",
    "trainer.optimizer.load_state_dict(ckpt[\"optimizer\"])  # load the optimizer state dict\n",
    "\n",
    "model = trainer.train()  # train the model\n",
    "trainer.plot_images = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd4d4f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Now that our model is trained, we can test it. Notice that we improve the PSNR compared to the zero-filled\n",
    "# reconstruction, both on the train (knee) set and the test (brain) set:\n",
    "\n",
    "# sphinx_gallery_start_ignore\n",
    "# sphinx_gallery_multi_image = \"single\"\n",
    "# sphinx_gallery_end_ignore\n",
    "\n",
    "_ = trainer.test(train_dataloader)\n",
    "\n",
    "_ = trainer.test(DataLoader(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load raw FastMRI data\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# It is also possible to use the raw data directly.\n",
    "# The raw multi-coil FastMRI train/validation data is provided as pairs of ``(x, y)`` where\n",
    "# ``y`` are the fully-sampled k-space measurements of arbitrary size, and\n",
    "# ``x`` are the cropped root-sum-square (RSS) magnitude reconstructions.\n",
    "# Let's download a sample volume and check out its middle slice.\n",
    "#\n",
    "\n",
    "dinv.datasets.download_archive(\n",
    "    dinv.utils.get_image_url(\"demo_fastmri_brain_multicoil.h5\"),\n",
    "    dinv.utils.get_data_home() / \"brain\" / \"fastmri.h5\",\n",
    ")\n",
    "\n",
    "dataset = dinv.datasets.FastMRISliceDataset(\n",
    "    dinv.utils.get_data_home() / \"brain\", slice_index=\"middle\"\n",
    ")\n",
    "\n",
    "x, y = next(iter(DataLoader(dataset)))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "img_size, kspace_shape = x.shape[-2:], y.shape[-2:]\n",
    "n_coils = y.shape[2]\n",
    "\n",
    "print(\"Shapes:\", x.shape, y.shape)  # x (B, 1, W, W); y (B, C, N, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we can relate ``x`` and fully-sampled ``y`` using our\n",
    "# :class:`deepinv.physics.MultiCoilMRI` (note that since we are not\n",
    "# provided with the ground-truth coil-maps, we can only perform the\n",
    "# adjoint operator).\n",
    "#\n",
    "\n",
    "physics = dinv.physics.MultiCoilMRI(\n",
    "    img_size=img_size,\n",
    "    mask=torch.ones(kspace_shape, device=device),\n",
    "    coil_maps=torch.ones(\n",
    "        (n_coils,) + kspace_shape, dtype=torch.complex64, device=device\n",
    "    ),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "x_rss = physics.A_adjoint(y, rss=True, crop=True)\n",
    "\n",
    "assert torch.allclose(x, x_rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also pre-estimate coil sensitivity maps using ESPIRiT from the raw data.\n",
    "#\n",
    "\n",
    "dataset = dinv.datasets.FastMRISliceDataset(\n",
    "    dinv.utils.get_data_home() / \"brain\",\n",
    "    slice_index=\"middle\",\n",
    "    transform=dinv.datasets.MRISliceTransform(\n",
    "        estimate_coil_maps=True,\n",
    "        acs=15,  # Num. low frequency, fix to 15\n",
    "    ),\n",
    ")\n",
    "\n",
    "x, y, params = next(iter(DataLoader(dataset)))\n",
    "\n",
    "physics.update(**params)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\"x\": x, \"maps0\": physics.coil_maps[:, 0], \"maps1\": physics.coil_maps[:, 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e53f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train using raw data\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# For training with multicoil raw data, we can simulate random masks **on-the-fly**:\n",
    "#\n",
    "\n",
    "dataset = dinv.datasets.FastMRISliceDataset(\n",
    "    dinv.utils.get_data_home() / \"brain\",\n",
    "    slice_index=\"middle\",\n",
    "    transform=dinv.datasets.MRISliceTransform(\n",
    "        mask_generator=dinv.physics.generator.GaussianMaskGenerator(\n",
    "            img_size=kspace_shape,\n",
    "            acceleration=4,\n",
    "        ),\n",
    "        seed_mask_generator=False,  # More diversity during training\n",
    "        estimate_coil_maps=False,  # Set to true if coil maps are not already set in physics.\n",
    "        # This will use ACS size from mask generator. If mask generator is None, then try find ACS size from metadata.\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Note if the data is already undersampled raw kspace data (e.g. FastMRI test set)\n",
    "# you can also easily directly load it and their associated masks for testing or training\n",
    "# (optionally specify separate target folder if targets are in a different folder):\n",
    "#\n",
    "# ::\n",
    "#\n",
    "#         dataset = dinv.datasets.FastMRISliceDataset(\n",
    "#             root=root,\n",
    "#             target_root=target_root,\n",
    "#             transform=dinv.datasets.MRISliceTransform()\n",
    "#         )\n",
    "#\n",
    "# We use the E2E-VarNet model designed for\n",
    "# multicoil MRI. For this example, we do not perform joint coil sensitivity map estimation and\n",
    "# simply assume they are flat. If you want to estimate the maps, either pass a model\n",
    "# as the ``sensitivity_model`` parameter, or use a different model which uses precomputed maps.\n",
    "#\n",
    "\n",
    "model = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"e2e-varnet\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df05f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We also need to modify the metrics used to crop the model output and take the magnitude when\n",
    "# comparing to the cropped magnitude RSS targets:\n",
    "#\n",
    "\n",
    "\n",
    "def crop(x_net, x):\n",
    "    \"\"\"Crop to GT shape then take magnitude.\"\"\"\n",
    "    return dinv.utils.MRIMixin().rss(\n",
    "        dinv.utils.MRIMixin().crop(x_net, shape=x.shape), multicoil=False\n",
    "    )\n",
    "\n",
    "\n",
    "class CropPSNR(dinv.metric.PSNR):\n",
    "    def forward(self, x_net=None, x=None, *args, **kwargs):\n",
    "        return super().forward(crop(x_net, x), x, *args, **kwargs)\n",
    "\n",
    "\n",
    "class CropMSE(dinv.metric.MSE):\n",
    "    def forward(self, x_net=None, x=None, *args, **kwargs):\n",
    "        return super().forward(crop(x_net, x), x, *args, **kwargs)\n",
    "\n",
    "\n",
    "trainer = dinv.Trainer(\n",
    "    model=model,\n",
    "    physics=physics,\n",
    "    losses=dinv.loss.SupLoss(metric=CropMSE()),\n",
    "    metrics=CropPSNR(),\n",
    "    optimizer=torch.optim.Adam(model.parameters()),\n",
    "    train_dataloader=DataLoader(dataset),\n",
    "    epochs=1,\n",
    "    save_path=None,\n",
    "    show_progress_bar=False,\n",
    "    device=device,\n",
    ")\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc78cc8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 5. Explore 3D MRI\n",
    "# ~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# We can also simulate 3D MRI data.\n",
    "# Here, we use a demo 3D brain volume of shape ``(181, 217, 181)`` from the\n",
    "# `BrainWeb <https://brainweb.bic.mni.mcgill.ca/brainweb/>`_ dataset\n",
    "# and simulate 3D single-coil or multi-coil Fourier measurements using\n",
    "# :class:`deepinv.physics.MRI` or\n",
    "# :class:`deepinv.physics.MultiCoilMRI`.\n",
    "#\n",
    "\n",
    "x = (\n",
    "    dinv.utils.load_np_url(\n",
    "        \"https://huggingface.co/datasets/deepinv/images/resolve/main/brainweb_t1_ICBM_1mm_subject_0.npy?download=true\"\n",
    "    )\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "x = torch.cat([x, torch.zeros_like(x)], dim=1)  # add imaginary dimension\n",
    "\n",
    "print(x.shape)  # (B, C, D, H, W) where D is depth\n",
    "\n",
    "physics = dinv.physics.MultiCoilMRI(img_size=x.shape[1:], three_d=True, device=device)\n",
    "physics = dinv.physics.MRI(img_size=x.shape[1:], three_d=True, device=device)\n",
    "\n",
    "dinv.utils.plot_ortho3D([x, physics(x)], titles=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Explore dynamic MRI\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Finally, we show how to use the dynamic MRI for image sequence data of\n",
    "# shape ``(B, C, T, H, W)`` where ``T`` is the time dimension. Note that\n",
    "# this is also compatible with 3D MRI. We use dynamic MRI data from the\n",
    "# `CMRxRecon <https://cmrxrecon.github.io/>`_ challenge of cardiac cine\n",
    "# sequences and load them using :class:`deepinv.datasets.CMRxReconSliceDataset`\n",
    "# provided in deepinv. We download demo data from the first patient\n",
    "# including ground truth images, undersampled kspace, and associated masks:\n",
    "#\n",
    "\n",
    "dinv.datasets.download_archive(\n",
    "    dinv.utils.get_image_url(\"CMRxRecon.zip\"),\n",
    "    dinv.utils.get_data_home() / \"CMRxRecon.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "\n",
    "dataset = dinv.datasets.CMRxReconSliceDataset(\n",
    "    dinv.utils.get_data_home() / \"CMRxRecon\",\n",
    ")\n",
    "\n",
    "x, y, params = next(iter(DataLoader(dataset)))\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Ground truth: {x.shape} (B, C, T, H, W)\n",
    "    Measurements: {y.shape}\n",
    "    Acc. mask: {params[\"mask\"].shape}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2292e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic MRI data is directly compatible with existing functionality.\n",
    "# For example, you can train with this data by passing the dataset to\n",
    "# :class:`deepinv.Trainer`, which will automatically load in the data\n",
    "# ``x, y, params``. Or, you can use the data directly with the physics\n",
    "# :class:`deepinv.physics.DynamicMRI`.\n",
    "#\n",
    "# You can also pass in a custom k-t acceleration mask generator to\n",
    "# generate random time-varying masks:\n",
    "#\n",
    "\n",
    "physics_generator = dinv.physics.generator.EquispacedMaskGenerator(\n",
    "    img_size=x.shape[1:],\n",
    "    acceleration=16,\n",
    ")\n",
    "physics = dinv.physics.DynamicMRI(img_size=(512, 256), device=device)\n",
    "\n",
    "dataset = dinv.datasets.CMRxReconSliceDataset(\n",
    "    dinv.utils.get_data_home() / \"CMRxRecon\",\n",
    "    mask_generator=physics_generator,\n",
    "    mask_dir=None,\n",
    ")\n",
    "\n",
    "x, y, params = next(iter(DataLoader(dataset)))\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "params = {k: v.to(device) for k, v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a video plotting function, :class:`deepinv.utils.plot_videos`. Here, we\n",
    "# visualize t=5 frames of the ground truth ``x``, the mask, and the zero-filled\n",
    "# reconstruction ``x_zf`` (and crop to square for better visibility):\n",
    "#\n",
    "\n",
    "x_zf = physics.A_adjoint(y, **params)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        f\"t={i}\": torch.cat([x[:, :, i], params[\"mask\"][:, :, i], x_zf[:, :, i]])[\n",
    "            ..., 128:384, :\n",
    "        ]\n",
    "        for i in range(5)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54857f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
