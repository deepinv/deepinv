{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Imaging inverse problems with adversarial networks\n\nThis example shows you how to train various networks using adversarial\ntraining for deblurring problems. We demonstrate running training and\ninference using a conditional GAN (i.e. DeblurGAN), CSGM, AmbientGAN and\nUAIR implemented in the library, and how to simply train\nyour own GAN by using :class:`deepinv.training.AdversarialTrainer`. These\nexamples can also be easily extended to train more complicated GANs such\nas CycleGAN.\n\nThis example is based on the papers DeblurGAN :footcite:p:`kupyn2018deblurgan`,\nCompressed Sensing using Generative Models (CSGM) :footcite:p:`bora2017compressed`,\nAmbiantGAN :footcite:p:`bora2018ambientgan`, and Unsupervised Adversarial Image Reconstruction (UAIR) :footcite:p:`pajot2019unsupervised`.\n\nAdversarial networks are characterized by the addition of an adversarial\nloss $\\mathcal{L}_\\text{adv}$ to the standard reconstruction loss:\n\n\\begin{align}\\mathcal{L}_\\text{adv}(x,\\hat x;D)=\\mathbb{E}_{x\\sim p_x}\\left[q(D(x))\\right]+\\mathbb{E}_{\\hat x\\sim p_{\\hat x}}\\left[q(1-D(\\hat x))\\right]\\end{align}\n\nwhere $D(\\cdot)$ is the discriminator model, $x$ is the\nreference image, $\\hat x$ is the estimated reconstruction,\n$q(\\cdot)$ is a quality function (e.g $q(x)=x$ for WGAN).\nTraining alternates between generator $G$ and discriminator\n$D$ in a minimax game. When there are no ground truths (i.e.\nunsupervised), this may be defined on the measurements $y$\ninstead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.transforms import Compose, ToTensor, CenterCrop, Resize\n\nimport deepinv as dinv\nfrom deepinv.loss import adversarial\nfrom deepinv.utils import get_data_home\nfrom deepinv.physics.generator import MotionBlurGenerator\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nBASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"measurments\"\nORGINAL_DATA_DIR = get_data_home() / \"Urban100\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate dataset\nIn this example we use the Urban100 dataset resized to 128x128. We apply random\nmotion blur physics using\n:class:`deepinv.physics.generator.MotionBlurGenerator`, and save the data\nusing :func:`deepinv.datasets.generate_dataset`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.Blur(padding=\"circular\", device=device)\nblur_generator = MotionBlurGenerator((11, 11), device=device)\n\ndataset = dinv.datasets.Urban100HR(\n    root=ORGINAL_DATA_DIR,\n    download=True,\n    transform=Compose([ToTensor(), Resize(256), CenterCrop(128)]),\n)\n\ntrain_dataset, test_dataset = random_split(dataset, (0.8, 0.2))\n\n# Generate data pairs x,y offline using a physics generator\ndataset_path = dinv.datasets.generate_dataset(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    physics=physics,\n    physics_generator=blur_generator,\n    device=device,\n    save_dir=DATA_DIR,\n    batch_size=1,\n)\n\ntrain_dataloader = DataLoader(\n    dinv.datasets.HDF5Dataset(\n        dataset_path, train=True, load_physics_generator_params=True\n    ),\n    shuffle=True,\n)\ntest_dataloader = DataLoader(\n    dinv.datasets.HDF5Dataset(\n        dataset_path, train=False, load_physics_generator_params=True\n    ),\n    shuffle=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define models\n\nWe first define reconstruction network (i.e conditional generator) and\ndiscriminator network to use for adversarial training. For demonstration\nwe use a simple U-Net as the reconstruction network and the\ndiscriminator from PatchGAN :footcite:p:`isola2017image`, but\nthese can be replaced with any architecture e.g transformers, unrolled\netc. Further discriminator models are in `adversarial models <adversarial>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_models(model=None, D=None, lr_g=1e-4, lr_d=1e-4, device=device):\n    if model is None:\n        model = dinv.models.UNet(\n            in_channels=3,\n            out_channels=3,\n            scales=2,\n            circular_padding=True,\n            batch_norm=False,\n        ).to(device)\n\n    if D is None:\n        D = dinv.models.PatchGANDiscriminator(n_layers=2, batch_norm=False).to(device)\n\n    optimizer = dinv.training.adversarial.AdversarialOptimizer(\n        torch.optim.Adam(model.parameters(), lr=lr_g, weight_decay=1e-8),\n        torch.optim.Adam(D.parameters(), lr=lr_d, weight_decay=1e-8),\n    )\n    scheduler = dinv.training.adversarial.AdversarialScheduler(\n        torch.optim.lr_scheduler.StepLR(optimizer.G, step_size=5, gamma=0.9),\n        torch.optim.lr_scheduler.StepLR(optimizer.D, step_size=5, gamma=0.9),\n    )\n\n    return model, D, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional GAN training\n\nConditional GANs :footcite:p:`kupyn2018deblurgan` are a type of GAN where the generator is conditioned on a label or input.\nIn the context of imaging, this can be used to generate images from a given measurement.\nIn this example, we use a simple U-Net as the generator\nand a PatchGAN discriminator. The forward pass of the generator is given by:\n\n**Conditional GAN** forward pass:\n\n\\begin{align}\\hat x = G(y)\\end{align}\n\n**Conditional GAN** loss:\n\n\\begin{align}\\mathcal{L}=\\mathcal{L}_\\text{sup}(\\hat x, x)+\\mathcal{L}_\\text{adv}(\\hat x, x;D)\\end{align}\n\nwhere $\\mathcal{L}_\\text{sup}$ is a supervised loss such as\npixel-wise MSE or VGG Perceptual Loss.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "G, D, optimizer, scheduler = get_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next define pixel-wise and adversarial losses as defined above. We use the\nMSE for the supervised pixel-wise metric for simplicity but this can be\neasily replaced with a perceptual loss if desired.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_g = [\n    dinv.loss.SupLoss(metric=torch.nn.MSELoss()),\n    adversarial.SupAdversarialGeneratorLoss(device=device),\n]\nloss_d = adversarial.SupAdversarialDiscriminatorLoss(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to train the networks using :class:`deepinv.training.AdversarialTrainer`.\nWe load the pretrained models that were trained in the exact same way after 50 epochs,\nand fine-tune the model for 1 epoch for a quick demo.\nYou can find the pretrained models on HuggingFace https://huggingface.co/deepinv/adversarial-demo.\nTo train from scratch, simply comment out the model loading code and increase the number of epochs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"adversarial-demo\", \"deblurgan_model.pth\"),\n    map_location=lambda s, _: s,\n)\n\nG.load_state_dict(ckpt[\"state_dict\"])\nD.load_state_dict(ckpt[\"state_dict_D\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])\n\ntrainer = dinv.training.AdversarialTrainer(\n    model=G,\n    D=D,\n    physics=physics,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=1,\n    losses=loss_g,\n    losses_d=loss_d,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=device,\n)\n\nG = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the trained model and plot the results. We compare to the pseudo-inverse as a baseline.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.plot_images = True\ntrainer.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UAIR training\n\nUnsupervised Adversarial Image Reconstruction (UAIR) :footcite:p:`pajot2019unsupervised`\nis a method for solving inverse problems using generative models. In this\nexample, we use a simple U-Net as the generator and discriminator, and\ntrain using the adversarial loss. The forward pass of the generator is defined as:\n\n**UAIR** forward pass:\n\n\\begin{align}\\hat x = G(y),\\end{align}\n\n**UAIR** loss:\n\n\\begin{align}\\mathcal{L}=\\mathcal{L}_\\text{adv}(\\hat y, y;D)+\\lVert \\forw{\\inverse{\\hat y}}- \\hat y\\rVert^2_2,\\quad\\hat y=\\forw{\\hat x}.\\end{align}\n\nWe next load the models and construct losses as defined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "G, D, optimizer, scheduler = get_models(\n    lr_g=1e-4, lr_d=4e-4\n)  # learning rates from original paper\n\nloss_g = adversarial.UAIRGeneratorLoss(device=device)\nloss_d = adversarial.UnsupAdversarialDiscriminatorLoss(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to train the networks using :class:`deepinv.training.AdversarialTrainer`.\nLike above, we load a pretrained model trained in the exact same way for 50 epochs,\nand fine-tune here for a quick demo with 1 epoch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"adversarial-demo\", \"uair_model.pth\"),\n    map_location=lambda s, _: s,\n)\n\nG.load_state_dict(ckpt[\"state_dict\"])\nD.load_state_dict(ckpt[\"state_dict_D\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])\n\ntrainer = dinv.training.AdversarialTrainer(\n    model=G,\n    D=D,\n    physics=physics,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=1,\n    losses=loss_g,\n    losses_d=loss_d,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=device,\n)\nG = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the trained model and plot the results:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.plot_images = True\ntrainer.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSGM / AmbientGAN training\n\nCompressed Sensing using Generative Models (CSGM) :footcite:p:`bora2017compressed` and AmbientGAN :footcite:p:`bora2018ambientgan` are two methods for solving inverse problems\nusing generative models. CSGM uses a generative model to solve the inverse problem by optimising the latent\nspace of the generator. AmbientGAN uses a generative model to solve the inverse problem by optimising the\nmeasurements themselves. Both methods are trained using an adversarial loss; the main difference is that CSGM requires\na ground truth dataset (supervised loss), while AmbientGAN does not (unsupervised loss).\n\nIn this example, we use a DCGAN as the\ngenerator and discriminator, and train using the adversarial loss. The forward pass of the generator is given by:\n\n**CSGM** forward pass at train time:\n\n\\begin{align}\\hat x = \\inverse{z},\\quad z\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_k)\\end{align}\n\n**CSGM**/**AmbientGAN** forward pass at eval time:\n\n\\begin{align}\\hat x = \\inverse{\\hat z}\\quad\\text{s.t.}\\quad\\hat z=\\operatorname*{argmin}_z \\lVert \\forw{\\inverse{z}}-y\\rVert _2^2\\end{align}\n\n**CSGM** loss:\n\n\\begin{align}\\mathcal{L}=\\mathcal{L}_\\text{adv}(\\hat x, x;D)\\end{align}\n\n**AmbientGAN** loss (where $\\forw{\\cdot}$ is the physics):\n\n\\begin{align}\\mathcal{L}=\\mathcal{L}_\\text{adv}(\\forw{\\hat x}, y;D)\\end{align}\n\nWe next load the models and construct losses as defined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "G = dinv.models.CSGMGenerator(\n    dinv.models.DCGANGenerator(output_size=128, nz=100, ngf=32), inf_tol=1e-2\n).to(device)\nD = dinv.models.DCGANDiscriminator(ndf=32).to(device)\n_, _, optimizer, scheduler = get_models(\n    model=G, D=D, lr_g=2e-4, lr_d=2e-4\n)  # learning rates from original paper\n\n# For AmbientGAN:\nloss_g = adversarial.UnsupAdversarialGeneratorLoss(device=device)\nloss_d = adversarial.UnsupAdversarialDiscriminatorLoss(device=device)\n\n# For CSGM:\nloss_g = adversarial.SupAdversarialGeneratorLoss(device=device)\nloss_d = adversarial.SupAdversarialDiscriminatorLoss(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we can now train our models. Since inference is very\nslow for CSGM/AmbientGAN as it requires an optimisation, we only do one\nevaluation at the end. Note the train PSNR is meaningless as this\ngenerative model is trained on random latents.\nLike above, we load a pretrained model trained in the exact same way for 50 epochs,\nand fine-tune here for a quick demo with 1 epoch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"adversarial-demo\", \"csgm_model.pth\"),\n    map_location=lambda s, _: s,\n)\n\nG.load_state_dict(ckpt[\"state_dict\"])\nD.load_state_dict(ckpt[\"state_dict_D\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])\n\ntrainer = dinv.training.AdversarialTrainer(\n    model=G,\n    D=D,\n    physics=physics,\n    train_dataloader=train_dataloader,\n    epochs=1,\n    losses=loss_g,\n    losses_d=loss_d,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=device,\n)\nG = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eventually, we run evaluation of the generative model by running test-time optimisation\nusing test measurements. Note that we do not get great results as CSGM /\nAmbientGAN relies on large datasets of diverse samples, and we run the\noptimisation to a relatively high tolerance for speed. Improve the results by\nrunning the optimisation for longer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.test(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}