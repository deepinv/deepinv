{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b905e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference and fine-tune a foundation model\n",
    "==========================================\n",
    "\n",
    "This example shows how to perform inference on and fine-tune the Reconstruct Anything Model (RAM) foundation model :footcite:p:`terris2025reconstruct` to solve inverse problems.\n",
    "\n",
    "The :class:`Reconstruct Anything Model <deepinv.models.RAM>` is a model that has been trained to work on a large\n",
    "variety of linear image reconstruction tasks and datasets (deblurring, inpainting, denoising, tomography, MRI, etc.)\n",
    "and is robust to a wide variety of imaging domains.\n",
    "\n",
    ".. tip::\n",
    "\n",
    "    * Want to use your own dataset? See :ref:`sphx_glr_auto_examples_basics_demo_custom_dataset.py`\n",
    "    * Want to use your own physics? See :ref:`sphx_glr_auto_examples_basics_demo_custom_physics.py`\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "import torch\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = dinv.models.RAM(device=device, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Zero-shot inference\n",
    "# ----------------------\n",
    "#\n",
    "# First, let's evaluate the zero-shot inference performance of the foundation model.\n",
    "#\n",
    "# Accelerated medical imaging\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Here, we demonstrated reconstructing brain MRI from an accelerated noisy MRI scan from `FastMRI <https://fastmri.med.nyu.edu/>`_:\n",
    "\n",
    "x = dinv.utils.load_example(\"demo_mini_subset_fastmri_brain_0.pt\", device=device)\n",
    "\n",
    "# Define physics\n",
    "physics = dinv.physics.MRI(noise_model=dinv.physics.GaussianNoise(0.05), device=device)\n",
    "\n",
    "physics_generator = dinv.physics.generator.GaussianMaskGenerator(\n",
    "    (320, 320), device=device\n",
    ")\n",
    "\n",
    "# Generate measurement\n",
    "y = physics(x, **physics_generator.step())\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    x_hat = model(y, physics)\n",
    "    x_lin = physics.A_adjoint(y)\n",
    "\n",
    "psnr = dinv.metric.PSNR()\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Ground truth\": x,\n",
    "        f\"Linear inverse\": x_lin,\n",
    "        f\"Pretrained RAM\": x_hat,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(x, x_lin).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational photography\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# Joint random motion deblurring and denoising, using a cropped image from color BSD:\n",
    "\n",
    "x = dinv.utils.load_example(\"CBSD_0010.png\", img_size=(200, 200), device=device)\n",
    "\n",
    "physics = dinv.physics.BlurFFT(\n",
    "    img_size=x.shape[1:],\n",
    "    noise_model=dinv.physics.GaussianNoise(sigma=0.05),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# fmt: off\n",
    "physics_generator = ( \n",
    "    dinv.physics.generator.MotionBlurGenerator((31, 31), l=2.0, sigma=2.4, device=device) +\n",
    "    dinv.physics.generator.SigmaGenerator(sigma_min=0.001, sigma_max=0.2, device=device)\n",
    ")\n",
    "# fmt: on\n",
    "\n",
    "y = physics(x, **physics_generator.step())\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat = model(y, physics)\n",
    "    x_lin = physics.A_adjoint(y)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Ground truth\": x,\n",
    "        f\"Linear inverse\": x_lin,\n",
    "        f\"Pretrained RAM\": x_hat,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(x, x_lin).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomography\n",
    "# ~~~~~~~~~~\n",
    "# Computed Tomography with limited angles\n",
    "# using data from the `The Cancer Imaging Archive <https://link.springer.com/article/10.1007/s10278-013-9622-7>`_ of lungs:\n",
    "#\n",
    "\n",
    "x = dinv.utils.load_example(\"CT100_256x256_0.pt\", device=device)\n",
    "\n",
    "physics = dinv.physics.Tomography(\n",
    "    img_width=256,\n",
    "    angles=10,\n",
    "    normalize=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "y = physics(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat = model(y, physics)\n",
    "    x_lin = physics.A_dagger(y)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Ground truth\": x,\n",
    "        f\"FBP pseudo-inverse\": x_lin,\n",
    "        f\"Pretrained RAM\": x_hat,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(x, x_lin).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9fc41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Remote sensing\n",
    "# ~~~~~~~~~~~~~~\n",
    "# Satellite denoising with Poisson-Gaussian noise using urban data from the `WorldView-3 satellite <https://earth.esa.int/eogateway/missions/worldview-3>`_\n",
    "# over Jacksonville:\n",
    "#\n",
    "\n",
    "x = dinv.utils.load_example(\"JAX_018_011_RGB.tif\", device=device)[..., :300, :300]\n",
    "\n",
    "physics = dinv.physics.Denoising(\n",
    "    noise_model=dinv.physics.PoissonGaussianNoise(sigma=0.1, gain=0.1)\n",
    ")\n",
    "\n",
    "y = physics(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat = model(y, physics)\n",
    "    # Alternatively, use the model without physics:\n",
    "    # x_hat = model(y, sigma=0.1, gain=0.1)\n",
    "    x_lin = physics.A_adjoint(y)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Ground truth\": x,\n",
    "        f\"Linear inverse\": x_lin,\n",
    "        f\"Pretrained RAM\": x_hat,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(x, x_lin).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fine-tuning\n",
    "# --------------\n",
    "# As with all models, there may be a drop in performance when used zero-shot on problems or data outside those seen during training.\n",
    "#\n",
    "# For instance, RAM is not trained on image demosaicing:\n",
    "\n",
    "x = dinv.utils.load_example(\"butterfly.png\", img_size=(127, 129), device=device)\n",
    "\n",
    "physics = dinv.physics.Demosaicing(\n",
    "    img_size=x.shape[1:], noise_model=dinv.physics.PoissonNoise(0.1), device=device\n",
    ")\n",
    "\n",
    "# Generate measurement\n",
    "y = physics(x)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    x_hat = model(y, physics)\n",
    "\n",
    "# Show results\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Original\": x,\n",
    "        f\"Measurement\": y,\n",
    "        f\"Reconstruction\": x_hat,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(x, y).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beaf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve results, we can fine-tune the model on our problem and data,\n",
    "# **even in the absence of ground truth data**, using a :ref:`self-supervised loss <self-supervised-losses>`,\n",
    "# and **even on a single image only**.\n",
    "#\n",
    "# Here, since this example is run in a no-GPU environment, we will use a small patch of the image to speed up training,\n",
    "# but in practice, we can use the full image.\n",
    "#\n",
    "# .. note::\n",
    "#     You can also fine-tune on larger datasets if you want, by replacing the :ref:`dataset <datasets>`.\n",
    "\n",
    "# Take small patch\n",
    "x_train = x[..., :64, :64]\n",
    "\n",
    "physics_train = dinv.physics.Demosaicing(\n",
    "    img_size=x_train.shape[1:],\n",
    "    noise_model=dinv.physics.PoissonNoise(0.1, clip_positive=True),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "y_train = physics_train(x_train)\n",
    "\n",
    "# Define training loss\n",
    "losses = [\n",
    "    dinv.loss.R2RLoss(),\n",
    "    dinv.loss.EILoss(dinv.transform.Shift(shift_max=0.4), weight=0.1),\n",
    "]\n",
    "\n",
    "dataset = dinv.datasets.TensorDataset(y=y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da424e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# We fine-tune using early stopping on a validation set, again without ground truth.\n",
    "# We use a small patch of another set of measurements as validation.\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dinv.datasets.TensorDataset(\n",
    "        y=physics_train(\n",
    "            dinv.utils.load_example(\"leaves.png\", device=device)[..., :64, :64]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "max_epochs = 20\n",
    "trainer = dinv.Trainer(\n",
    "    model=model,\n",
    "    physics=physics_train,\n",
    "    eval_interval=5,\n",
    "    ckp_interval=max_epochs - 1,\n",
    "    metrics=None,\n",
    "    compute_eval_losses=True,  # use self-supervised loss for evaluation\n",
    "    early_stop_on_losses=True,  # stop using self-supervised eval loss\n",
    "    early_stop=2,  # early stop after 2 evals without improvement\n",
    "    device=device,\n",
    "    losses=losses,\n",
    "    epochs=max_epochs,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=5e-5),\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n",
    ")\n",
    "\n",
    "finetuned_model = trainer.train()\n",
    "\n",
    "finetuned_model = trainer.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use the fine-tuned model to reconstruct the image from the measurement `y`.\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat_ft = finetuned_model(y, physics)\n",
    "\n",
    "# Show results\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        \"Original\": x,\n",
    "        f\"Measurement\": y,\n",
    "        f\"Zero-shot \\nReconstruction\": x_hat,\n",
    "        f\"Fine-tuned \\nReconstruction\": x_hat_ft,\n",
    "    },\n",
    "    subtitles=[\n",
    "        \"PSNR:\",\n",
    "        f\"{psnr(y, x).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat).item():.2f} dB\",\n",
    "        f\"{psnr(x, x_hat_ft).item():.2f} dB\",\n",
    "    ],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
