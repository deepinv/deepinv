{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82917a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Implementing DiffPIR\n",
    "====================\n",
    "\n",
    "In this tutorial, we revisit the implementation of the DiffPIR diffusion algorithm for image reconstruction from :footcite:t:`zhu2023denoising`.\n",
    "The full algorithm is implemented in :class:`deepinv.sampling.DiffPIR`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1856d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import deepinv as dinv\n",
    "from deepinv.utils.plotting import plot\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.utils import load_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089bea9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Generate an inverse problem\n",
    "# ---------------------------\n",
    "#\n",
    "# We first generate a deblurring problem with the butterfly image. We use a square blur kernel of size 5x5 and\n",
    "# Gaussian noise with standard deviation 12.75/255.0.\n",
    "#\n",
    "# .. note::\n",
    "#           We work with an image of size 64x64 to reduce the computational time of this example.\n",
    "#           The algorithm works best with images of size 256x256.\n",
    "#\n",
    "\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_true = load_example(\"69037.png\", img_size=256, device=device)\n",
    "\n",
    "x = x_true.clone()\n",
    "mask = torch.ones_like(x)\n",
    "mask[:, :, 50:100, 50:100] = 0\n",
    "mask[:, :, 80:130, 50:100] = 0\n",
    "\n",
    "sigma_noise = 12.75 / 255.0  # noise level\n",
    "physics = dinv.physics.Inpainting(\n",
    "    mask=mask,\n",
    "    img_size=x.shape[1:],\n",
    "    noise_model=dinv.physics.GaussianNoise(sigma=sigma_noise),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "y = physics(x)\n",
    "\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Ground Truth\": x_true,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbbbf5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The DiffPIR algorithm\n",
    "# ---------------------\n",
    "#\n",
    "# Now that the inverse problem is defined, we can apply the DiffPIR algorithm to solve it. The DiffPIR algorithm is\n",
    "# a diffusion algorithm that alternates between a denoising step, a proximal step and a reverse diffusion sampling step.\n",
    "# The algorithm writes as follows, for :math:`t` decreasing from :math:`T` to :math:`1`:\n",
    "#\n",
    "# .. math::\n",
    "#         \\begin{equation*}\n",
    "#         \\begin{aligned}\n",
    "#         \\mathbf{x}_{0}^{t} &= \\denoiser{\\mathbf{x}_t}{\\sqrt{1-\\overline{\\alpha}_t}/\\sqrt{\\overline{\\alpha}_t}} \\\\\n",
    "#         \\widehat{\\mathbf{x}}_{0}^{t} &= \\operatorname{prox}_{2 f(y, \\cdot) /{\\rho_t}}(\\mathbf{x}_{0}^{t}) \\\\\n",
    "#         \\widehat{\\mathbf{\\varepsilon}} &= \\left(\\mathbf{x}_t - \\sqrt{\\overline{\\alpha}_t} \\,\\,\n",
    "#         \\widehat{\\mathbf{x}}_{0}^t\\right)/\\sqrt{1-\\overline{\\alpha}_t} \\\\\n",
    "#         \\mathbf{\\varepsilon}_t &= \\mathcal{N}(0, \\mathbf{I}) \\\\\n",
    "#         \\mathbf{x}_{t-1} &= \\sqrt{\\overline{\\alpha}_t} \\,\\, \\widehat{\\mathbf{x}}_{0}^t + \\sqrt{1-\\overline{\\alpha}_t}\n",
    "#         \\left(\\sqrt{1-\\zeta} \\,\\, \\widehat{\\mathbf{\\varepsilon}} + \\sqrt{\\zeta} \\,\\, \\mathbf{\\varepsilon}_t\\right),\n",
    "#         \\end{aligned}\n",
    "#         \\end{equation*}\n",
    "#\n",
    "# where :math:`\\denoiser{\\cdot}{\\sigma}` is a denoising network with noise level :math:`\\sigma`,\n",
    "# :math:`\\mathcal{N}(0, \\mathbf{I})` is a Gaussian noise\n",
    "# with zero mean and unit variance, :math:`\\zeta` is a parameter that controls the amount of noise added at each\n",
    "# iteration and :math:`f` refers to the data fidelity/measurement consistency term,\n",
    "# which for Gaussian Noise (implemented as :class:`deepinv.optim.data_fidelity.L2`) is given by:\n",
    "#\n",
    "# .. math::\n",
    "#               f(\\mathbf{y}, \\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{y} - \\mathcal{A}(\\mathbf{x})\\|^2\n",
    "#\n",
    "# Note that other data fidelity terms can be used, such as :class:`deepinv.optim.PoissonLikelihood`.\n",
    "# The parameters :math:`(\\overline{\\alpha}_t)_{0\\leq t\\leq T}` and :math:`(\\rho_t)_{0\\leq t\\leq T}` are\n",
    "# sequences of positive numbers, which we will detail later on.\n",
    "#\n",
    "# Let us now implement each step of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising step\n",
    "# --------------\n",
    "#\n",
    "# In this section, we show how to use the denoising diffusion model from DiffPIR.\n",
    "# The denoising step is implemented by a denoising network conditioned on the noise power. The authors\n",
    "# of DiffPIR use a U-Net architecture from :footcite:t:`ho2020denoising`,\n",
    "# which can be loaded as follows:\n",
    "\n",
    "model = dinv.models.DiffUNet(large_model=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4efebc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Before being able to use the pretrained model, we need to define the sequence\n",
    "# :math:`(\\overline{\\alpha}_t)_{0\\leq t\\leq T}`.\n",
    "# The following function returns these sequence:\n",
    "#\n",
    "\n",
    "T = 1000  # Number of timesteps used during training\n",
    "\n",
    "\n",
    "def get_alphas(beta_start=0.1 / 1000, beta_end=20 / 1000, num_train_timesteps=T):\n",
    "    betas = np.linspace(beta_start, beta_end, num_train_timesteps, dtype=np.float32)\n",
    "    betas = torch.from_numpy(betas).to(device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas.cpu(), axis=0)  # This is \\overline{\\alpha}_t\n",
    "    return torch.tensor(alphas_cumprod)\n",
    "\n",
    "\n",
    "alphas_cumprod = get_alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the sequence of interest, there remains to link noise power to the timestep. The following function\n",
    "# returns the timestep corresponding to a given noise power, which is given by\n",
    "#\n",
    "# .. math::\n",
    "#           \\sigma_t = \\sqrt{1-\\overline{\\alpha}_t}/\\overline{\\alpha}_t.\n",
    "\n",
    "\n",
    "sigmas = torch.sqrt(1.0 - alphas_cumprod) / alphas_cumprod.sqrt()\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "t = 100  # choose arbitrary timestep\n",
    "\n",
    "# We can now apply the model to a noisy image. We first generate a noisy image\n",
    "x_noisy = x_true + torch.randn_like(x_true) * sigmas[t]\n",
    "\n",
    "den = model(x_noisy, sigmas[t])\n",
    "\n",
    "plot(\n",
    "    {\n",
    "        \"Noisy Input\": x_noisy,\n",
    "        \"Denoised Image\": den,\n",
    "        \"Error\": den - x_true,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fidelity step\n",
    "# ------------------\n",
    "#\n",
    "# The data fidelity step is easily implemented in this library. We simply need to define a data fidelity function and use\n",
    "# its prox attribute. For instance:\n",
    "\n",
    "data_fidelity = L2()\n",
    "\n",
    "# In order to take a meaningful data fidelity step, it is best if we apply it to denoised measurements.\n",
    "# First, denoise the measurements. To do so, we need to estimate the timestep associated with the noise level of the\n",
    "# measurements. This is done as follows:\n",
    "t_temp = find_nearest(sigmas.cpu().numpy(), sigma_noise * 2)\n",
    "y_denoised = model(y, sigmas[t_temp] / 2.0)\n",
    "\n",
    "# Next, apply the proximity operator of the data fidelity term (this is the data fidelity step). In the algorithm,\n",
    "# the regularization parameter is carefully chosen. Here, for simplicity, we set it to :math:`1/\\sigma`.\n",
    "x_prox = data_fidelity.prox(y_denoised, y, physics, gamma=(1 / sigmas[t]).to(device))\n",
    "\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Denoised Measurement\": y_denoised,\n",
    "        \"Data Fidelity Step\": x_prox,\n",
    "    },\n",
    "    tight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34017594",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Sampling step\n",
    "# -------------\n",
    "#\n",
    "# The last step to be implemented is the DiffPIR sampling step and this can be computed in two steps.\n",
    "# Firstly, we need to compute the effective noise in the estimated reconstruction,\n",
    "# i.e. the residual between the previous\n",
    "# reconstruction and the data fidelity step. This is done as follows:\n",
    "#\n",
    "# .. note::\n",
    "#           The diffPIR algorithm assumes that the images are in the range [-1, 1], whereas standard denoisers\n",
    "#           usually output images in the range [0, 1]. This is why we rescale the images before applying the steps.\n",
    "\n",
    "x_prox_scaled = 2 * x_prox - 1  # Rescale the output of the proximal step in [-1, 1]\n",
    "y_scaled = 2 * y - 1  # Rescale the measurement in [-1, 1]\n",
    "\n",
    "t_i = find_nearest(\n",
    "    sigmas.cpu().numpy(), sigma_noise * 2\n",
    ")  # time step associated with the noise level sigma\n",
    "eps = (y_scaled - alphas_cumprod[t_i].sqrt() * x_prox_scaled) / torch.sqrt(\n",
    "    1.0 - alphas_cumprod[t_i]\n",
    ")  # effective noise\n",
    "\n",
    "# (notice the rescaling)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd99d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, we need to perform the sampling step, which is a linear combination between the estimated noise and\n",
    "# the realizations of a Gaussian white noise. This is done as follows:\n",
    "zeta = 0.3\n",
    "x_sampled_scaled = alphas_cumprod[t_i - 1].sqrt() * x_prox_scaled + torch.sqrt(\n",
    "    1.0 - alphas_cumprod[t_i - 1]\n",
    ") * (np.sqrt(1 - zeta) * eps + np.sqrt(zeta) * torch.randn_like(x))\n",
    "\n",
    "x_sampled = (x_sampled_scaled + 1) / 2  # Rescale the output in [0, 1]\n",
    "\n",
    "imgs = {\n",
    "    \"Measurement\": y,\n",
    "    \"Denoised Measurement\": y_denoised,\n",
    "    \"Data Fidelity Step\": x_prox,\n",
    "    \"Sampling Step\": x_sampled,\n",
    "}\n",
    "plot(imgs, tight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (notice that noise has been added everywhere in the image, including in the masked region)\n",
    "#\n",
    "# Setting the noise and regularization schedules\n",
    "# ----------------------------------------------\n",
    "#\n",
    "# The only remaining step is to set the noise schedule (i.e. the sequence of noise powers and regularization parameters)\n",
    "# appropriately. This is done with the following function:\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#   We only use 30 steps to reduce the computational time of this example. As suggested by the authors of DiffPIR, the\n",
    "#   algorithm works best with ``diffusion_steps = 100``.\n",
    "\n",
    "max_iter = 30  # maximum number of iterations of the DiffPIR algorithm\n",
    "\n",
    "# Useful sequences for the algorithm\n",
    "sqrt_1m_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "reduced_alpha_cumprod = torch.div(\n",
    "    sqrt_1m_alphas_cumprod, sqrt_alphas_cumprod\n",
    ")  # equivalent noise sigma on image\n",
    "sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)\n",
    "\n",
    "\n",
    "# noise schedule of the algorithm\n",
    "def get_noise_schedule(sigma, lambda_=7.0, num_train_timesteps=1000, max_iter=max_iter):\n",
    "    sigmas = []\n",
    "    sigma_ks = []\n",
    "    rhos = []\n",
    "    for i in range(num_train_timesteps):\n",
    "        sigmas.append(reduced_alpha_cumprod[num_train_timesteps - 1 - i])\n",
    "        sigma_ks.append((sqrt_1m_alphas_cumprod[i] / sqrt_alphas_cumprod[i]))\n",
    "        rhos.append(lambda_ * (sigma**2) / (sigma_ks[i] ** 2))\n",
    "    rhos, sigmas = torch.tensor(rhos).to(device), torch.tensor(sigmas).to(device)\n",
    "\n",
    "    seq = np.sqrt(np.linspace(0, num_train_timesteps**2, max_iter))\n",
    "    seq = [int(s) for s in list(seq)]\n",
    "    seq[-1] = seq[-1] - 1\n",
    "\n",
    "    return rhos, sigmas, seq\n",
    "\n",
    "\n",
    "rhos, sigmas, seq = get_noise_schedule(sigma_noise)\n",
    "\n",
    "# Plot the noise and regularization schedules\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.rcParams.update({\"font.size\": 9})\n",
    "plt.subplot(121)\n",
    "plt.plot(\n",
    "    2 / rhos.cpu().numpy()[::-1]\n",
    ")  # Note that the regularization parameter is 2/rho and not rho\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.ylabel(r\"$\\rho$\")\n",
    "plt.subplot(122)\n",
    "plt.plot(sigmas.cpu().numpy()[::-1])\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.ylabel(r\"$\\sigma$\")\n",
    "plt.suptitle(\"Regularisation parameter and noise schedules (fully sampled)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876511ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# In the algorithm, we will only use sub-sampled versions of the noise and regularization schedules. Let's visualize\n",
    "# those.\n",
    "\n",
    "list_sigmas_algo = [sigmas[seq[i]].cpu().item() for i in range(max_iter)]\n",
    "list_rhos_algo = [rhos[seq[i]].cpu().item() for i in range(max_iter)]\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(\n",
    "    2 / torch.tensor(list_rhos_algo).cpu().numpy()\n",
    ")  # Note that the regularization parameter is 2/rho and not rho\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.ylabel(r\"$\\rho$\")\n",
    "plt.subplot(122)\n",
    "plt.plot(list_sigmas_algo)\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.ylabel(r\"$\\sigma$\")\n",
    "plt.suptitle(f\"Regularisation parameter and noise schedules (for {max_iter} steps)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together: the DiffPIR algorithm\n",
    "# ----------------------------------------------\n",
    "#\n",
    "# We can now put all the steps together and implement the DiffPIR algorithm! First, we initialize the algorithm, and\n",
    "# then we iterate over the different steps detailed above.\n",
    "\n",
    "# Initialization\n",
    "x = 2 * physics.A_adjoint(y) - 1  # Rescale\n",
    "x = (\n",
    "    x + (sigmas[seq[0]] ** 2 - 4 * sigma_noise**2).sqrt() * torch.randn_like(x)\n",
    ") / sqrt_recip_alphas_cumprod[\n",
    "    -1\n",
    "]  # Add noise (simpler than the original code, may be suboptimal)\n",
    "\n",
    "# Images to save for visualization\n",
    "list_denoised, list_prox, list_noisy = [], [], []\n",
    "save_steps = [0, 1, 2, 5, 10, 20, 29]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(seq))):\n",
    "\n",
    "        sigma_cur = sigmas[seq[i]]\n",
    "\n",
    "        # time step associated with the noise level sigmas[i]\n",
    "        t_i = find_nearest(reduced_alpha_cumprod, sigma_cur.cpu().numpy())\n",
    "        at = 1 / sqrt_recip_alphas_cumprod[t_i] ** 2\n",
    "\n",
    "        # Denoising step\n",
    "        x_aux = x / (2 * at.sqrt()) + 0.5  # renormalize in [0, 1]\n",
    "        out = model(x_aux, sigma_cur / 2)\n",
    "        denoised = 2 * out - 1  # back to [-1, 1]\n",
    "        x0 = denoised.clamp(-1, 1)  # optional\n",
    "\n",
    "        if not seq[i] == seq[-1]:\n",
    "            # 2. Data fidelity step\n",
    "            x0 = data_fidelity.prox(x0, y, physics, gamma=1 / (2 * rhos[t_i]))\n",
    "\n",
    "            # 3. Sampling step\n",
    "            next_sigma = sigmas[T - 1 - seq[i + 1]].cpu().numpy()\n",
    "            t_im1 = find_nearest(\n",
    "                sigmas.cpu().numpy(), next_sigma\n",
    "            )  # time step associated with the next noise level\n",
    "\n",
    "            eps = (x - alphas_cumprod[t_i].sqrt() * x0) / torch.sqrt(\n",
    "                1.0 - alphas_cumprod[t_i]\n",
    "            )  # effective noise\n",
    "\n",
    "            x = alphas_cumprod[t_im1].sqrt() * x0 + torch.sqrt(\n",
    "                1.0 - alphas_cumprod[t_im1]\n",
    "            ) * (np.sqrt(1 - zeta) * eps + np.sqrt(zeta) * torch.randn_like(x))\n",
    "\n",
    "        if i in save_steps:\n",
    "            list_noisy.append(x_aux)\n",
    "            list_denoised.append(denoised)\n",
    "            list_prox.append(x0)\n",
    "\n",
    "# Renormalize in [0, 1]\n",
    "x = (x + 1) / 2\n",
    "\n",
    "# Plotting the results\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Model Output\": x,\n",
    "        \"Ground Truth\": x_true,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the sample, its denoised version and the proximal steps at different iterations.\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 9\n",
    "# sphinx_gallery_multi_image = \"single\"\n",
    "plot(\n",
    "    list_noisy,\n",
    "    titles=[f\"Noisy Sample Step {i}\" for i in save_steps],\n",
    "    dpi=1500,\n",
    "    tight=False,\n",
    ")\n",
    "\n",
    "plot(\n",
    "    list_denoised,\n",
    "    titles=[f\"Denoised Step {i}\" for i in save_steps],\n",
    "    dpi=1500,\n",
    ")\n",
    "\n",
    "plot(\n",
    "    list_prox,\n",
    "    titles=[f\"Proximal Step {i}\" for i in save_steps],\n",
    "    dpi=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the DiffPIR algorithm in your inverse problem\n",
    "# ------------------------------------------------------\n",
    "# You can readily use this algorithm via the :class:`deepinv.sampling.DiffPIR` class.\n",
    "#\n",
    "# ::\n",
    "#\n",
    "#       y = physics(x)\n",
    "#       model = dinv.sampling.DiffPIR(dinv.models.DiffUNet(), data_fidelity=dinv.optim.data_fidelity.L2())\n",
    "#       xhat = model(y, physics)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
