{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Implementing DPS\n",
    "================\n",
    "\n",
    "In this tutorial, we will go over the steps in the Diffusion Posterior Sampling (DPS) algorithm introduced in\n",
    ":footcite:t:`chung2022diffusion`. The full algorithm is implemented in :class:`deepinv.sampling.DPS`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies\n",
    "# -----------------------\n",
    "# Let us ``import`` the relevant packages, and load a sample\n",
    "# image of size 64 x 64. This will be used as our ground truth image.\n",
    "#\n",
    "# .. note::\n",
    "#           We work with an image of size 64 x 64 to reduce the computational time of this example.\n",
    "#           The DiffUNet we use in the algorithm works best with images of size 256 x 256.\n",
    "#\n",
    "\n",
    "import torch\n",
    "\n",
    "import deepinv as dinv\n",
    "from deepinv.utils.plotting import plot\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.utils import load_example\n",
    "from tqdm import tqdm  # to visualize progress\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x_true = load_example(\"butterfly.png\", img_size=64).to(device)\n",
    "x = x_true.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74454390",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# In this tutorial we consider random inpainting as the inverse problem, where the forward operator is implemented\n",
    "# in :class:`deepinv.physics.Inpainting`. In the example that we use, 90% of the pixels will be masked out randomly,\n",
    "# and we will additionally have Additive White Gaussian Noise (AWGN) of standard deviation  12.75/255.\n",
    "\n",
    "sigma = 12.75 / 255.0  # noise level\n",
    "\n",
    "physics = dinv.physics.Inpainting(\n",
    "    img_size=(3, x.shape[-2], x.shape[-1]),\n",
    "    mask=0.1,\n",
    "    pixelwise=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "y = physics(x_true)\n",
    "\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Ground Truth\": x_true,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fee4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion model loading\n",
    "# -----------------------\n",
    "#\n",
    "# We will take a pre-trained diffusion model that was also used for the DiffPIR algorithm, namely the one trained on\n",
    "# the FFHQ 256x256 dataset. Note that this means that the diffusion model was trained with human face images,\n",
    "# which is very different from the image that we consider in our example. Nevertheless, we will see later on that\n",
    "# ``DPS`` generalizes sufficiently well even in such case.\n",
    "\n",
    "\n",
    "model = dinv.models.DiffUNet(large_model=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f93021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diffusion schedule\n",
    "# -------------------------\n",
    "#\n",
    "# We will use the standard linear diffusion noise schedule. Once :math:`\\beta_t` is defined to follow a linear schedule\n",
    "# that interpolates between :math:`\\beta_{\\rm min}` and :math:`\\beta_{\\rm max}`,\n",
    "# we have the following additional definitions:\n",
    "# :math:`\\alpha_t := 1 - \\beta_t`, :math:`\\bar\\alpha_t := \\prod_{j=1}^t \\alpha_j`.\n",
    "# The following equations will also be useful\n",
    "# later on (we always assume that :math:`\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})` hereafter.)\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#           \\mathbf{x}_t = \\sqrt{1 - \\beta_t}\\mathbf{x}_{t-1} + \\sqrt{\\beta_t}\\mathbf{\\epsilon}\n",
    "#\n",
    "#           \\mathbf{x}_t = \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_t}\\mathbf{\\epsilon}\n",
    "#\n",
    "# where we use the reparametrization trick.\n",
    "\n",
    "num_train_timesteps = 1000  # Number of timesteps used during training\n",
    "\n",
    "\n",
    "betas = torch.linspace(1e-4, 2e-2, num_train_timesteps).to(device)\n",
    "alphas = (1 - betas).cumprod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db5d73",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The DPS algorithm\n",
    "# -----------------\n",
    "#\n",
    "# Now that the inverse problem is defined, we can apply the DPS algorithm to solve it. The DPS algorithm is\n",
    "# a diffusion algorithm that alternates between a denoising step, a gradient step and a reverse diffusion sampling step.\n",
    "# The algorithm writes as follows, for :math:`t` decreasing from :math:`T` to :math:`1`:\n",
    "#\n",
    "# .. math::\n",
    "#         \\begin{equation*}\n",
    "#         \\begin{aligned}\n",
    "#         \\widehat{\\mathbf{x}}_{0} (\\mathbf{x}_t) &= \\denoiser{\\mathbf{x}_t}{\\sqrt{1-\\overline{\\alpha}_t}/\\sqrt{\\overline{\\alpha}_t}}\n",
    "#         \\\\\n",
    "#         \\mathbf{g}_t &= \\nabla_{\\mathbf{x}_t} \\log p( \\widehat{\\mathbf{x}}_{0}(\\mathbf{x}_t) | \\mathbf{y} ) \\\\\n",
    "#         \\mathbf{\\varepsilon}_t &= \\mathcal{N}(0, \\mathbf{I}) \\\\\n",
    "#         \\mathbf{x}_{t-1} &= a_t \\,\\, \\mathbf{x}_t\n",
    "#         + b_t \\, \\, \\widehat{\\mathbf{x}}_0\n",
    "#         + \\tilde{\\sigma}_t \\, \\, \\mathbf{\\varepsilon}_t + \\mathbf{g}_t,\n",
    "#         \\end{aligned}\n",
    "#         \\end{equation*}\n",
    "#\n",
    "# where :math:`\\denoiser{\\cdot}{\\sigma}` is a denoising network for noise level :math:`\\sigma`,\n",
    "# :math:`\\eta` is a hyperparameter in [0, 1], and the constants :math:`\\tilde{\\sigma}_t, a_t, b_t` are defined as\n",
    "#\n",
    "# .. math::\n",
    "#         \\begin{equation*}\n",
    "#         \\begin{aligned}\n",
    "#           \\tilde{\\sigma}_t &= \\eta \\sqrt{ (1 - \\frac{\\overline{\\alpha}_t}{\\overline{\\alpha}_{t-1}})\n",
    "#           \\frac{1 - \\overline{\\alpha}_{t-1}}{1 - \\overline{\\alpha}_t}} \\\\\n",
    "#           a_t &= \\sqrt{1 - \\overline{\\alpha}_{t-1} - \\tilde{\\sigma}_t^2}/\\sqrt{1-\\overline{\\alpha}_t} \\\\\n",
    "#           b_t &= \\sqrt{\\overline{\\alpha}_{t-1}} - \\sqrt{1 - \\overline{\\alpha}_{t-1} - \\tilde{\\sigma}_t^2}\n",
    "#           \\frac{\\sqrt{\\overline{\\alpha}_{t}}}{\\sqrt{1 - \\overline{\\alpha}_{t}}}\n",
    "#         \\end{aligned}\n",
    "#         \\end{equation*}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b589626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising step\n",
    "# --------------\n",
    "#\n",
    "# The first step of DPS consists of applying a denoiser function to the current image :math:`\\mathbf{x}_t`,\n",
    "# with standard deviation :math:`\\sigma_t = \\sqrt{1 - \\overline{\\alpha}_t}/\\sqrt{\\overline{\\alpha}_t}`.\n",
    "#\n",
    "# This is equivalent to sampling :math:`\\mathbf{x}_t \\sim q(\\mathbf{x}_t|\\mathbf{x}_0)`, and then computing the\n",
    "# posterior mean.\n",
    "#\n",
    "\n",
    "\n",
    "t = 200  # choose some arbitrary timestep\n",
    "at = alphas[t]\n",
    "sigmat = (1 - at).sqrt() / at.sqrt()\n",
    "\n",
    "x0 = x_true\n",
    "xt = x0 + sigmat * torch.randn_like(x0)\n",
    "\n",
    "# apply denoiser\n",
    "x0_t = model(xt, sigmat)\n",
    "\n",
    "# Visualize\n",
    "plot(\n",
    "    {\n",
    "        \"Ground Truth\": x0,\n",
    "        \"Noisy\": xt,\n",
    "        \"Posterior Mean\": x0_t,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35621c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPS approximation\n",
    "# -----------------\n",
    "#\n",
    "# In order to perform gradient-based **posterior sampling** with diffusion models, we have to be able to compute\n",
    "# :math:`\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y})`. Applying Bayes rule, we have\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#           \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y}) = \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "#           + \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\mathbf{x}_t)\n",
    "#\n",
    "# For the former term, we can simply plug-in our estimated score function as in Tweedie's formula. As the latter term\n",
    "# is intractable, DPS proposes the following approximation (for details, see Theorem 1 of :footcite:t:`chung2022diffusion`)\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#           \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y}) \\approx \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "#           + \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\widehat{\\mathbf{x}}_{0}(\\mathbf{x_t}))\n",
    "#\n",
    "# Remarkably, we can now compute the latter term when we have Gaussian noise, as\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#       \\log p(\\mathbf{y}|\\widehat{\\mathbf{x}}_0(\\mathbf{x_t})) =\n",
    "#       -\\frac{\\|\\mathbf{y} - A\\widehat{\\mathbf{x}}_0((\\mathbf{x_t})\\|_2^2}{2\\sigma_y^2}.\n",
    "#\n",
    "# Moreover, taking the gradient w.r.t. :math:`\\mathbf{x}_t` can be performed through automatic differentiation.\n",
    "# Let's see how this can be done in PyTorch. Note that when we are taking the gradient w.r.t. a tensor,\n",
    "# we first have to enable the gradient computation by ``tensor.requires_grad_()``\n",
    "#\n",
    "# .. note::\n",
    "#           The DPS algorithm assumes that the images are in the range [-1, 1], whereas standard denoisers\n",
    "#           usually output images in the range [0, 1]. This is why we rescale the images before applying the steps.\n",
    "\n",
    "\n",
    "x0 = x_true * 2.0 - 1.0  # [0, 1] -> [-1, 1]\n",
    "\n",
    "data_fidelity = L2()\n",
    "\n",
    "# xt ~ q(xt|x0)\n",
    "t = 200  # choose some arbitrary timestep\n",
    "at = alphas[t]\n",
    "sigma_cur = (1 - at).sqrt() / at.sqrt()\n",
    "xt = x0 + sigma_cur * torch.randn_like(x0)\n",
    "\n",
    "# DPS\n",
    "with torch.enable_grad():\n",
    "    # Turn on gradient\n",
    "    xt.requires_grad_()\n",
    "\n",
    "    # normalize to [0, 1], denoise, and rescale to [-1, 1]\n",
    "    x0_t = model(xt / 2 + 0.5, sigma_cur / 2) * 2 - 1\n",
    "    # Log-likelihood\n",
    "    ll = data_fidelity(x0_t, y, physics).sqrt().sum()\n",
    "    # Take gradient w.r.t. xt\n",
    "    grad_ll = torch.autograd.grad(outputs=ll, inputs=xt)[0]\n",
    "\n",
    "# Visualize\n",
    "plot(\n",
    "    {\n",
    "        \"Ground Truth\": x0,\n",
    "        \"Noisy\": xt,\n",
    "        \"Posterior Mean\": x0_t,\n",
    "        \"Gradient\": grad_ll,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f1a63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# DPS Algorithm\n",
    "# -------------\n",
    "#\n",
    "# As we visited all the key components of DPS, we are now ready to define the algorithm. For every denoising\n",
    "# timestep, the algorithm iterates the following\n",
    "#\n",
    "# 1. Get :math:`\\hat{\\mathbf{x}}` using the denoiser network.\n",
    "# 2. Compute :math:`\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\hat{\\mathbf{x}}_t)` through backpropagation.\n",
    "# 3. Perform reverse diffusion sampling with DDPM(IM), corresponding to an update with :math:`\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)`.\n",
    "# 4. Take a gradient step with :math:`\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\hat{\\mathbf{x}}_t)`.\n",
    "#\n",
    "# There are two caveats here. First, in the original work, DPS used DDPM ancestral sampling. As the DDIM sampler :footcite:t:`song2020denoising`\n",
    "# is a generalization of DDPM in a sense that it retrieves DDPM when\n",
    "# :math:`\\eta = 1.0`, here we consider DDIM sampling.\n",
    "# One can freely choose the :math:`\\eta` parameter here, but since we will consider 1000\n",
    "# neural function evaluations (NFEs),\n",
    "# it is advisable to keep it :math:`\\eta = 1.0`. Second, when taking the log-likelihood gradient step,\n",
    "# the gradient is weighted so that the actual implementation is a static step size times the :math:`\\ell_2`\n",
    "# norm of the residual:\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#           \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\hat{\\mathbf{x}}_{t}(\\mathbf{x}_t)) \\simeq\n",
    "#           \\rho \\nabla_{\\mathbf{x}_t} \\|\\mathbf{y} - \\mathbf{A}\\hat{\\mathbf{x}}_{t}\\|_2\n",
    "#\n",
    "# With these in mind, let us solve the inverse problem with DPS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b89931",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# .. note::\n",
    "#\n",
    "#   We only use 200 steps to reduce the computational time of this example. As suggested by the authors of DPS, the\n",
    "#   algorithm works best with ``num_steps = 1000``.\n",
    "#\n",
    "\n",
    "num_steps = 200\n",
    "\n",
    "skip = num_train_timesteps // num_steps\n",
    "\n",
    "batch_size = 1\n",
    "eta = 1.0  # DDPM scheme; use eta < 1 for DDIM\n",
    "\n",
    "\n",
    "# measurement\n",
    "x0 = x_true * 2.0 - 1.0\n",
    "# x0 = x_true.clone()\n",
    "y = physics(x0.to(device))\n",
    "\n",
    "# initial sample from x_T\n",
    "x = torch.randn_like(x0)\n",
    "\n",
    "xs = [x]\n",
    "x0_preds = []\n",
    "\n",
    "for t in tqdm(reversed(range(0, num_train_timesteps, skip))):\n",
    "    at = alphas[t]\n",
    "    at_next = alphas[t - skip] if t - skip >= 0 else torch.tensor(1)\n",
    "    # we cannot use bt = betas[t] if skip > 1:\n",
    "    bt = 1 - at / at_next\n",
    "\n",
    "    xt = xs[-1].to(device)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        xt.requires_grad_()\n",
    "\n",
    "        # 1. denoising step\n",
    "        aux_x = xt / (2 * at.sqrt()) + 0.5  # renormalize in [0, 1]\n",
    "        sigma_cur = (1 - at).sqrt() / at.sqrt()  # sigma_t\n",
    "\n",
    "        x0_t = 2 * model(aux_x, sigma_cur / 2) - 1\n",
    "        x0_t = torch.clip(x0_t, -1.0, 1.0)  # optional\n",
    "\n",
    "        # 2. likelihood gradient approximation\n",
    "        l2_loss = data_fidelity(x0_t, y, physics).sqrt().sum()\n",
    "\n",
    "    norm_grad = torch.autograd.grad(outputs=l2_loss, inputs=xt)[0]\n",
    "    norm_grad = norm_grad.detach()\n",
    "\n",
    "    sigma_tilde = (bt * (1 - at_next) / (1 - at)).sqrt() * eta\n",
    "    c2 = ((1 - at_next) - sigma_tilde**2).sqrt()\n",
    "\n",
    "    # 3. noise step\n",
    "    epsilon = torch.randn_like(xt)\n",
    "\n",
    "    # 4. DDIM(PM) step\n",
    "    xt_next = (\n",
    "        (at_next.sqrt() - c2 * at.sqrt() / (1 - at).sqrt()) * x0_t\n",
    "        + sigma_tilde * epsilon\n",
    "        + c2 * xt / (1 - at).sqrt()\n",
    "        - norm_grad\n",
    "    )\n",
    "    x0_preds.append(x0_t.to(\"cpu\"))\n",
    "    xs.append(xt_next.to(\"cpu\"))\n",
    "\n",
    "recon = xs[-1]\n",
    "\n",
    "# plot the results\n",
    "x = recon / 2 + 0.5\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Model Output\": x,\n",
    "        \"Ground Truth\": x_true,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DPS in your inverse problem\n",
    "# ---------------------------------\n",
    "# You can readily use this algorithm via the :class:`deepinv.sampling.DPS` class.\n",
    "#\n",
    "# ::\n",
    "#\n",
    "#       y = physics(x)\n",
    "#       model = dinv.sampling.DPS(dinv.models.DiffUNet(), data_fidelity=dinv.optim.data_fidelity.L2())\n",
    "#       xhat = model(y, physics)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c24f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
