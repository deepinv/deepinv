{
  "cells": [
    {
      "id": "cebe638b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "e921ff4b",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "\n# Uncertainty quantification with PnP-ULA.\n\nThis code shows you how to use sampling algorithms to quantify uncertainty of a reconstruction\nfrom incomplete and noisy measurements.\n\nULA obtains samples by running the following iteration:\n\n\\begin{align}x_{k+1} = x_k +  \\alpha \\eta \\nabla \\log p_{\\sigma}(x_k) + \\eta \\nabla \\log p(y|x_k)  + \\sqrt{2 \\eta} z_k\\end{align}\n\nwhere $z_k \\sim \\mathcal{N}(0, I)$ is a Gaussian random variable, $\\eta$ is the step size and\n$\\alpha$ is a parameter controlling the regularization.\n\nThe PnP-ULA method is described in the paper :footcite:t:`laumont2022bayesian`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom deepinv.utils.plotting import plot\nimport torch\nfrom deepinv.utils import load_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Load image from the internet\n\nThis example uses an image of Messi.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nx = load_example(\"messi.jpg\", img_size=32).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define forward operator and noise model\n\nThis example uses inpainting as the forward operator and Gaussian noise as the noise model.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 0.1  # noise level\nphysics = dinv.physics.Inpainting(mask=0.5, img_size=x.shape[1:], device=device)\nphysics.noise_model = dinv.physics.GaussianNoise(sigma=sigma)\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define the likelihood\n\nSince the noise model is Gaussian, the negative log-likelihood is the L2 loss.\n\n\\begin{align}-\\log p(y|x) \\propto \\frac{1}{2\\sigma^2} \\|y-Ax\\|^2\\end{align}\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load Gaussian Likelihood\nlikelihood = dinv.optim.data_fidelity.L2(sigma=sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Define the prior\n\nThe score a distribution can be approximated using Tweedie's formula via the\n[`deepinv.optim.ScorePrior`](https://deepinv.github.io/deepinv/api/stubs/deepinv.optim.ScorePrior.html) class.\n\n\\begin{align}\\nabla \\log p_{\\sigma}(x) \\approx \\frac{1}{\\sigma^2} \\left(D(x,\\sigma)-x\\right)\\end{align}\n\nThis example uses a pretrained DnCNN model.\nFrom a Bayesian point of view, the score plays the role of the gradient of the\nnegative log prior\nThe hyperparameter ``sigma_denoiser`` ($sigma$) controls the strength of the prior.\n\nIn this example, we use a pretrained DnCNN model using the [`deepinv.loss.FNEJacobianSpectralNorm`](https://deepinv.github.io/deepinv/api/stubs/deepinv.loss.FNEJacobianSpectralNorm.html) loss,\nwhich makes sure that the denoiser is firmly non-expansive (see :footcite:t:`terris2020building`), and helps to\nstabilize the sampling algorithm.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma_denoiser = 2 / 255\nprior = dinv.optim.ScorePrior(\n    denoiser=dinv.models.DnCNN(pretrained=\"download_lipschitz\")\n).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Create the MCMC sampler\n\nHere we use the Unadjusted Langevin Algorithm (ULA) to sample from the posterior defined in\n[`deepinv.sampling.ULAIterator`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.ULAIterator.html).\nThe hyperparameter ``step_size`` controls the step size of the MCMC sampler,\n``regularization`` controls the strength of the prior and\n``iterations`` controls the number of iterations of the sampler.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "regularization = 0.9\nstep_size = 0.01 * (sigma**2)\niterations = int(5e3) if torch.cuda.is_available() else 10\nparams = {\n    \"step_size\": step_size,\n    \"alpha\": regularization,\n    \"sigma\": sigma_denoiser,\n}\nf = dinv.sampling.sampling_builder(\n    \"ULA\",\n    prior=prior,\n    data_fidelity=likelihood,\n    max_iter=iterations,\n    params_algo=params,\n    thinning=1,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Generate the measurement\nWe apply the forward model to generate the noisy measurement.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = physics(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Run sampling algorithm and plot results\nThe sampling algorithm returns the posterior mean and variance.\nWe compare the posterior mean with a simple linear reconstruction.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean, var = f.sample(y, physics)\n\n# compute linear inverse\nx_lin = physics.A_adjoint(y)\n\n# compute PSNR\nprint(f\"Linear reconstruction PSNR: {dinv.metric.PSNR()(x, x_lin).item():.2f} dB\")\nprint(f\"Posterior mean PSNR: {dinv.metric.PSNR()(x, mean).item():.2f} dB\")\n\n# plot results\nerror = (mean - x).abs().sum(dim=1).unsqueeze(1)  # per pixel average abs. error\nstd = var.sum(dim=1).unsqueeze(1).sqrt()  # per pixel average standard dev.\nimgs = [x_lin, x, mean, std / std.flatten().max(), error / error.flatten().max()]\nplot(\n    imgs,\n    titles=[\"measurement\", \"ground truth\", \"post. mean\", \"post. std\", \"abs. error\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": ":References:\n\n> **Footbibliography**\n>\n>\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}