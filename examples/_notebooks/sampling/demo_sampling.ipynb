{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e52f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Uncertainty quantification with PnP-ULA.\n",
    "====================================================================================================\n",
    "\n",
    "This code shows you how to use sampling algorithms to quantify uncertainty of a reconstruction\n",
    "from incomplete and noisy measurements.\n",
    "\n",
    "ULA obtains samples by running the following iteration:\n",
    "\n",
    ".. math::\n",
    "\n",
    "    x_{k+1} = x_k +  \\alpha \\eta \\nabla \\log p_{\\sigma}(x_k) + \\eta \\nabla \\log p(y|x_k)  + \\sqrt{2 \\eta} z_k\n",
    "\n",
    "where :math:`z_k \\sim \\mathcal{N}(0, I)` is a Gaussian random variable, :math:`\\eta` is the step size and\n",
    ":math:`\\alpha` is a parameter controlling the regularization.\n",
    "\n",
    "The PnP-ULA method is described in the paper :footcite:t:`laumont2022bayesian`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "from deepinv.utils.plotting import plot\n",
    "import torch\n",
    "from deepinv.utils import load_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from the internet\n",
    "# --------------------------------------------\n",
    "#\n",
    "# This example uses an image of Messi.\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x = load_example(\"messi.jpg\", img_size=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward operator and noise model\n",
    "# --------------------------------------------------------------\n",
    "#\n",
    "# This example uses inpainting as the forward operator and Gaussian noise as the noise model.\n",
    "\n",
    "sigma = 0.1  # noise level\n",
    "physics = dinv.physics.Inpainting(mask=0.5, img_size=x.shape[1:], device=device)\n",
    "physics.noise_model = dinv.physics.GaussianNoise(sigma=sigma)\n",
    "\n",
    "# Set the global random seed from pytorch to ensure reproducibility of the example.\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5281be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood\n",
    "# --------------------------------------------------------------\n",
    "#\n",
    "# Since the noise model is Gaussian, the negative log-likelihood is the L2 loss.\n",
    "#\n",
    "# .. math::\n",
    "#   -\\log p(y|x) \\propto \\frac{1}{2\\sigma^2} \\|y-Ax\\|^2\n",
    "\n",
    "# load Gaussian Likelihood\n",
    "likelihood = dinv.optim.data_fidelity.L2(sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ca654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior\n",
    "# -------------------------------------------\n",
    "#\n",
    "# The score a distribution can be approximated using Tweedie's formula via the\n",
    "# :class:`deepinv.optim.ScorePrior` class.\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#            \\nabla \\log p_{\\sigma}(x) \\approx \\frac{1}{\\sigma^2} \\left(D(x,\\sigma)-x\\right)\n",
    "#\n",
    "# This example uses a pretrained DnCNN model.\n",
    "# From a Bayesian point of view, the score plays the role of the gradient of the\n",
    "# negative log prior\n",
    "# The hyperparameter ``sigma_denoiser`` (:math:`sigma`) controls the strength of the prior.\n",
    "#\n",
    "# In this example, we use a pretrained DnCNN model using the :class:`deepinv.loss.FNEJacobianSpectralNorm` loss,\n",
    "# which makes sure that the denoiser is firmly non-expansive (see :footcite:t:`terris2020building`), and helps to\n",
    "# stabilize the sampling algorithm.\n",
    "\n",
    "sigma_denoiser = 2 / 255\n",
    "prior = dinv.optim.ScorePrior(\n",
    "    denoiser=dinv.models.DnCNN(pretrained=\"download_lipschitz\")\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MCMC sampler\n",
    "# --------------------------------------------------------------\n",
    "#\n",
    "# Here we use the Unadjusted Langevin Algorithm (ULA) to sample from the posterior defined in\n",
    "# :class:`deepinv.sampling.ULAIterator`.\n",
    "# The hyperparameter ``step_size`` controls the step size of the MCMC sampler,\n",
    "# ``regularization`` controls the strength of the prior and\n",
    "# ``iterations`` controls the number of iterations of the sampler.\n",
    "\n",
    "regularization = 0.9\n",
    "step_size = 0.01 * (sigma**2)\n",
    "iterations = int(5e3) if torch.cuda.is_available() else 10\n",
    "params = {\n",
    "    \"step_size\": step_size,\n",
    "    \"alpha\": regularization,\n",
    "    \"sigma\": sigma_denoiser,\n",
    "}\n",
    "f = dinv.sampling.sampling_builder(\n",
    "    \"ULA\",\n",
    "    prior=prior,\n",
    "    data_fidelity=likelihood,\n",
    "    max_iter=iterations,\n",
    "    params_algo=params,\n",
    "    thinning=1,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b3e5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate the measurement\n",
    "# --------------------------------------------------------------\n",
    "# We apply the forward model to generate the noisy measurement.\n",
    "\n",
    "y = physics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37880d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampling algorithm and plot results\n",
    "# --------------------------------------------------------------\n",
    "# The sampling algorithm returns the posterior mean and variance.\n",
    "# We compare the posterior mean with a simple linear reconstruction.\n",
    "\n",
    "mean, var = f.sample(y, physics)\n",
    "\n",
    "# compute linear inverse\n",
    "x_lin = physics.A_adjoint(y)\n",
    "\n",
    "# compute PSNR\n",
    "print(f\"Linear reconstruction PSNR: {dinv.metric.PSNR()(x, x_lin).item():.2f} dB\")\n",
    "print(f\"Posterior mean PSNR: {dinv.metric.PSNR()(x, mean).item():.2f} dB\")\n",
    "\n",
    "# plot results\n",
    "error = (mean - x).abs().sum(dim=1).unsqueeze(1)  # per pixel average abs. error\n",
    "std = var.sum(dim=1).unsqueeze(1).sqrt()  # per pixel average standard dev.\n",
    "imgs = [x_lin, x, mean, std / std.flatten().max(), error / error.flatten().max()]\n",
    "plot(\n",
    "    imgs,\n",
    "    titles=[\"measurement\", \"ground truth\", \"post. mean\", \"post. std\", \"abs. error\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
