{
  "cells": [
    {
      "id": "0c638ecc",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "# Install deepinv (skip if already installed)\n%pip install deepinv",
      "outputs": []
    },
    {
      "id": "d051dcfd",
      "cell_type": "markdown",
      "source": "<!-- MathJax macro definitions inserted automatically -->\n$$\n\\newcommand{\\forw}[1]{{A\\left({#1}\\right)}}\n\\newcommand{\\noise}[1]{{N\\left({#1}\\right)}}\n\\newcommand{\\inverse}[1]{{R\\left({#1}\\right)}}\n\\newcommand{\\inversef}[2]{{R\\left({#1},{#2}\\right)}}\n\\newcommand{\\inversename}{R}\n\\newcommand{\\reg}[1]{{g_\\sigma\\left({#1}\\right)}}\n\\newcommand{\\regname}{g_\\sigma}\n\\newcommand{\\sensor}[1]{{\\eta\\left({#1}\\right)}}\n\\newcommand{\\datafid}[2]{{f\\left({#1},{#2}\\right)}}\n\\newcommand{\\datafidname}{f}\n\\newcommand{\\distance}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\distancename}{d}\n\\newcommand{\\denoiser}[2]{{\\operatorname{D}_{{#2}}\\left({#1}\\right)}}\n\\newcommand{\\denoisername}{\\operatorname{D}_{\\sigma}}\n\\newcommand{\\xset}{\\mathcal{X}}\n\\newcommand{\\yset}{\\mathcal{Y}}\n\\newcommand{\\group}{\\mathcal{G}}\n\\newcommand{\\metric}[2]{{d\\left({#1},{#2}\\right)}}\n\\newcommand{\\loss}[1]{{\\mathcal\\left({#1}\\right)}}\n\\newcommand{\\conj}[1]{{\\overline{#1}^{\\top}}}\n$$",
      "metadata": {
        "language": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "\n# Building your diffusion posterior sampling method using SDEs\n\nThis demo shows you how to use\n[`deepinv.sampling.PosteriorDiffusion`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.PosteriorDiffusion.html) to perform posterior sampling. It also can be used to perform unconditional image generation with arbitrary denoisers, if the data fidelity term is not specified.\n\nThis method requires:\n\n* A well-trained denoiser with varying noise levels (ideally with large noise levels) (e.g., [`deepinv.models.NCSNpp`](https://deepinv.github.io/deepinv/api/stubs/deepinv.models.NCSNpp.html)).\n\n* A (noisy) data fidelity term (e.g., [`deepinv.sampling.DPSDataFidelity`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.DPSDataFidelity.html)).\n\n* Define a drift term $f(x, t)$ and a diffusion term $g(t)$ for the forward-time SDE. They can be defined through the [`deepinv.sampling.DiffusionSDE`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.DiffusionSDE.html) (e.g., [`deepinv.sampling.VarianceExplodingDiffusion`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.VarianceExplodingDiffusion.html)).\n\nThe [`deepinv.sampling.PosteriorDiffusion`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.PosteriorDiffusion.html) class can be used to perform posterior sampling for inverse problems.\nConsider the acquisition model:\n\n\\begin{align}y = \\noise{\\forw{x}}\\end{align}\n\nwhere $\\forw{x}$ is the forward operator (e.g., a convolutional operator) and $\\noise{\\cdot}$ is the noise operator (e.g., Gaussian noise).\nThis class defines the reverse-time SDE for the posterior distribution $p(x|y)$ given the data $y$:\n\n\\begin{align}d\\, x_t = \\left( f(x_t, t) - \\frac{1 + \\alpha}{2} g(t)^2 \\nabla_{x_t} \\log p_t(x_t | y) \\right) d\\,t + g(t) \\sqrt{\\alpha} d\\, w_{t}\\end{align}\n\nwhere $f$ is the drift term, $g$ is the diffusion coefficient and $w$ is the standard Brownian motion.\nThe drift term and the diffusion coefficient are defined by the underlying (unconditional) forward-time SDE `sde`.\nIn this example, we will use 2 well-known SDE in the literature: the Variance-Exploding (VE) and Variance-Preserving (VP or DDPM).\n\nThe (conditional) score function $\\nabla_{x_t} \\log p_t(x_t | y)$ can be decomposed using the Bayes' rule:\n\n\\begin{align}\\nabla_{x_t} \\log p_t(x_t | y) = \\nabla_{x_t} \\log p_t(x_t) + \\nabla_{x_t} \\log p_t(y | x_t).\\end{align}\n\nThe first term is the score function of the unconditional SDE, which is typically approximated by an MMSE denoiser (`denoiser`) using the well-known Tweedie's formula, while the\nsecond term is approximated by the (noisy) data-fidelity term (`data_fidelity`).\nWe implement various data-fidelity terms in [the user guide](https://deepinv.github.io/deepinv/user_guide/reconstruction/sampling.html#id2).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this demo, we limit the number of diffusion steps for the sake of speed, but in practice, you should use a larger number of steps to obtain better results.</p></div>"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "---------------------------------------------------\n\nLet us import the necessary modules, define the denoiser and the SDE.\n\nIn this first example, we use the Variance-Exploding SDE, whose forward process is defined as:\n\n\\begin{align}d\\, x_t = g(t) d\\, w_t \\quad \\mbox{where } g(t) = \\sigma_{\\mathrm{min}}\\left( \\frac{\\sigma_{\\mathrm{max}}}{\\sigma_{\\mathrm{min}}}\\right)^t\\sqrt{2\\log\\frac{\\sigma_{\\mathrm{max}}}{\\sigma_{\\mathrm{min}}} }.\\end{align}\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport deepinv as dinv\nfrom deepinv.models import NCSNpp\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float64\nfigsize = 2.5\ngif_frequency = 10  # Increase this value to save the GIF saving time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepinv.sampling import (\n    PosteriorDiffusion,\n    DPSDataFidelity,\n    EulerSolver,\n    VarianceExplodingDiffusion,\n)\nfrom deepinv.optim import ZeroFidelity\n\n# In this example, we use the pre-trained FFHQ-64 model from the\n# EDM framework: https://arxiv.org/pdf/2206.00364 .\n# The network architecture is from Song et al: https://arxiv.org/abs/2011.13456 .\ndenoiser = NCSNpp(pretrained=\"download\").to(device)\n\n# The solution is obtained by calling the SDE object with a desired solver (here, Euler).\n# The reproducibility of the SDE Solver class can be controlled by providing the pseudo-random number generator.\nnum_steps = 150\nrng = torch.Generator(device).manual_seed(42)\ntimesteps = torch.linspace(1, 0.001, num_steps)\nsolver = EulerSolver(timesteps=timesteps, rng=rng)\n\nsigma_min = 0.005\nsigma_max = 5\nsde = VarianceExplodingDiffusion(\n    sigma_max=sigma_max,\n    sigma_min=sigma_min,\n    alpha=0.5,\n    device=device,\n    dtype=dtype,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Reverse-time SDE as sampling process\n\nWhen the data fidelity is not given, the posterior diffusion is equivalent to the unconditional diffusion.\nSampling is performed by solving the reverse-time SDE. To do so, we generate a reverse-time trajectory.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = PosteriorDiffusion(\n    data_fidelity=ZeroFidelity(),\n    sde=sde,\n    denoiser=denoiser,\n    solver=solver,\n    dtype=dtype,\n    device=device,\n    verbose=True,\n)\nx, trajectory = model(\n    y=None,\n    physics=None,\n    x_init=(1, 3, 64, 64),\n    seed=1,\n    get_trajectory=True,\n)\ndinv.utils.plot(\n    x,\n    titles=\"Unconditional generation\",\n    save_fn=\"sde_sample.png\",\n    figsize=(figsize, figsize),\n)\n\ndinv.utils.save_videos(\n    trajectory.cpu()[::gif_frequency],\n    time_dim=0,\n    titles=[\"VE-SDE Trajectory\"],\n    save_fn=\"sde_trajectory.gif\",\n    figsize=(figsize, figsize),\n)\n\n# sphinx_gallery_start_ignore\n# cleanup\nimport os\nimport shutil\nfrom pathlib import Path\n\n\ntry:\n    final_dir = (\n        Path(os.getcwd()).parent.parent / \"docs\" / \"source\" / \"auto_examples\" / \"images\"\n    )\n    shutil.move(\"sde_trajectory.gif\", final_dir / \"sde_trajectory.gif\")\n    shutil.move(\"sde_sample.png\", final_dir / \"sde_sample.png\")\nexcept FileNotFoundError:\n    pass\n\n# sphinx_gallery_end_ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "We obtain the following unconditional sample\n\n> **Container**: image-row\n>\n>\n> .. image-sg-ignore:: /auto_examples/images/sde_sample.png\n> :alt: example of unconditional sample\n> :srcset: /auto_examples/images/sde_sample.png\n> :class: custom-img\n> :ignore_missing: true\n>\n> .. image-sg-ignore:: /auto_examples/images/sde_trajectory.gif\n> :alt: example of unconditional trajectory\n> :srcset: /auto_examples/images/sde_trajectory.gif\n> :class: custom-gif\n> :ignore_missing: true\n>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "When the data fidelity is given, together with the measurements and the physics, this class can be used to perform posterior sampling for inverse problems.\nFor example, consider the inpainting problem, where we have a noisy image and we want to recover the original image.\nWe can use the [`deepinv.sampling.DPSDataFidelity`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.DPSDataFidelity.html) as the data fidelity term.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del trajectory  # clean memory\nmask = torch.ones_like(x)\nmask[..., 24:40, 24:40] = 0.0\nphysics = dinv.physics.Inpainting(img_size=x.shape[1:], mask=mask, device=device)\ny = physics(x)\n\nweight = 3.0  # guidance strength\ndps_fidelity = DPSDataFidelity(denoiser=denoiser, weight=weight)\n\nmodel = PosteriorDiffusion(\n    data_fidelity=dps_fidelity,\n    denoiser=denoiser,\n    sde=sde,\n    solver=solver,\n    dtype=dtype,\n    device=device,\n    verbose=True,\n)\n\n# To perform posterior sampling, we need to provide the measurements, the physics and the solver.\n# Moreover, when the physics is given, the initial point can be inferred from the physics if not given explicitly.\nseed_1 = 11\nx_hat, trajectory = model(\n    y,\n    physics,\n    seed=seed_1,\n    get_trajectory=True,\n)\n# Here, we plot the original image, the measurement and the posterior sample\ndinv.utils.plot(\n    [x, y, x_hat],\n    show=True,\n    titles=[\"Original\", \"Measurement\", \"Posterior sample\"],\n    save_fn=\"posterior_sample.png\",\n    figsize=(figsize * 3, figsize),\n)\n# We can also save the trajectory of the posterior sample\ndinv.utils.save_videos(\n    trajectory[::gif_frequency],\n    time_dim=0,\n    titles=[\"Posterior sample with VE\"],\n    save_fn=\"posterior_trajectory.gif\",\n    figsize=(figsize, figsize),\n)\n# sphinx_gallery_start_ignore\n# cleanup\nimport os\nimport shutil\nfrom pathlib import Path\n\ntry:\n    final_dir = (\n        Path(os.getcwd()).parent.parent / \"docs\" / \"source\" / \"auto_examples\" / \"images\"\n    )\n    shutil.move(\"posterior_trajectory.gif\", final_dir / \"posterior_trajectory.gif\")\n    shutil.move(\"posterior_sample.png\", final_dir / \"posterior_sample.png\")\n\nexcept FileNotFoundError:\n    pass\n\n\n# sphinx_gallery_end_ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "We obtain the following posterior sample and trajectory\n\n> **Container**: image-col\n>\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_sample.png\n> :alt: example of posterior sample\n> :srcset: /auto_examples/images/posterior_sample.png\n> :ignore_missing: true\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_trajectory.gif\n> :alt: example of posterior trajectory\n> :srcset: /auto_examples/images/posterior_trajectory.gif\n> :ignore_missing: true\n> :class: custom-gif\n>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "<div class=\"alert alert-info\"><h4>Note</h4><p>**Reproducibility**: To ensure the reproducibility, if the parameter `rng` is given, the same sample will\n    be generated when the same seed is used.\n    One can obtain varying samples by using a different seed.\n\n    **Parallel sampling**: one can draw multiple samples in parallel by giving the initial shape, e.g., `x_init = (B, C, H, W)`</p></div>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Varying the SDE\n\nOne can also change the underlying SDE for sampling.\nFor example, we can also use the Variance-Preserving (VP or DDPM) in [`deepinv.sampling.VariancePreservingDiffusion`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.VariancePreservingDiffusion.html), whose forward drift and diffusion term are defined as:\n\n\\begin{align}f(x_t, t) = -\\frac{1}{2} \\beta(t)x_t \\qquad \\mbox{ and } \\qquad g(t) = \\beta(t)  \\qquad \\mbox{ with } \\beta(t) = \\beta_{\\mathrm{min}}  + t \\left( \\beta_{\\mathrm{max}} - \\beta_{\\mathrm{min}} \\right).\\end{align}\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepinv.sampling import VariancePreservingDiffusion\n\ndel trajectory\nsde = VariancePreservingDiffusion(device=device, dtype=dtype)\nmodel = PosteriorDiffusion(\n    data_fidelity=dps_fidelity,\n    denoiser=denoiser,\n    sde=sde,\n    solver=solver,\n    device=device,\n    dtype=dtype,\n    verbose=True,\n)\n\nx_hat_vp, trajectory = model(\n    y,\n    physics,\n    seed=111,\n    timesteps=torch.linspace(1, 0.001, 150),\n    get_trajectory=True,\n)\ndinv.utils.plot(\n    [x_hat, x_hat_vp],\n    titles=[\n        \"posterior sample with VE\",\n        \"posterior sample with VP\",\n    ],\n    save_fn=\"posterior_sample_ve_vp.png\",\n    figsize=(figsize * 3, figsize),\n)\n\n\n# We can also save the trajectory of the posterior sample\ndinv.utils.save_videos(\n    trajectory[::gif_frequency],\n    time_dim=0,\n    titles=[\"Posterior sample with VP\"],\n    save_fn=\"posterior_trajectory_vp.gif\",\n    figsize=(figsize, figsize),\n)\n\n# sphinx_gallery_start_ignore\n# cleanup\nimport os\nimport shutil\nfrom pathlib import Path\n\ntry:\n    final_dir = (\n        Path(os.getcwd()).parent.parent / \"docs\" / \"source\" / \"auto_examples\" / \"images\"\n    )\n    shutil.move(\"posterior_sample_ve_vp.png\", final_dir / \"posterior_sample_ve_vp.png\")\n    shutil.move(\n        \"posterior_trajectory_vp.gif\", final_dir / \"posterior_trajectory_vp.gif\"\n    )\n\nexcept FileNotFoundError:\n    pass\n\n# sphinx_gallery_end_ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "We can comparing the sampling trajectory depending on the underlying SDE\n\n> **Container**: image-col\n>\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_sample_ve_vp.png\n> :alt: posterior sample with VP\n> :srcset: /auto_examples/images/posterior_sample_ve_vp.png\n> :ignore_missing: true\n> .. container:: image-row\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_trajectory.gif\n> :alt: posterior trajectory with VE\n> :srcset: /auto_examples/images/posterior_trajectory.gif\n> :ignore_missing: true\n> :class: custom-gif\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_trajectory_vp.gif\n> :alt: posterior trajectory with VP\n> :srcset: /auto_examples/images/posterior_trajectory_vp.gif\n> :ignore_missing: true\n> :class: custom-gif\n>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "## Plug-and-play Posterior Sampling with arbitrary denoisers\n\nThe [`deepinv.sampling.PosteriorDiffusion`](https://deepinv.github.io/deepinv/api/stubs/deepinv.sampling.PosteriorDiffusion.html) class can be used together with any (well-trained) denoisers for posterior sampling.\nFor example, we can use the [`deepinv.models.DRUNet`](https://deepinv.github.io/deepinv/api/stubs/deepinv.models.DRUNet.html) for posterior sampling.\nWe can also change the underlying SDE, for example change the `sigma_max` value.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del trajectory  # clean memory\nsigma_min = 0.001\nsigma_max = 10.0\nrng = torch.Generator(device)\ndtype = torch.float32\ntimesteps = torch.linspace(1, 0.001, 250)\nsolver = EulerSolver(timesteps=timesteps, rng=rng)\ndenoiser = dinv.models.DRUNet(pretrained=\"download\").to(device)\n\nsde = VarianceExplodingDiffusion(\n    sigma_max=sigma_max, sigma_min=sigma_min, alpha=0.75, device=device, dtype=dtype\n)\nx = dinv.utils.load_example(\n    \"butterfly.png\",\n    img_size=256,\n    resize_mode=\"resize\",\n).to(device)\n\nmask = torch.ones_like(x)\nmask[..., 70:150, 120:180] = 0\nphysics = dinv.physics.Inpainting(\n    mask=mask,\n    img_size=x.shape[1:],\n    device=device,\n)\n\ny = physics(x)\nmodel = PosteriorDiffusion(\n    data_fidelity=DPSDataFidelity(denoiser=denoiser, weight=0.3),\n    denoiser=denoiser,\n    sde=sde,\n    solver=solver,\n    dtype=dtype,\n    device=device,\n    verbose=True,\n)\n\n# To perform posterior sampling, we need to provide the measurements, the physics and the solver.\nx_hat, trajectory = model(\n    y=y,\n    physics=physics,\n    seed=12,\n    get_trajectory=True,\n)\n\n# Here, we plot the original image, the measurement and the posterior sample\ndinv.utils.plot(\n    [x, y, x_hat.clip(0, 1)],\n    titles=[\"Original\", \"Measurement\", \"Posterior sample DRUNet\"],\n    figsize=(figsize * 3, figsize),\n    save_fn=\"posterior_sample_DRUNet.png\",\n)\n\n# We can also save the trajectory of the posterior sample\ndinv.utils.save_videos(\n    trajectory[::gif_frequency].clip(0, 1),\n    time_dim=0,\n    titles=[\"Posterior trajectory DRUNet\"],\n    save_fn=\"posterior_sample_DRUNet.gif\",\n    figsize=(figsize, figsize),\n)\n\n# sphinx_gallery_start_ignore\n# cleanup\nimport os\nimport shutil\nfrom pathlib import Path\n\ntry:\n    final_dir = (\n        Path(os.getcwd()).parent.parent / \"docs\" / \"source\" / \"auto_examples\" / \"images\"\n    )\n    shutil.move(\n        \"posterior_sample_DRUNet.png\", final_dir / \"posterior_sample_DRUNet.png\"\n    )\n    shutil.move(\n        \"posterior_sample_DRUNet.gif\", final_dir / \"posterior_sample_DRUNet.gif\"\n    )\n\nexcept FileNotFoundError:\n    pass\n\n# sphinx_gallery_end_ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": "We obtain the following posterior trajectory\n\n> **Container**: image-col\n>\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_sample_DRUNet.png\n> :alt: posterior sample DRUNet\n> :srcset: /auto_examples/images/posterior_sample_DRUNet.png\n> :ignore_missing: true\n>\n> .. image-sg-ignore:: /auto_examples/images/posterior_sample_DRUNet.gif\n> :alt: posterior trajectory DRUNet\n> :srcset: /auto_examples/images/posterior_sample_DRUNet.gif\n> :ignore_missing: true\n> :class: custom-gif\n>\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}