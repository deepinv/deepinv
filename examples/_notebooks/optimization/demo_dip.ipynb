{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reconstructing an image using the deep image prior.\n\nThis code shows how to reconstruct a noisy and incomplete image using the deep image prior.\n\nThis method is based on the paper \"Deep Image Prior\" :footcite:t:`ulyanov2018deep` and reconstructs\nan image by minimizing the loss function\n\n\\begin{align}\\min_{\\theta}  \\|y-Af_{\\theta}(z)\\|^2\\end{align}\n\nwhere $z$ is a random input and $f_{\\theta}$ is a convolutional decoder network with parameters\n$\\theta$. The minimization should be stopped early to avoid overfitting. The method uses the Adam\noptimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom deepinv.utils.plotting import plot\nimport torch\nfrom deepinv.utils import load_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load image from the internet\n\nThis example uses an image of Messi.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nx = load_example(\"messi.jpg\", img_size=32).to(device)\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define forward operator and noise model\n\nWe use image inpainting as the forward operator and Gaussian noise as the noise model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 0.1  # noise level\nphysics = dinv.physics.Inpainting(mask=0.5, img_size=x.shape[1:], device=device)\nphysics.noise_model = dinv.physics.GaussianNoise(sigma=sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the measurement\nWe apply the forward model to generate the noisy measurement.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = physics(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the deep image prior\n\nThis method only works with certain convolutional decoder networks. We recommend using the\nnetwork :class:`deepinv.models.ConvDecoder`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The number of iterations and learning rate have been set manually to obtain good results. However, these\n    values may not be optimal for all problems. We recommend experimenting with different values.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Here we run a small number of iterations to reduce the runtime of the example. However, the results could\n    be improved by running more iterations.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "iterations = 100\nlr = 1e-2  # learning rate for the optimizer.\nchannels = 64  # number of channels per layer in the decoder.\nin_size = [2, 2]  # size of the input to the decoder.\nbackbone = dinv.models.ConvDecoder(\n    img_size=x.shape[1:], in_size=in_size, channels=channels\n).to(device)\n\nf = dinv.models.DeepImagePrior(\n    backbone,\n    learning_rate=lr,\n    iterations=iterations,\n    verbose=True,\n    input_size=[channels] + in_size,\n).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run DIP algorithm and plot results\nWe run the DIP algorithm and plot the results.\n\nThe good performance of DIP is somewhat surprising, since the network has many parameters and could potentially\noverfit the noisy measurement data. However, the architecture acts as an implicit regularizer, providing good\nreconstructions if the optimization is stopped early.\nWhile this phenomenon is not yet well understood, there has been some efforts to explain it. For example, see :footcite:t:`tachella2021neural`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dip = f(y, physics)\n\n# compute linear inverse\nx_lin = physics.A_adjoint(y)\n\n# compute PSNR\npsnr_linear = dinv.metric.PSNR()(x, x_lin).item()\npsnr_dip = dinv.metric.PSNR()(x, dip).item()\n\n# plot results\nplot(\n    {\n        \"Measurement\": y,\n        \"Ground Truth\": x,\n        \"DIP\": dip,\n    },\n    subtitles=[\"PSNR\", f\"{psnr_linear:.2f} dB\", f\"{psnr_dip:.2f} dB\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}