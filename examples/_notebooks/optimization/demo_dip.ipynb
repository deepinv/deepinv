{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49314f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Reconstructing an image using the deep image prior.\n",
    "===================================================\n",
    "\n",
    "This code shows how to reconstruct a noisy and incomplete image using the deep image prior.\n",
    "\n",
    "This method is based on the paper \"Deep Image Prior\" :footcite:t:`ulyanov2018deep` and reconstructs\n",
    "an image by minimizing the loss function\n",
    "\n",
    ".. math::\n",
    "\n",
    "    \\min_{\\theta}  \\|y-Af_{\\theta}(z)\\|^2\n",
    "\n",
    "where :math:`z` is a random input and :math:`f_{\\theta}` is a convolutional decoder network with parameters\n",
    ":math:`\\theta`. The minimization should be stopped early to avoid overfitting. The method uses the Adam\n",
    "optimizer.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepinv as dinv\n",
    "from deepinv.utils.plotting import plot\n",
    "import torch\n",
    "from deepinv.utils import load_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from the internet\n",
    "# ----------------------------\n",
    "#\n",
    "# This example uses an image of Messi.\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x = load_example(\"messi.jpg\", img_size=32).to(device)\n",
    "\n",
    "# Set the global random seed from pytorch to ensure reproducibility of the example.\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67769808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward operator and noise model\n",
    "# ---------------------------------------\n",
    "#\n",
    "# We use image inpainting as the forward operator and Gaussian noise as the noise model.\n",
    "\n",
    "sigma = 0.1  # noise level\n",
    "physics = dinv.physics.Inpainting(mask=0.5, img_size=x.shape[1:], device=device)\n",
    "physics.noise_model = dinv.physics.GaussianNoise(sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f23b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the measurement\n",
    "# ------------------------\n",
    "# We apply the forward model to generate the noisy measurement.\n",
    "\n",
    "y = physics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f49f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep image prior\n",
    "# ----------------------------\n",
    "#\n",
    "# This method only works with certain convolutional decoder networks. We recommend using the\n",
    "# network :class:`deepinv.models.ConvDecoder`.\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#     The number of iterations and learning rate have been set manually to obtain good results. However, these\n",
    "#     values may not be optimal for all problems. We recommend experimenting with different values.\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#     Here we run a small number of iterations to reduce the runtime of the example. However, the results could\n",
    "#     be improved by running more iterations.\n",
    "\n",
    "iterations = 100\n",
    "lr = 1e-2  # learning rate for the optimizer.\n",
    "channels = 64  # number of channels per layer in the decoder.\n",
    "in_size = [2, 2]  # size of the input to the decoder.\n",
    "backbone = dinv.models.ConvDecoder(\n",
    "    img_size=x.shape[1:], in_size=in_size, channels=channels\n",
    ").to(device)\n",
    "\n",
    "f = dinv.models.DeepImagePrior(\n",
    "    backbone,\n",
    "    learning_rate=lr,\n",
    "    iterations=iterations,\n",
    "    verbose=True,\n",
    "    input_size=[channels] + in_size,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e659ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DIP algorithm and plot results\n",
    "# ----------------------------------\n",
    "# We run the DIP algorithm and plot the results.\n",
    "#\n",
    "# The good performance of DIP is somewhat surprising, since the network has many parameters and could potentially\n",
    "# overfit the noisy measurement data. However, the architecture acts as an implicit regularizer, providing good\n",
    "# reconstructions if the optimization is stopped early.\n",
    "# While this phenomenon is not yet well understood, there has been some efforts to explain it. For example, see :footcite:t:`tachella2021neural`.\n",
    "dip = f(y, physics)\n",
    "\n",
    "# compute linear inverse\n",
    "x_lin = physics.A_adjoint(y)\n",
    "\n",
    "# compute PSNR\n",
    "psnr_linear = dinv.metric.PSNR()(x, x_lin).item()\n",
    "psnr_dip = dinv.metric.PSNR()(x, dip).item()\n",
    "\n",
    "# plot results\n",
    "plot(\n",
    "    {\n",
    "        \"Measurement\": y,\n",
    "        \"Ground Truth\": x,\n",
    "        \"DIP\": dip,\n",
    "    },\n",
    "    subtitles=[\"PSNR\", f\"{psnr_linear:.2f} dB\", f\"{psnr_dip:.2f} dB\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957e140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
