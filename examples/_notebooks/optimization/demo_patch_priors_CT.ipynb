{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871920ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    ".. _patch-prior-demo:\n",
    "\n",
    "Patch priors for limited-angle computed tomography\n",
    "====================================================================================================\n",
    "\n",
    "In this example we use patch priors for limited angle computed tomography. More precisely, we consider the\n",
    "inverse problem :math:`y = \\mathrm{noisy}(Ax)`, where :math:`A` is the discretized Radon transform\n",
    "with :math:`100` equispace angles between 20 and 160 degrees.\n",
    "For the reconstruction, we minimize the variational problem\n",
    "\n",
    ".. math::\n",
    "    \\begin{equation*}\n",
    "    \\label{eq:min_prob}\n",
    "    \\underset{x}{\\arg\\min} \\quad \\datafid{x}{y} + \\lambda g(x).\n",
    "    \\end{equation*}\n",
    "\n",
    "Here, the regularizier :math:`g` is explicitly defined as\n",
    "\n",
    ".. math::\n",
    "    \\begin{equation*}\n",
    "    g(x)=\\sum_{i\\in\\mathcal{I}} h(P_i x),\n",
    "    \\end{equation*}\n",
    "\n",
    "where :math:`P_i` is the linear operator which extracts the :math:`i`-th patch from the image :math:`x` and\n",
    ":math:`h` is a regularizer on the space of patches.\n",
    "We consider the following two choices of :math:`h`:\n",
    "\n",
    "* The expected patch log-likelihood (EPLL) prior was proposed by :footcite:t:`zoran2011learning`.\n",
    "  It sets :math:`h(x)=-\\log(p_\\theta(x))`, where :math:`p_\\theta` is the probability density function of a Gaussian mixture model.\n",
    "  The parameters :math:`\\theta` are estimated a-priori on a (possibly small) data set of training patches using\n",
    "  an expectation maximization algorithm.\n",
    "  In contrast to the original paper by Zoran and Weiss, we minimize the arising variational problem by simply applying\n",
    "  the Adam optimizers. For an example for using the (approximated) half-quadratic splitting algorithm proposed by Zoran\n",
    "  and Weiss, we refer to the denoising example...\n",
    "\n",
    "* The patch normalizing flow regularizer (PatchNR) was proposed by :footcite:t:`altekruger2023patchnr`.\n",
    "  It models :math:`h(x)=-\\log(p_{\\theta}(x))` as negative log-likelihood function of a probaility density function\n",
    "  :math:`p_\\theta={\\mathcal{T}_\\theta}_\\#\\mathcal{N}(0,I)` which is given as the push-forward measure of a standard\n",
    "  normal distribution under a normalizing flow (invertible neural network) :math:`\\mathcal{T}_\\theta`.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from deepinv.datasets import PatchDataset\n",
    "from deepinv import Trainer\n",
    "from deepinv.physics import LogPoissonNoise, Tomography, Denoising, UniformNoise\n",
    "from deepinv.optim import LogPoissonLikelihood, PatchPrior, PatchNR, EPLL\n",
    "from deepinv.loss.metric import PSNR\n",
    "from deepinv.utils import plot\n",
    "from deepinv.utils import load_torch_url\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test images\n",
    "# -----------------------------------------\n",
    "# Here, we use downsampled images from the `\"LoDoPaB-CT dataset\" <https://zenodo.org/records/3384092>`_.\n",
    "# Moreover, we define the size of the used patches and generate the dataset of patches in the training images.\n",
    "\n",
    "url = \"https://huggingface.co/datasets/deepinv/LoDoPaB-CT_toy/resolve/main/LoDoPaB-CT_small.pt\"\n",
    "dataset = load_torch_url(url)\n",
    "train_imgs = dataset[\"train_imgs\"].to(device)\n",
    "test_imgs = dataset[\"test_imgs\"].to(device)\n",
    "img_size = train_imgs.shape[-1]\n",
    "\n",
    "patch_size = 3\n",
    "verbose = True\n",
    "train_dataset = PatchDataset(train_imgs, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for EPLL and PatchNR\n",
    "# -----------------------------------------\n",
    "# For PatchNR, we choose the number of hidden neurons in the subnetworks and for the training batch size and number of epochs.\n",
    "# For EPLL, we set the number of mixture components and the maximum number of steps and batch size for fitting the EM algorithm.\n",
    "\n",
    "patchnr_subnetsize = 128\n",
    "patchnr_epochs = 5\n",
    "patchnr_batch_size = 32\n",
    "patchnr_learning_rate = 1e-4\n",
    "\n",
    "epll_num_components = 20\n",
    "epll_max_iter = 20\n",
    "epll_batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / EM algorithm\n",
    "# -----------------------------------------\n",
    "# If the parameter retrain is False, we just load pretrained weights. Set the parameter to True for retraining.\n",
    "# On the cpu, this takes up to a couple of minutes.\n",
    "# After training, we define the corresponding patch priors\n",
    "#\n",
    "# .. note::\n",
    "#\n",
    "#          The normalizing flow training minimizes the forward Kullback-Leibler (maximum likelihood) loss function given by\n",
    "#\n",
    "#            .. math::\n",
    "#                       \\mathcal{L}(\\theta)=\\mathrm{KL}(P_X,{\\mathcal{T}_\\theta}_\\#P_Z)=\n",
    "#                       \\mathbb{E}_{x\\sim P_X}[p_{{\\mathcal{T}_\\theta}_\\#P_Z}(x)]+\\mathrm{const},\n",
    "#\n",
    "#            where :math:`\\mathcal{T}_\\theta` is the normalizing flow with parameters :math:`\\theta`, latent distribution\n",
    "#            :math:`P_Z`, data distribution :math:`P_X` and push-forward measure :math:`{\\mathcal{T}_\\theta}_\\#P_Z`.\n",
    "\n",
    "\n",
    "retrain = False\n",
    "if retrain:\n",
    "    model_patchnr = PatchNR(\n",
    "        pretrained=None,\n",
    "        sub_net_size=patchnr_subnetsize,\n",
    "        device=device,\n",
    "        patch_size=patch_size,\n",
    "    )\n",
    "    patchnr_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=patchnr_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    class NFTrainer(Trainer):\n",
    "        def compute_loss(self, physics, x, y, train=True, epoch=None):\n",
    "            logs = {}\n",
    "\n",
    "            self.optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Evaluate reconstruction network\n",
    "            invs, jac_inv = self.model(y)\n",
    "\n",
    "            # Compute the Kullback Leibler loss\n",
    "            loss_total = torch.mean(\n",
    "                0.5 * torch.sum(invs.view(invs.shape[0], -1) ** 2, -1)\n",
    "                - jac_inv.view(invs.shape[0])\n",
    "            )\n",
    "            current_log = (\n",
    "                self.logs_total_loss_train if train else self.logs_total_loss_eval\n",
    "            )\n",
    "            current_log.update(loss_total.item())\n",
    "            logs[f\"TotalLoss\"] = current_log.avg\n",
    "\n",
    "            if train:\n",
    "                loss_total.backward()  # Backward the total loss\n",
    "                self.optimizer.step()  # Optimizer step\n",
    "\n",
    "            return invs, logs\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_patchnr.normalizing_flow.parameters(), lr=patchnr_learning_rate\n",
    "    )\n",
    "    trainer = NFTrainer(\n",
    "        model=model_patchnr.normalizing_flow,\n",
    "        physics=Denoising(UniformNoise(1.0 / 255.0)),\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=patchnr_dataloader,\n",
    "        device=device,\n",
    "        losses=[],\n",
    "        epochs=patchnr_epochs,\n",
    "        online_measurements=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model_epll = EPLL(\n",
    "        pretrained=None,\n",
    "        n_components=epll_num_components,\n",
    "        patch_size=patch_size,\n",
    "        device=device,\n",
    "    )\n",
    "    epll_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=epll_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    model_epll.GMM.fit(epll_dataloader, verbose=verbose, max_iters=epll_max_iter)\n",
    "else:\n",
    "    model_patchnr = PatchNR(\n",
    "        pretrained=\"PatchNR_lodopab_small2\",\n",
    "        sub_net_size=patchnr_subnetsize,\n",
    "        device=device,\n",
    "        patch_size=patch_size,\n",
    "    )\n",
    "    model_epll = EPLL(\n",
    "        pretrained=\"GMM_lodopab_small2\",\n",
    "        n_components=epll_num_components,\n",
    "        patch_size=patch_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "patchnr_prior = PatchPrior(model_patchnr, patch_size=patch_size)\n",
    "epll_prior = PatchPrior(model_epll.negative_log_likelihood, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of forward operator and noise model\n",
    "# -----------------------------------------------\n",
    "# The training depends only on the image domain or prior distribution.\n",
    "# For the reconstruction, we now define forward operator and noise model.\n",
    "# For the noise model, we use log-Poisson noise as suggested for the LoDoPaB dataset.\n",
    "# Then, we generate an observation by applying the physics and compute the filtered backprojection.\n",
    "\n",
    "mu = 1 / 50.0 * (362.0 / img_size)\n",
    "N0 = 1024.0\n",
    "num_angles = 100\n",
    "noise_model = LogPoissonNoise(mu=mu, N0=N0)\n",
    "data_fidelity = LogPoissonLikelihood(mu=mu, N0=N0)\n",
    "angles = torch.linspace(20, 160, steps=num_angles, device=device)\n",
    "physics = Tomography(\n",
    "    img_width=img_size, angles=angles, device=device, noise_model=noise_model\n",
    ")\n",
    "observation = physics(test_imgs)\n",
    "fbp = physics.A_dagger(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc29315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction loop\n",
    "# -----------------------------------------------\n",
    "# We define a reconstruction loop for minimizing the variational problem using the Adam optimizer.\n",
    "# As initialization, we choose the filtered backprojection.\n",
    "\n",
    "optim_steps = 200\n",
    "lr_variational_problem = 0.02\n",
    "\n",
    "\n",
    "def minimize_variational_problem(prior, lam):\n",
    "    imgs = fbp.detach().clone()\n",
    "    imgs.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([imgs], lr=lr_variational_problem)\n",
    "    for i in (progress_bar := tqdm(range(optim_steps))):\n",
    "        optimizer.zero_grad()\n",
    "        loss = data_fidelity(imgs, observation, physics).mean() + lam * prior(imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_description(\"Step {}\".format(i + 1))\n",
    "    return imgs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cad6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and plot\n",
    "# -----------------------------------------------\n",
    "# Finally, we run the reconstruction loop for both priors and plot the results.\n",
    "# The regularization parameter is roughly choosen by a grid search but not fine-tuned\n",
    "\n",
    "lam_patchnr = 120.0\n",
    "lam_epll = 120.0\n",
    "\n",
    "recon_patchnr = minimize_variational_problem(patchnr_prior, lam_patchnr)\n",
    "recon_epll = minimize_variational_problem(epll_prior, lam_epll)\n",
    "\n",
    "psnr_fbp = PSNR()(fbp, test_imgs).item()\n",
    "psnr_patchnr = PSNR()(recon_patchnr, test_imgs).item()\n",
    "psnr_epll = PSNR()(recon_epll, test_imgs).item()\n",
    "\n",
    "print(\"PSNRs:\")\n",
    "print(\"Filtered Backprojection: {0:.2f}\".format(psnr_fbp))\n",
    "print(\"EPLL: {0:.2f}\".format(psnr_epll))\n",
    "print(\"PatchNR: {0:.2f}\".format(psnr_patchnr))\n",
    "\n",
    "plot(\n",
    "    [\n",
    "        test_imgs,\n",
    "        fbp.clip(0, 1),\n",
    "        recon_epll.clip(0, 1),\n",
    "        recon_patchnr.clip(0, 1),\n",
    "    ],\n",
    "    [\"Ground truth\", \"Filtered Backprojection\", \"EPLL\", \"PatchNR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de61716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :References:\n",
    "#\n",
    "# .. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
