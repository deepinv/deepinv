{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b837f1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Use iterative reconstruction algorithms\n",
    "====================================================================================================\n",
    "\n",
    "Follow this example to reconstruct images using an iterative algorithm.\n",
    "\n",
    "The library provides a flexible framework to define your own iterative reconstruction algorithm, which are generally\n",
    "written as the optimization of the following problem:\n",
    "\n",
    ".. math::\n",
    "    \\begin{equation}\n",
    "    \\label{eq:min_prob}\n",
    "    \\tag{1}\n",
    "    \\underset{x}{\\arg\\min} \\quad \\datafid{x}{y} + \\lambda \\reg{x},\n",
    "    \\end{equation}\n",
    "\n",
    "where :math:`\\datafid{x}{y}` is the data fidelity term, :math:`\\reg{x}` is the (explicit or implicit) regularization term,\n",
    "and :math:`\\lambda` is a regularization parameter. In this example, we demonstrate:\n",
    "\n",
    "1. How to define your own iterative algorithm\n",
    "2. How to package it as a :class:`reconstructor model <deepinv.models.Reconstructor>`\n",
    "3. How to use predefined algorithms using :class:`optim builder <deepinv.optim.optim_builder>`\n",
    "\n",
    "1. Defining your own iterative algorithm\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\"\"\"\n",
    "\n",
    "import deepinv as dinv\n",
    "import torch\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the physics of the problem\n",
    "# -----------------------------------\n",
    "# Here we define a simple inpainting problem, where we want to reconstruct an image from partial measurements.\n",
    "# We also load an image of a butterfly to use as ground truth.\n",
    "\n",
    "x = dinv.utils.load_example(\"butterfly.png\", device=device, img_size=(128, 128))\n",
    "\n",
    "# Forward operator, here inpainting with a mask of 50% of the pixels\n",
    "physics = dinv.physics.Inpainting(img_size=(3, 128, 128), mask=0.5, device=device)\n",
    "\n",
    "# Generate measurements\n",
    "y = physics(x)\n",
    "\n",
    "dinv.utils.plot([x, y], titles=[\"Ground truth\", \"Measurements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data fidelity term and prior\n",
    "# ---------------------------------------\n",
    "# The library provides a set of :ref:`data fidelity <data-fidelity>` terms and :ref:`priors <priors>`\n",
    "# that can be used in the optimization problem.\n",
    "# Here we use the :math:`\\ell_2` data fidelity term and the Total Variation (TV) prior.\n",
    "#\n",
    "# These classes provide all the necessary methods for the optimization problem, such as the evaluation of the term,\n",
    "# the gradient, and the proximal operator.\n",
    "\n",
    "\n",
    "data_fidelity = dinv.optim.L2()  # Data fidelity term\n",
    "prior = dinv.optim.TVPrior()  # Prior term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba763b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the iterative algorithm\n",
    "# -----------------------------------\n",
    "# We will use the Proximal Gradient Descent (PGD) algorithm to solve the\n",
    "# optimization problem defined above, which is defined as\n",
    "#\n",
    "# .. math::\n",
    "#        \\qquad x_{k+1} = \\operatorname{prox}_{\\gamma \\lambda \\regname} \\left( x_k - \\gamma \\nabla \\datafidname(x_k, y) \\right),\n",
    "#\n",
    "# where :math:`\\operatorname{prox}_{\\gamma \\lambda \\regname}` is the proximal operator of the regularization term,\n",
    "# :math:`\\nabla \\datafidname(x_k, y)` is the gradient of the data fidelity term, :math:`\\gamma` is the stepsize.\n",
    "# and :math:`\\lambda` is the regularization parameter.\n",
    "#\n",
    "# We can choose the stepsize as :math:`\\gamma < \\frac{2}{\\|A\\|^2}`, where :math:`A` is the forward operator,\n",
    "# in order to ensure convergence of the algorithm.\n",
    "\n",
    "lambd = 0.05  # Regularization parameter\n",
    "\n",
    "# Compute the squared norm of the operator A\n",
    "norm_A2 = physics.compute_sqnorm(y, tol=1e-4, verbose=False).item()\n",
    "stepsize = 1.9 / norm_A2  # stepsize for the PGD algorithm\n",
    "\n",
    "# PGD algorithm\n",
    "max_iter = 20  # number of iterations\n",
    "x_k = torch.zeros_like(x, device=device)  # initial guess\n",
    "\n",
    "# To store the cost at each iteration:\n",
    "cost_history = torch.zeros(max_iter, device=device)\n",
    "\n",
    "with torch.no_grad():  # disable autodifferentiation\n",
    "    for it in range(max_iter):\n",
    "        u = x_k - stepsize * data_fidelity.grad(x_k, y, physics)  # Gradient step\n",
    "        x_k = prior.prox(u, gamma=lambd * stepsize)  # Proximal step\n",
    "        cost = data_fidelity(x_k, y, physics) + lambd * prior(x_k)  # Compute the cost\n",
    "        cost_history[it] = cost  # Store the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(cost_history.detach().cpu().numpy(), marker=\"o\")\n",
    "plt.title(\"Cost history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d76b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot the results and metrics\n",
    "\n",
    "metric = dinv.metric.PSNR()\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        f\"Ground truth\": x,\n",
    "        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n",
    "        f\"Recon w/ TV prior\\n {metric(x_k, x).item():.2f} dB\": x_k,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b828d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pretrained denoiser as prior\n",
    "# ----------------------------------\n",
    "#\n",
    "# We can improve the reconstruction by using a pretrained denoiser as prior, by replacing the proximal operator\n",
    "# with a denoising step.\n",
    "# The library provides :ref:`a collection of classical and pretrained denoisers <denoisers>`\n",
    "# that can be used in iterative algorithms.\n",
    "#\n",
    "# .. note::\n",
    "#     Plug-and-play algorithms can be sensitive to the choice of initialization.\n",
    "#     Here we use the TV estimate as the initial guess.\n",
    "\n",
    "\n",
    "x_k = x_k.clone()\n",
    "\n",
    "denoiser = dinv.models.DRUNet(device=device)  # Load a pretrained denoiser\n",
    "\n",
    "with torch.no_grad():  # disable autodifferentiation\n",
    "    for it in range(max_iter):\n",
    "        u = x_k - stepsize * data_fidelity.grad(x_k, y, physics)  # Gradient step\n",
    "        x_k = denoiser(u, sigma=0.05)  # replace prox by denoising step\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        f\"Ground truth\": x,\n",
    "        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n",
    "        f\"Recon w/ PnP prior\\n {metric(x_k, x).item():.2f} dB\": x_k,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ead18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Package your algorithm as a Reconstructor\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# The iterative algorithm we defined above can be packaged as a :class:`Reconstructor <deepinv.optim.BaseOptim>`.\n",
    "# This allows you to :class:`test it <deepinv.test>` on different physics and datasets, and to use it in a more flexible way,\n",
    "# including unfolding it and learning some of its parameters.\n",
    "\n",
    "\n",
    "class MyPGD(dinv.models.Reconstructor):\n",
    "    def __init__(self, data_fidelity, prior, stepsize, lambd, max_iter):\n",
    "        super().__init__()\n",
    "        self.data_fidelity = data_fidelity\n",
    "        self.prior = prior\n",
    "        self.stepsize = stepsize\n",
    "        self.lambd = lambd\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, y, physics, **kwargs):\n",
    "        \"\"\"Algorithm forward pass.\n",
    "\n",
    "        :param torch.Tensor y: measurements.\n",
    "        :param dinv.physics.Physics physics: measurement operator.\n",
    "        :return: torch.Tensor: reconstructed image.\n",
    "        \"\"\"\n",
    "        x_k = torch.zeros_like(y, device=y.device)  # initial guess\n",
    "\n",
    "        # Disable autodifferentiation, remove this if you want to unfold\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.max_iter):\n",
    "                u = x_k - self.stepsize * self.data_fidelity.grad(\n",
    "                    x_k, y, physics\n",
    "                )  # Gradient step\n",
    "                x_k = self.prior.prox(\n",
    "                    u, gamma=self.lambd * self.stepsize\n",
    "                )  # Proximal step\n",
    "\n",
    "        return x_k\n",
    "\n",
    "\n",
    "tv_algo = MyPGD(data_fidelity, prior, stepsize, lambd, max_iter)\n",
    "\n",
    "# Standard reconstructor forward pass\n",
    "x_hat = tv_algo(y, physics)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        f\"Ground truth\": x,\n",
    "        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n",
    "        f\"Recon w/ custom PGD\\n {metric(x_hat, x).item():.2f} dB\": x_hat,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219337ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using a predefined optimization algorithm with `optim_builder`\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# The library also lets you define :ref:`standard optimization algorithms <optim_iterators>`\n",
    "# as standard :class:`Reconstructors <deepinv.models.Reconstructor>` in one line of code using the :class:`deepinv.optim.optim_builder` function.\n",
    "# For example, the above PnP algorithm can be defined as follows:\n",
    "#\n",
    "# .. seealso::\n",
    "#     See :ref:`the optimization examples <sphx_glr_auto_examples_optimization_demo_TV_minimisation.py>` for more examples of using `optim_builder`.\n",
    "\n",
    "prior = dinv.optim.PnP(denoiser=denoiser)  # prior with prox via denoising step\n",
    "\n",
    "\n",
    "def custom_init(y: torch.Tensor, physics: dinv.physics.Physics) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom initialization function for the optimization algorithm.\n",
    "    The function should return a dictionary with the key \"est\" containing a tuple\n",
    "    with the initial guess (the TV solution in this case)\n",
    "    and the dual variables (None in this case).\n",
    "    \"\"\"\n",
    "    primal = tv_algo(y, physics)\n",
    "    dual = None  #  No dual variables in this case\n",
    "    return {\"est\": (primal, dual)}\n",
    "\n",
    "\n",
    "model = dinv.optim.optim_builder(\n",
    "    iteration=\"PGD\",\n",
    "    prior=prior,\n",
    "    data_fidelity=data_fidelity,\n",
    "    params_algo={\"stepsize\": stepsize, \"g_param\": 0.05},\n",
    "    max_iter=max_iter,\n",
    "    custom_init=custom_init,\n",
    ")\n",
    "\n",
    "x_hat = model(y, physics)\n",
    "\n",
    "dinv.utils.plot(\n",
    "    {\n",
    "        f\"Ground truth\": x,\n",
    "        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n",
    "        f\"Reconstruction\\n {metric(x_hat, x).item():.2f} dB\": x_hat,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ‰ Well done, you now know how to define your own iterative reconstruction algorithm!\n",
    "#\n",
    "# What's next?\n",
    "# ~~~~~~~~~~~~\n",
    "#\n",
    "# * Check out more about optimization algorithms in the :ref:`optimization user guide <optim>`.\n",
    "# * Check out diffusion and MCMC iterative algorithms in the :ref:`sampling user guide <sampling>`.\n",
    "# * Check out more :ref:`iterative algorithms examples <sphx_glr_auto_examples_optimization>`.\n",
    "# * Check out how to try the algorithm on a whole dataset by following the :ref:`bring your own dataset <sphx_glr_auto_examples_basics_demo_custom_dataset.py>` tutorial."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
