{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised learning from incomplete measurements of multiple operators.\n\nThis example shows you how to train a reconstruction network for an inpainting\ninverse problem on a fully self-supervised way, i.e., using measurement data only.\n\nThe dataset consists of pairs $(y_i,A_{g_i})$ where $y_i$ are the measurements and $A_{g_i}$ is a\nbinary sampling operator out of $G$ (i.e., $g_i\\in \\{1,\\dots,G\\}$).\n\nThis self-supervised learning approach is presented in :footcite:t:`tachella2022unsupervised` and minimizes the loss function:\n\n\\begin{align}\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\left\\|A_{g_i} \\hat{x}_{i,\\theta} - y_i \\right\\|_2^2 + \\sum_{s=1}^{G}\n    \\left\\|\\hat{x}_{i,\\theta} - R_{\\theta}(A_s\\hat{x}_{i,\\theta},A_s) \\right\\|_2^2\\end{align}\n\nwhere $R_{\\theta}$ is a reconstruction network with parameters $\\theta$, $y_i$ are the measurements,\n$A_s$ is a binary sampling operator, and $\\hat{x}_{i,\\theta} = R_{\\theta}(y_i,A_{g_i})$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport deepinv as dinv\nfrom deepinv.utils import get_data_home\nfrom deepinv.models.utils import get_weights_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"measurements\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\nORIGINAL_DATA_DIR = get_data_home()\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use the MNIST dataset for training and testing.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n\ntrain_base_dataset = datasets.MNIST(\n    root=ORIGINAL_DATA_DIR, train=True, transform=transform, download=True\n)\ntest_base_dataset = datasets.MNIST(\n    root=ORIGINAL_DATA_DIR, train=False, transform=transform, download=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of subsampled images and load it.\nWe generate 10 different inpainting operators, each one with a different random mask.\nIf the :func:`deepinv.datasets.generate_dataset` receives a list of physics operators, it\ngenerates a dataset for each operator and returns a list of paths to the generated datasets.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We only use 10 training images per operator to reduce the computational time of this example. You can use the whole\n  dataset by setting ``n_images_max = None``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_operators = 10\n\n# defined physics\nphysics = [\n    dinv.physics.Inpainting(mask=0.5, img_size=(1, 28, 28), device=device)\n    for _ in range(number_of_operators)\n]\n\n# Use parallel dataloader if using a GPU to reduce training time,\n# otherwise, as all computes are on CPU, use synchronous data loading.\nnum_workers = 4 if torch.cuda.is_available() else 0\nn_images_max = (\n    None if torch.cuda.is_available() else 50\n)  # number of images used for training (uses the whole dataset if you have a gpu)\n\noperation = \"inpainting\"\nmy_dataset_name = \"demo_multioperator_imaging\"\nmeasurement_dir = DATA_DIR / \"MNIST\" / operation\ndeepinv_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_base_dataset,\n    test_dataset=test_base_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    test_datapoints=10,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = [\n    dinv.datasets.HDF5Dataset(path=path, train=True) for path in deepinv_datasets_path\n]\ntest_dataset = [\n    dinv.datasets.HDF5Dataset(path=path, train=False) for path in deepinv_datasets_path\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the reconstruction network\n\nAs a reconstruction network, we use a simple artifact removal network based on a U-Net.\nThe network is defined as a $R_{\\theta}(y,A)=\\phi_{\\theta}(A^{\\top}y)$ where $\\phi$ is the U-Net.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the unfolded trainable model.\nmodel = dinv.models.ArtifactRemoval(\n    backbone_net=dinv.models.UNet(in_channels=1, out_channels=1, scales=3)\n)\nmodel = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the training parameters\nWe choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)\nand the multi-operator imaging loss (MOI).\nNecessary and sufficient conditions on the number of operators and measurements are described in :footcite:t:`tachella2023sensing`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use a pretrained model to reduce training time. You can get the same results by training from scratch\n      for 100 epochs.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 1\nlearning_rate = 5e-4\nbatch_size = 64 if torch.cuda.is_available() else 1\n\n# choose self-supervised training losses\n# generates 4 random rotations per image in the batch\nlosses = [dinv.loss.MCLoss(), dinv.loss.MOILoss(physics)]\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)\n\n# start with a pretrained model to reduce training time\nfile_name = \"demo_moi_ckp_10.pth\"\nurl = get_weights_url(model_name=\"demo\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=file_name\n)\n# load a checkpoint to reduce training time\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\nTo simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\nsuch as PSNR or SSIM, which require clean ground truth images.\n\n.. tip::\n\n      We can use the same self-supervised loss for evaluation, as it does not require clean images,\n      to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verbose = True  # print training information\n\ntrain_dataloader = [\n    DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n    for dataset in train_dataset\n]\ntest_dataloader = [\n    DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n    for dataset in test_dataset\n]\n\n# Initialize the trainer\ntrainer = dinv.Trainer(\n    model=model,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    optimizer=optimizer,\n    physics=physics,\n    device=device,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    metrics=None,  # no supervised metrics\n    early_stop=2,  # early stop using the self-supervised loss on the test set\n    save_path=str(CKPT_DIR / operation),\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    verbose=verbose,\n    plot_images=True,\n    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n    ckp_interval=10,\n)\n\n# Train the network\nmodel = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\nWe now assume that we have access to a small test set of ground-truth images to evaluate the performance of the trained network.\nand we compute the PSNR between the denoised images and the clean ground truth images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.test(test_dataloader, metrics=dinv.metric.PSNR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}