{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Image transformations for Equivariant Imaging\n\nThis example demonstrates various geometric image transformations\nimplemented in ``deepinv`` that can be used in Equivariant Imaging (EI)\nfor self-supervised learning:\n\n-  Shift: integer pixel 2D shift;\n-  Rotate: 2D image rotation;\n-  Scale: continuous 2D image downscaling;\n-  Euclidean: includes continuous translation, rotation, and reflection,\n   forming the group $\\mathbb{E}(2)$;\n-  Similarity: as above but includes scale, forming the group\n   $\\text{S}(2)$;\n-  Affine: as above but includes shear effects, forming the group\n   $\\text{Aff}(3)$;\n-  Homography: as above but includes perspective (i.e pan and tilt)\n   effects, forming the group $\\text{PGL}(3)$;\n-  PanTiltRotate: pure 3D camera rotation i.e pan, tilt and 2D image\n   rotation.\n\nSee `docs <transform>` for full list.\n\nThese were proposed in the papers:\n\n-  ``Shift``, ``Rotate``: :footcite:t:`chen2021equivariant`.\n-  ``Scale``: :footcite:t:`scanvic2025scale`.\n-  ``Homography`` and the projective geometry framework: :footcite:t:`wang2024perspective`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.transforms import Compose, ToTensor, CenterCrop, Resize\n\nimport deepinv as dinv\nfrom deepinv.utils import get_data_home\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nORIGINAL_DATA_DIR = get_data_home() / \"Urban100\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define transforms. For the transforms that involve 3D camera rotation\n(i.e pan or tilt), we limit ``theta_max`` for display.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transforms = [\n    dinv.transform.Shift(),\n    dinv.transform.Rotate(),\n    dinv.transform.Scale(),\n    dinv.transform.Homography(theta_max=10),\n    dinv.transform.projective.Euclidean(),\n    dinv.transform.projective.Similarity(),\n    dinv.transform.projective.Affine(),\n    dinv.transform.projective.PanTiltRotate(theta_max=10),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot transforms on a sample image. Note that, during training, we never\nhave access to these ground truth images ``x``, only partial and noisy\nmeasurements ``y``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"celeba_example.jpg\")\ndinv.utils.plot(\n    [x] + [t(x) for t in transforms],\n    [\"Orig\"] + [t.__class__.__name__ for t in transforms],\n    fontsize=24,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we run an inpainting experiment to reconstruct images from images\nmasked with a random mask, without ground truth, using EI. For this\nexample we use the Urban100 images of natural urban scenes. As these\nscenes are imaged with a camera free to move and rotate in the world,\nall of the above transformations are valid invariances that we can\nimpose on the unknown image set $x\\in X$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = dinv.datasets.Urban100HR(\n    root=ORIGINAL_DATA_DIR,\n    download=True,\n    transform=Compose([ToTensor(), Resize(256), CenterCrop(256)]),\n)\n\ntrain_dataset, test_dataset = random_split(dataset, (0.8, 0.2))\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True)\ntest_dataloader = DataLoader(test_dataset)\n\n# Use physics to generate data online\nphysics = dinv.physics.Inpainting((3, 256, 256), mask=0.6, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For training, use a small UNet, Adam optimizer, EI loss with homography\ntransform, and the ``deepinv.Trainer`` functionality:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We only train for a single epoch in the demo, but it is recommended to train multiple epochs in practice.</p></div>\n\nTo simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\nsuch as PSNR or SSIM, which require clean ground truth images.\n\n.. tip::\n\n      We can use the same self-supervised loss for evaluation, as it does not require clean images,\n      to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.UNet(\n    in_channels=3, out_channels=3, scales=2, circular_padding=True, batch_norm=False\n).to(device)\n\nlosses = [\n    dinv.loss.MCLoss(),\n    dinv.loss.EILoss(dinv.transform.Homography(theta_max=10, device=device)),\n]\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n\nmodel = dinv.Trainer(\n    model=model,\n    physics=physics,\n    online_measurements=True,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    epochs=1,\n    losses=losses,\n    metrics=None,  # no supervised metrics\n    early_stop=2,  # we can use early stopping as we have a validation set\n    optimizer=optimizer,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=device,\n).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show results of a pretrained model trained using a larger UNet for 40\nepochs:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.UNet(\n    in_channels=3, out_channels=3, scales=3, circular_padding=True, batch_norm=False\n).to(device)\n\nckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"ei\", \"Urban100_inpainting_homography_model.pth\"),\n    map_location=device,\n)\n\nmodel.load_state_dict(ckpt[\"state_dict\"])\n\nx = next(iter(train_dataloader))\nx = x.to(device)\ny = physics(x)\nx_hat = model(y)\n\ndinv.utils.plot([x, y, x_hat], [\"x\", \"y\", \"reconstruction\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}