{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised learning with Equivariant Imaging for MRI.\n\nThis example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.\n\nThe equivariant imaging loss is presented in :footcite:t:`chen2021equivariant`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nimport deepinv as dinv\nfrom deepinv.datasets import SimpleFastMRISliceDataset\nfrom deepinv.utils import get_data_home, load_degradation\nfrom deepinv.models.utils import get_weights_url\nfrom deepinv.models import MoDL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"measurements\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use a mini demo subset of the single-coil [FastMRI dataset](https://fastmri.org/)\nas the base image dataset, consisting of 2 knee images of size 320x320.\n\n.. seealso::\n\n  Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n      We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n      You can download more data on the [FastMRI site](https://fastmri.med.nyu.edu/).\n\n.. important::\n\n   By using this dataset, you confirm that you have agreed to and signed the [FastMRI data use agreement](https://fastmri.med.nyu.edu/).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We reduce to the size to 128x128 for faster training in the demo.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "operation = \"MRI\"\nimg_size = 128\n\ntransform = transforms.Compose([transforms.Resize(img_size)])\n\ntrain_dataset = SimpleFastMRISliceDataset(\n    get_data_home(), transform=transform, train_percent=0.5, train=True, download=True\n)\ntest_dataset = SimpleFastMRISliceDataset(\n    get_data_home(), transform=transform, train_percent=0.5, train=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of knee images and load it.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = load_degradation(\"mri_mask_128x128.npy\")\n\n# defined physics\nphysics = dinv.physics.MRI(mask=mask, device=device)\n\n# Use parallel dataloader if using a GPU to speed up training,\n# otherwise, as all computes are on CPU, use synchronous data loading.\nnum_workers = 4 if torch.cuda.is_available() else 0\nn_images_max = (\n    900 if torch.cuda.is_available() else 5\n)  # number of images used for training\n\nmy_dataset_name = \"demo_equivariant_imaging\"\nmeasurement_dir = DATA_DIR / \"fastmri\" / operation\ndeepinv_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the reconstruction network\n\nAs a (static) reconstruction network, we use an unrolled network\n(half-quadratic splitting) with a trainable denoising prior based on the\nDnCNN architecture which was proposed in MoDL :footcite:t:`aggarwal2018modl`.\nSee :class:`deepinv.models.MoDL` for details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = MoDL().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the training parameters\nWe choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)\nand the equivariant imaging loss (EI).\nThe EI loss requires a group of transformations to be defined. The forward model should not be equivariant to\nthese transformations :footcite:t:`tachella2023sensing`.\nHere we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is\nnot equivariant to rotations (while it is equivariant to translations).\n\nSee `docs <transform>` for full list of available transforms.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use a pretrained model to reduce training time. You can get the same results by training from scratch\n      for 150 epochs using a larger knee dataset of ~1000 images.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 1  # choose training epochs\nlearning_rate = 5e-4\nbatch_size = 16 if torch.cuda.is_available() else 1\n\n# choose self-supervised training losses\n# generates 4 random rotations per image in the batch\nlosses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(n_trans=4))]\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)\n\n# start with a pretrained model to reduce training time\nfile_name = \"new_demo_ei_ckp_150_v3.pth\"\nurl = get_weights_url(model_name=\"demo\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url,\n    map_location=lambda storage, loc: storage,\n    file_name=file_name,\n)\n# load a checkpoint to reduce training time\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\nTo simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\nsuch as PSNR or SSIM, which require clean ground truth images.\n\n.. tip::\n\n      We can use the same self-supervised loss for evaluation, as it does not require clean images,\n      to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verbose = True  # print training information\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n)\n\n# Initialize the trainer\ntrainer = dinv.Trainer(\n    model,\n    physics=physics,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    optimizer=optimizer,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    metrics=None,  # no supervised metrics\n    early_stop=2,  # early stop using the self-supervised loss on the test set\n    plot_images=True,\n    device=device,\n    save_path=str(CKPT_DIR / operation),\n    verbose=verbose,\n    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n    ckp_interval=10,\n)\n\nmodel = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\n\nWe now assume that we have access to a small test set of ground-truth images to evaluate the performance of the trained network.\nand we compute the PSNR between the denoised images and the clean ground truth images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.test(test_dataloader, metrics=dinv.metric.PSNR())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}