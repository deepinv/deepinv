{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Patch priors for limited-angle computed tomography\n\nIn this example we use patch priors for limited angle computed tomography. More precisely, we consider the\ninverse problem $y = \\mathrm{noisy}(Ax)$, where $A$ is the discretized Radon transform\nwith $100$ equispace angles between 20 and 160 degrees.\nFor the reconstruction, we minimize the variational problem\n\n\\begin{align}\\begin{equation*}\n    \\label{eq:min_prob}\n    \\underset{x}{\\arg\\min} \\quad \\datafid{x}{y} + \\lambda g(x).\n    \\end{equation*}\\end{align}\n\nHere, the regularizier $g$ is explicitly defined as\n\n\\begin{align}\\begin{equation*}\n    g(x)=\\sum_{i\\in\\mathcal{I}} h(P_i x),\n    \\end{equation*}\\end{align}\n\nwhere $P_i$ is the linear operator which extracts the $i$-th patch from the image $x$ and\n$h$ is a regularizer on the space of patches.\nWe consider the following two choices of $h$:\n\n* The expected patch log-likelihood (EPLL) prior was proposed by :footcite:t:`zoran2011learning`.\n  It sets $h(x)=-\\log(p_\\theta(x))$, where $p_\\theta$ is the probability density function of a Gaussian mixture model.\n  The parameters $\\theta$ are estimated a-priori on a (possibly small) data set of training patches using\n  an expectation maximization algorithm.\n  In contrast to the original paper by Zoran and Weiss, we minimize the arising variational problem by simply applying\n  the Adam optimizers. For an example for using the (approximated) half-quadratic splitting algorithm proposed by Zoran\n  and Weiss, we refer to the denoising example...\n\n* The patch normalizing flow regularizer (PatchNR) was proposed by :footcite:t:`altekruger2023patchnr`.\n  It models $h(x)=-\\log(p_{\\theta}(x))$ as negative log-likelihood function of a probaility density function\n  $p_\\theta={\\mathcal{T}_\\theta}_\\#\\mathcal{N}(0,I)$ which is given as the push-forward measure of a standard\n  normal distribution under a normalizing flow (invertible neural network) $\\mathcal{T}_\\theta$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils.data import DataLoader\nfrom deepinv.datasets import PatchDataset\nfrom deepinv import Trainer\nfrom deepinv.physics import LogPoissonNoise, Tomography, Denoising, UniformNoise\nfrom deepinv.optim import LogPoissonLikelihood, PatchPrior, PatchNR, EPLL\nfrom deepinv.loss.metric import PSNR\nfrom deepinv.utils import plot\nfrom deepinv.utils import load_torch_url\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training and test images\nHere, we use downsampled images from the [\"LoDoPaB-CT dataset\"](https://zenodo.org/records/3384092).\nMoreover, we define the size of the used patches and generate the dataset of patches in the training images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = \"https://huggingface.co/datasets/deepinv/LoDoPaB-CT_toy/resolve/main/LoDoPaB-CT_small.pt\"\ndataset = load_torch_url(url)\ntrain_imgs = dataset[\"train_imgs\"].to(device)\ntest_imgs = dataset[\"test_imgs\"].to(device)\nimg_size = train_imgs.shape[-1]\n\npatch_size = 3\nverbose = True\ntrain_dataset = PatchDataset(train_imgs, patch_size=patch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set parameters for EPLL and PatchNR\nFor PatchNR, we choose the number of hidden neurons in the subnetworks and for the training batch size and number of epochs.\nFor EPLL, we set the number of mixture components and the maximum number of steps and batch size for fitting the EM algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "patchnr_subnetsize = 128\npatchnr_epochs = 5\npatchnr_batch_size = 32\npatchnr_learning_rate = 1e-4\n\nepll_num_components = 20\nepll_max_iter = 20\nepll_batch_size = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training / EM algorithm\nIf the parameter retrain is False, we just load pretrained weights. Set the parameter to True for retraining.\nOn the cpu, this takes up to a couple of minutes.\nAfter training, we define the corresponding patch priors\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The normalizing flow training minimizes the forward Kullback-Leibler (maximum likelihood) loss function given by\n\n           .. math::\n                      \\mathcal{L}(\\theta)=\\mathrm{KL}(P_X,{\\mathcal{T}_\\theta}_\\#P_Z)=\n                      \\mathbb{E}_{x\\sim P_X}[p_{{\\mathcal{T}_\\theta}_\\#P_Z}(x)]+\\mathrm{const},\n\n           where $\\mathcal{T}_\\theta$ is the normalizing flow with parameters $\\theta$, latent distribution\n           $P_Z$, data distribution $P_X$ and push-forward measure ${\\mathcal{T}_\\theta}_\\#P_Z$.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "retrain = False\nif retrain:\n    model_patchnr = PatchNR(\n        pretrained=None,\n        sub_net_size=patchnr_subnetsize,\n        device=device,\n        patch_size=patch_size,\n    )\n    patchnr_dataloader = DataLoader(\n        train_dataset,\n        batch_size=patchnr_batch_size,\n        shuffle=True,\n        drop_last=True,\n    )\n\n    class NFTrainer(Trainer):\n        def compute_loss(self, physics, x, y, train=True, epoch=None):\n            logs = {}\n\n            self.optimizer.zero_grad()  # Zero the gradients\n\n            # Evaluate reconstruction network\n            invs, jac_inv = self.model(y)\n\n            # Compute the Kullback Leibler loss\n            loss_total = torch.mean(\n                0.5 * torch.sum(invs.view(invs.shape[0], -1) ** 2, -1)\n                - jac_inv.view(invs.shape[0])\n            )\n            current_log = (\n                self.logs_total_loss_train if train else self.logs_total_loss_eval\n            )\n            current_log.update(loss_total.item())\n            logs[f\"TotalLoss\"] = current_log.avg\n\n            if train:\n                loss_total.backward()  # Backward the total loss\n                self.optimizer.step()  # Optimizer step\n\n            return invs, logs\n\n    optimizer = torch.optim.Adam(\n        model_patchnr.normalizing_flow.parameters(), lr=patchnr_learning_rate\n    )\n    trainer = NFTrainer(\n        model=model_patchnr.normalizing_flow,\n        physics=Denoising(UniformNoise(1.0 / 255.0)),\n        optimizer=optimizer,\n        train_dataloader=patchnr_dataloader,\n        device=device,\n        losses=[],\n        epochs=patchnr_epochs,\n        online_measurements=True,\n        verbose=verbose,\n    )\n\n    trainer.train()\n\n    model_epll = EPLL(\n        pretrained=None,\n        n_components=epll_num_components,\n        patch_size=patch_size,\n        device=device,\n    )\n    epll_dataloader = DataLoader(\n        train_dataset,\n        batch_size=epll_batch_size,\n        shuffle=True,\n        drop_last=False,\n    )\n    model_epll.GMM.fit(epll_dataloader, verbose=verbose, max_iters=epll_max_iter)\nelse:\n    model_patchnr = PatchNR(\n        pretrained=\"PatchNR_lodopab_small2\",\n        sub_net_size=patchnr_subnetsize,\n        device=device,\n        patch_size=patch_size,\n    )\n    model_epll = EPLL(\n        pretrained=\"GMM_lodopab_small2\",\n        n_components=epll_num_components,\n        patch_size=patch_size,\n        device=device,\n    )\n\npatchnr_prior = PatchPrior(model_patchnr, patch_size=patch_size)\nepll_prior = PatchPrior(model_epll.negative_log_likelihood, patch_size=patch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of forward operator and noise model\nThe training depends only on the image domain or prior distribution.\nFor the reconstruction, we now define forward operator and noise model.\nFor the noise model, we use log-Poisson noise as suggested for the LoDoPaB dataset.\nThen, we generate an observation by applying the physics and compute the filtered backprojection.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mu = 1 / 50.0 * (362.0 / img_size)\nN0 = 1024.0\nnum_angles = 100\nnoise_model = LogPoissonNoise(mu=mu, N0=N0)\ndata_fidelity = LogPoissonLikelihood(mu=mu, N0=N0)\nangles = torch.linspace(20, 160, steps=num_angles, device=device)\nphysics = Tomography(\n    img_width=img_size, angles=angles, device=device, noise_model=noise_model\n)\nobservation = physics(test_imgs)\nfbp = physics.A_dagger(observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconstruction loop\nWe define a reconstruction loop for minimizing the variational problem using the Adam optimizer.\nAs initialization, we choose the filtered backprojection.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim_steps = 200\nlr_variational_problem = 0.02\n\n\ndef minimize_variational_problem(prior, lam):\n    imgs = fbp.detach().clone()\n    imgs.requires_grad_(True)\n    optimizer = torch.optim.Adam([imgs], lr=lr_variational_problem)\n    for i in (progress_bar := tqdm(range(optim_steps))):\n        optimizer.zero_grad()\n        loss = data_fidelity(imgs, observation, physics).mean() + lam * prior(imgs)\n        loss.backward()\n        optimizer.step()\n        progress_bar.set_description(\"Step {}\".format(i + 1))\n    return imgs.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and plot\nFinally, we run the reconstruction loop for both priors and plot the results.\nThe regularization parameter is roughly choosen by a grid search but not fine-tuned\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lam_patchnr = 120.0\nlam_epll = 120.0\n\nrecon_patchnr = minimize_variational_problem(patchnr_prior, lam_patchnr)\nrecon_epll = minimize_variational_problem(epll_prior, lam_epll)\n\npsnr_fbp = PSNR()(fbp, test_imgs).item()\npsnr_patchnr = PSNR()(recon_patchnr, test_imgs).item()\npsnr_epll = PSNR()(recon_epll, test_imgs).item()\n\nprint(\"PSNRs:\")\nprint(\"Filtered Backprojection: {0:.2f}\".format(psnr_fbp))\nprint(\"EPLL: {0:.2f}\".format(psnr_epll))\nprint(\"PatchNR: {0:.2f}\".format(psnr_patchnr))\n\nplot(\n    [\n        test_imgs,\n        fbp.clip(0, 1),\n        recon_epll.clip(0, 1),\n        recon_patchnr.clip(0, 1),\n    ],\n    [\"Ground truth\", \"Filtered Backprojection\", \"EPLL\", \"PatchNR\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}