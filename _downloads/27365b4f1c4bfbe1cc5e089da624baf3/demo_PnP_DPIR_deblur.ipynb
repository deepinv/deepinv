{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DPIR method for PnP image deblurring.\n\nThis example shows how to use the DPIR method to solve a PnP image deblurring problem. The DPIR method is described in :footcite:t:`zhang2021plug`.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3929-3938).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import DataLoader\nfrom deepinv.models import DRUNet\nfrom deepinv.optim.data_fidelity import L2\nfrom deepinv.optim.prior import PnP\nfrom deepinv.optim.optimizers import optim_builder\nfrom deepinv.training import test\nfrom torchvision import transforms\nfrom deepinv.optim.dpir import get_DPIR_params\nfrom deepinv.utils import load_dataset, load_degradation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nDATA_DIR = BASE_DIR / \"measurements\"\nRESULTS_DIR = BASE_DIR / \"results\"\nDEG_DIR = BASE_DIR / \"degradations\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use the Set3C dataset and a motion blur kernel from :footcite:t:`levin2009understanding`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\n# Set up the variable to fetch dataset and operators.\nmethod = \"DPIR\"\ndataset_name = \"set3c\"\nimg_size = 128 if torch.cuda.is_available() else 32\nval_transform = transforms.Compose(\n    [transforms.CenterCrop(img_size), transforms.ToTensor()]\n)\n\n# Generate a motion blur operator.\nkernel_index = 1  # which kernel to chose among the 8 motion kernels from 'Levin09.mat'\nkernel_torch = load_degradation(\"Levin09.npy\", DEG_DIR / \"kernels\", index=kernel_index)\nkernel_torch = kernel_torch.unsqueeze(0).unsqueeze(\n    0\n)  # add batch and channel dimensions\ndataset = load_dataset(dataset_name, transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of blurred images and load it.\nWe use the BlurFFT class from the physics module to generate a dataset of blurred images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_level_img = 0.03  # Gaussian Noise standard deviation for the degradation\nn_channels = 3  # 3 for color images, 1 for gray-scale images\np = dinv.physics.BlurFFT(\n    img_size=(n_channels, img_size, img_size),\n    filter=kernel_torch,\n    device=device,\n    noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),\n)\n\n# Use parallel dataloader if using a GPU to speed up training,\n# otherwise, as all computes are on CPU, use synchronous data loading.\nnum_workers = 4 if torch.cuda.is_available() else 0\n\nn_images_max = 3  # Maximal number of images to restore from the input dataset\n# Generate a dataset in a HDF5 folder in \"{dir}/dinv_dataset0.h5'\" and load it.\noperation = \"deblur\"\nmeasurement_dir = DATA_DIR / dataset_name / operation\ndinv_dataset_path = dinv.datasets.generate_dataset(\n    train_dataset=dataset,\n    test_dataset=None,\n    physics=p,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n)\n\nbatch_size = 3  # batch size for testing. As the number of iterations is fixed, we can use batch_size > 1\n# and restore multiple images in parallel.\ndataset = dinv.datasets.HDF5Dataset(path=dinv_dataset_path, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the DPIR algorithm to solve the inverse problem.\nThis method is based on half-quadratic splitting (HQS).\nThe algorithm alternates between a denoising step and a data fidelity step, where\nthe denoising step is performed by a pretrained denoiser :class:`deepinv.models.DRUNet`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We provide a wrapper for rapidly creating the DPIR algorithm in :class:`deepinv.optim.DPIR`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load specific parameters for DPIR\nsigma_denoiser, stepsize, max_iter = get_DPIR_params(noise_level_img, device=device)\nparams_algo = {\"stepsize\": stepsize, \"g_param\": sigma_denoiser}\nearly_stop = False  # Do not stop algorithm with convergence criteria\n\n# Select the data fidelity term\ndata_fidelity = L2()\n\n# Specify the denoising prior\nprior = PnP(denoiser=DRUNet(pretrained=\"download\", device=device))\n\n# instantiate the algorithm class to solve the IP problem.\nmodel = optim_builder(\n    iteration=\"HQS\",\n    prior=prior,\n    data_fidelity=data_fidelity,\n    early_stop=early_stop,\n    max_iter=max_iter,\n    verbose=True,\n    params_algo=params_algo,\n)\n\n# Set the model to evaluation mode. We do not require training here.\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model on the problem.\nThe test function evaluates the model on the test dataset and computes the metrics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "save_folder = RESULTS_DIR / method / operation / dataset_name\nplot_convergence_metrics = True  # Metrics are saved in save_folder.\nplot_images = True  # Images are saved in save_folder.\n\ndataloader = DataLoader(\n    dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n)\n\ntest(\n    model=model,\n    test_dataloader=dataloader,\n    physics=p,\n    metrics=[dinv.metric.PSNR(), dinv.metric.LPIPS(device=device)],\n    device=device,\n    plot_images=plot_images,\n    save_folder=save_folder,\n    plot_convergence_metrics=plot_convergence_metrics,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}