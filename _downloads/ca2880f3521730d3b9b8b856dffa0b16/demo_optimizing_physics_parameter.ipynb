{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Solving blind inverse problems / estimating physics parameters\n\nThis demo shows you how to use\n:class:`deepinv.physics.Physics` together with automatic differentiation to optimize your operator.\n\nConsider the forward model\n\n\\begin{align}y = \\noise{\\forw{x, \\theta}}\\end{align}\n\nwhere $N$ is the noise model, $\\forw{\\cdot, \\theta}$ is the forward operator, and the goal is to learn the parameter $\\theta$ (e.g., the filter in :class:`deepinv.physics.Blur`).\n\nIn a typical blind inverse problem, given a measurement $y$, we would like to recover both the underlying image $x$ and the operator parameter $\\theta$,\nresulting in a highly ill-posed inverse problem.\n\nIn this example, we only focus on a much more simpler problem: given the measurement $y$ and the ground truth $x$, find the parameter $\\theta$.\nThis can be reformulated as the following optimization problem:\n\n\\begin{align}\\min_{\\theta} \\frac{1}{2} \\|\\forw{x, \\theta} - y \\|^2\\end{align}\n\nThis problem can be addressed by first-order optimization if we can compute the gradient of the above function with respect to $\\theta$.\nThe dependence between the operator $A$ and the parameter $\\theta$ can be complicated.\nDeepInverse provides a wide range of physics operators, implemented as differentiable classes.\nWe can leverage the automatic differentiation engine provided in Pytorch to compute the gradient of the above loss function w.r.t. the physics parameters $\\theta$.\n\nThe purpose of this demo is to show how to use the physics classes in DeepInverse to estimate the physics parameters, together with the automatic differentiation.\nWe show 3 different ways to do this: manually implementing the projected gradient descent algorithm, using a Pytorch optimizer and optimizing the physics as a usual neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import required packages\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the physics\n\nIn this first example, we use the convolution operator, defined in the :class:`deepinv.physics.Blur` class.\nWe also generate a random convolution kernel of motion blur\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generator = dinv.physics.generator.MotionBlurGenerator(\n    psf_size=(25, 25), rng=torch.Generator(device), device=device\n)\ntrue_kernel = generator.step(1, seed=123)[\"filter\"]\nphysics = dinv.physics.Blur(noise_model=dinv.physics.GaussianNoise(0.02), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_url_image(\n    dinv.utils.get_image_url(\"celeba_example.jpg\"),\n    img_size=256,\n    resize_mode=\"resize\",\n).to(device)\n\ny = physics(x, filter=true_kernel)\n\ndinv.utils.plot([x, y, true_kernel], titles=[\"Sharp\", \"Blurry\", \"True kernel\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define an optimization algorithm\n\nThe convolution kernel lives in the simplex, ie the kernel must have positive entries summing to 1.\nWe can use a simple optimization algorithm - Projected Gradient Descent - to enforce this constraint.\nThe following function allows one to compute the orthogonal projection onto the simplex, by a sorting algorithm\n(Reference: [Large-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex\n-- Mathieu Blondel, Akinori Fujino, and Naonori Ueda](https://ieeexplore.ieee.org/document/6976941))\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\ndef projection_simplex_sort(v: torch.Tensor) -> torch.Tensor:\n    r\"\"\"\n    Projects a tensor onto the simplex using a sorting algorithm.\n    \"\"\"\n    shape = v.shape\n    B = shape[0]\n    v = v.view(B, -1)\n    n_features = v.size(1)\n    u = torch.sort(v, descending=True, dim=-1).values\n    cssv = torch.cumsum(u, dim=-1) - 1.0\n    ind = torch.arange(n_features, device=v.device)[None, :].expand(B, -1) + 1.0\n    cond = u - cssv / ind > 0\n    rho = ind[cond][-1]\n    theta = cssv[cond][-1] / rho\n    w = torch.maximum(v - theta, torch.zeros_like(v))\n    return w.reshape(shape)\n\n\n# We also define a data fidelity term\ndata_fidelity = dinv.optim.L2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the algorithm\n\nInitialize a constant kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_init = torch.zeros_like(true_kernel)\nkernel_init[..., 5:-5, 5:-5] = 1.0\nkernel_init = projection_simplex_sort(kernel_init)\nn_iter = 1000\nstepsize = 0.7\n\nkernel_hat = kernel_init\nlosses = []\nfor i in tqdm(range(n_iter)):\n    # compute the gradient\n    with torch.enable_grad():\n        kernel_hat.requires_grad_(True)\n        physics.update(filter=kernel_hat)\n        loss = data_fidelity(y=y, x=x, physics=physics) / y.numel()\n        loss.backward()\n    grad = kernel_hat.grad\n\n    # gradient step and projection step\n    with torch.no_grad():\n        kernel_hat = kernel_hat - stepsize * grad\n        kernel_hat = projection_simplex_sort(kernel_hat)\n\n    losses.append(loss.item())\n\ndinv.utils.plot(\n    [true_kernel, kernel_init, kernel_hat],\n    titles=[\"True kernel\", \"Init. kernel\", \"Estimated kernel\"],\n    suptitle=\"Result with Projected Gradient Descent\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the loss to make sure that it decreases\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(range(n_iter), losses)\nplt.title(\"Loss evolution\")\nplt.yscale(\"log\")\nplt.xlabel(\"Iteration\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine with arbitrary optimizer\n\nPytorch provides a wide range of optimizers for training neural networks.\nWe can also pick one of those to optimizer our parameter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_init = torch.zeros_like(true_kernel)\nkernel_init[..., 5:-5, 5:-5] = 1.0\nkernel_init = projection_simplex_sort(kernel_init)\n\nkernel_hat = kernel_init.clone()\noptimizer = torch.optim.Adam([kernel_hat], lr=0.1)\n\n# We will alternate a gradient step and a projection step\nlosses = []\nn_iter = 200\nfor i in tqdm(range(n_iter)):\n    optimizer.zero_grad()\n    # compute the gradient, this will directly change the gradient of `kernel_hat`\n    with torch.enable_grad():\n        kernel_hat.requires_grad_(True)\n        physics.update(filter=kernel_hat)\n        loss = data_fidelity(y=y, x=x, physics=physics) / y.numel()\n        loss.backward()\n\n    # a gradient step\n    optimizer.step()\n    # projection step, when doing additional steps, it's important to change only\n    # the tensor data to avoid breaking the gradient computation\n    kernel_hat.data = projection_simplex_sort(kernel_hat.data)\n    # loss\n    losses.append(loss.item())\n\ndinv.utils.plot(\n    [true_kernel, kernel_init, kernel_hat],\n    titles=[\"True kernel\", \"Init. kernel\", \"Estimated kernel\"],\n    suptitle=\"Result with ADAM\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the loss to make sure that it decreases\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.semilogy(range(n_iter), losses)\nplt.title(\"Loss evolution\")\nplt.xlabel(\"Iteration\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the physics as a usual neural network\n\nBelow we show another way to optimize the parameter of the physics, as we usually do for neural networks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_init = torch.zeros_like(true_kernel)\nkernel_init[..., 5:-5, 5:-5] = 1.0\nkernel_init = projection_simplex_sort(kernel_init)\n\n# The gradient is off by default, we need to enable the gradient of the parameter\nphysics = dinv.physics.Blur(\n    filter=kernel_init.clone().requires_grad_(True), device=device\n)\n\n# Set up the optimizer by giving the parameter to an optimizer\n# Try to change your favorite optimizer\noptimizer = torch.optim.AdamW([physics.filter], lr=0.1)\n\n\n# Try to change another loss function\n# loss_fn = torch.nn.MSELoss()\nloss_fn = torch.nn.L1Loss()\n\n# We will alternate a gradient step and a projection step\nlosses = []\nn_iter = 100\nfor i in tqdm(range(n_iter)):\n    # update the gradient\n    optimizer.zero_grad()\n    y_hat = physics.A(x)\n    loss = loss_fn(y_hat, y)\n    loss.backward()\n\n    # a gradient step\n    optimizer.step()\n\n    # projection step.\n    # Note: when doing additional steps, it's important to change only\n    # the tensor data to avoid breaking the gradient computation\n    physics.filter.data = projection_simplex_sort(physics.filter.data)\n\n    # loss\n    losses.append(loss.item())\n\nkernel_hat = physics.filter.data\ndinv.utils.plot(\n    [true_kernel, kernel_init, kernel_hat],\n    titles=[\"True kernel\", \"Init. kernel\", \"Estimated kernel\"],\n    suptitle=\"Result with AdamW and L1 Loss\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the loss to make sure that it decreases\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.semilogy(range(n_iter), losses)\nplt.title(\"Loss evolution\")\nplt.xlabel(\"Iteration\")\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}