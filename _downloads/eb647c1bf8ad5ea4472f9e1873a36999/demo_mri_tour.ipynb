{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tour of MRI functionality in DeepInverse\n\nThis example presents the various datasets, forward physics and models\navailable in DeepInverse for Magnetic Resonance Imaging (MRI) problems:\n\n-  Physics: :class:`deepinv.physics.MRI`,\n   :class:`deepinv.physics.MultiCoilMRI`,\n   :class:`deepinv.physics.DynamicMRI`\n-  Datasets: raw kspace with the [FastMRI](https://fastmri.med.nyu.edu)_ dataset\n   :class:`deepinv.datasets.FastMRISliceDataset` and an in-memory easy-to-use version\n   :class:`deepinv.datasets.SimpleFastMRISliceDataset`, and raw dynamic k-t-space data with the\n   [CMRxRecon](https://cmrxrecon.github.io)_ dataset.\n-  Models: :class:`deepinv.models.VarNet`\n   (VarNet :footcite:t:`hammernik2018learning`, E2E-VarNet :footcite:t:`sriram2020end`),\n   :class:`deepinv.models.MoDL` (a simple MoDL :footcite:t:`aggarwal2018modl` unrolled model)\n\nContents:\n\n1. Get started with FastMRI (singlecoil + multicoil)\n2. Train an accelerated MRI with neural networks\n3. Load raw FastMRI data (singlecoil + multicoil)\n4. Train using raw data\n5. Explore 3D MRI\n6. Explore dynamic MRI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch, torchvision\nfrom torch.utils.data import DataLoader\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\nrng = torch.Generator(device=device).manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get started with FastMRI\n\nYou can get started with our simple\n[FastMRI](https://fastmri.med.nyu.edu)_ mini slice subsets which provide\nquick, easy-to-use, in-memory datasets which can be used for simulation\nexperiments.\n\n.. important::\n\n   By using this dataset, you confirm that you have agreed to and signed the [FastMRI data use agreement](https://fastmri.med.nyu.edu/).\n\n.. seealso::\n\n  Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n      We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n      You can download more data on the [FastMRI site](https://fastmri.med.nyu.edu/).\n\nLoad mini demo knee and brain datasets (original data is 320x320 but we resize to\n128 for speed):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Resize(128)\nknee_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(),\n    anatomy=\"knee\",\n    transform=transform,\n    train=True,\n    download=True,\n)\nbrain_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(),\n    anatomy=\"brain\",\n    transform=transform,\n    train=True,\n    download=True,\n)\n\nimg_size = knee_dataset[0].shape[-2:]  # (128, 128)\ndinv.utils.plot({\"knee\": knee_dataset[0], \"brain\": brain_dataset[0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with single-coil MRI. We can define a constant Cartesian 4x\nundersampling mask by sampling once from a physics generator. The mask,\ndata and measurements will all be of shape ``(B, C, H, W)`` where\n``C=2`` is the real and imaginary parts.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics_generator = dinv.physics.generator.GaussianMaskGenerator(\n    img_size=img_size, acceleration=4, rng=rng, device=device\n)\nmask = physics_generator.step()[\"mask\"]\n\nphysics = dinv.physics.MRI(mask=mask, img_size=img_size, device=device)\n\nx = next(iter(DataLoader(knee_dataset))).to(device)\n\ndinv.utils.plot(\n    {\n        \"x\": x,\n        \"mask\": mask,\n        \"y\": physics(x).clamp(-1, 1),\n    }\n)\nprint(\"Shapes:\", x.shape, physics.mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can next generate an accelerated single-coil MRI measurement dataset. Let's use knees\nfor training and brains for testing.\n\nWe can also use the physics generator to randomly sample a new mask per\nsample, and save the masks alongside the measurements.\n\nNote that you could alternatively train using `online_measurements`, where you can generate\nrandom measurements on the fly.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = dinv.datasets.generate_dataset(\n    train_dataset=knee_dataset,\n    test_dataset=brain_dataset,\n    val_dataset=None,\n    physics=physics,\n    physics_generator=physics_generator,\n    save_physics_generator_params=True,\n    overwrite_existing=False,\n    device=device,\n    save_dir=dinv.utils.get_data_home(),\n    batch_size=1,\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(\n    dataset_path, split=\"train\", load_physics_generator_params=True\n)\ntest_dataset = dinv.datasets.HDF5Dataset(\n    dataset_path, split=\"test\", load_physics_generator_params=True\n)\n\ntrain_dataloader = DataLoader(train_dataset)\niterator = iter(train_dataloader)\n\nx0, y0, params0 = next(iterator)\nx1, y1, params1 = next(iterator)\n\ndinv.utils.plot(\n    {\n        \"x0\": x0,\n        \"mask0\": params0[\"mask\"],\n        \"x1\": x1,\n        \"mask1\": params1[\"mask\"],\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also simulate multicoil MRI data. Either pass in ground-truth\ncoil maps, or pass an integer to simulate simple birdcage coil maps. The\nmeasurements ``y`` are now of shape ``(B, C, N, H, W)``, where ``N`` is\nthe coil-dimension.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mc_physics = dinv.physics.MultiCoilMRI(img_size=img_size, coil_maps=3, device=device)\n\ndinv.utils.plot(\n    {\n        \"x\": x,\n        \"mask\": mask,\n        \"coil_map_0\": mc_physics.coil_maps.abs()[:, 0, ...],\n        \"coil_map_1\": mc_physics.coil_maps.abs()[:, 1, ...],\n        \"coil_map_2\": mc_physics.coil_maps.abs()[:, 2, ...],\n        \"RSS\": mc_physics.A_adjoint_A(x, mask=mask, rss=True),\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train an accelerated MRI problem with neural networks\n\nNext, we train a neural network to solve the MRI inverse problem. We provide various\nmodels specifically used for MRI reconstruction. These are unrolled\nnetworks which require a backbone denoiser, such as UNet or DnCNN:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "denoiser = dinv.models.UNet(\n    in_channels=2,\n    out_channels=2,\n    scales=2,\n)\n\ndenoiser = dinv.models.DnCNN(\n    in_channels=2,\n    out_channels=2,\n    pretrained=None,\n    depth=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These backbones can be used within specific MRI models, such as\nVarNet :footcite:t:`hammernik2018learning`, E2E-VarNet :footcite:t:`sriram2020end` and MoDL :footcite:t:`aggarwal2018modl`,\nfor which we provide implementations:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"varnet\").to(device)\n\nmodel = dinv.models.MoDL(denoiser, num_iter=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our architecture defined, we can train it with supervised or self-supervised (using Equivariant\nImaging) loss. We use the PSNR metric on the complex magnitude.\n\nFor the sake of speed in this example, we only use a very small 2-layer DnCNN inside an unrolled\nnetwork with 2 cascades, and train with 2 images for 1 epoch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = dinv.loss.SupLoss()\nloss = dinv.loss.EILoss(transform=dinv.transform.CPABDiffeomorphism())\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=train_dataloader,\n    metrics=dinv.metric.PSNR(complex_abs=True),\n    epochs=1,\n    show_progress_bar=False,\n    save_path=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve results in the case of this very short training, we start training from a pretrained model state (trained on 900 images):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = dinv.models.utils.get_weights_url(\n    model_name=\"demo\", file_name=\"demo_tour_mri.pth\"\n)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=\"demo_tour_mri.pth\"\n)\ntrainer.model.load_state_dict(ckpt[\"state_dict\"])  # load the state dict\ntrainer.optimizer.load_state_dict(ckpt[\"optimizer\"])  # load the optimizer state dict\n\nmodel = trainer.train()  # train the model\ntrainer.plot_images = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our model is trained, we can test it. Notice that we improve the PSNR compared to the zero-filled\nreconstruction, both on the train (knee) set and the test (brain) set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = trainer.test(train_dataloader)\n\n_ = trainer.test(DataLoader(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load raw FastMRI data\n\nIt is also possible to use the raw data directly.\nThe raw multi-coil FastMRI train/validation data is provided as pairs of ``(x, y)`` where\n``y`` are the fully-sampled k-space measurements of arbitrary size, and\n``x`` are the cropped root-sum-square (RSS) magnitude reconstructions.\nLet's download a sample volume and check out its middle slice.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.datasets.download_archive(\n    dinv.utils.get_image_url(\"demo_fastmri_brain_multicoil.h5\"),\n    dinv.utils.get_data_home() / \"brain\" / \"fastmri.h5\",\n)\n\ndataset = dinv.datasets.FastMRISliceDataset(\n    dinv.utils.get_data_home() / \"brain\", slice_index=\"middle\"\n)\n\nx, y = next(iter(DataLoader(dataset)))\nx = x.to(device)\ny = y.to(device)\n\nimg_size, kspace_shape = x.shape[-2:], y.shape[-2:]\nn_coils = y.shape[2]\n\nprint(\"Shapes:\", x.shape, y.shape)  # x (B, 1, W, W); y (B, C, N, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we can relate ``x`` and fully-sampled ``y`` using our\n:class:`deepinv.physics.MultiCoilMRI` (note that since we are not\nprovided with the ground-truth coil-maps, we can only perform the\nadjoint operator).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.MultiCoilMRI(\n    img_size=img_size,\n    mask=torch.ones(kspace_shape, device=device),\n    coil_maps=torch.ones(\n        (n_coils,) + kspace_shape, dtype=torch.complex64, device=device\n    ),\n    device=device,\n)\n\nx_rss = physics.A_adjoint(y, rss=True, crop=True)\n\nassert torch.allclose(x, x_rss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also pre-estimate coil sensitivity maps using ESPIRiT from the raw data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = dinv.datasets.FastMRISliceDataset(\n    dinv.utils.get_data_home() / \"brain\",\n    slice_index=\"middle\",\n    transform=dinv.datasets.MRISliceTransform(\n        estimate_coil_maps=True,\n        acs=15,  # Num. low frequency, fix to 15\n    ),\n)\n\nx, y, params = next(iter(DataLoader(dataset)))\n\nphysics.update(**params)\n\ndinv.utils.plot(\n    {\"x\": x, \"maps0\": physics.coil_maps[:, 0], \"maps1\": physics.coil_maps[:, 1]}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train using raw data\n\nFor training with multicoil raw data, we can simulate random masks **on-the-fly**:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = dinv.datasets.FastMRISliceDataset(\n    dinv.utils.get_data_home() / \"brain\",\n    slice_index=\"middle\",\n    transform=dinv.datasets.MRISliceTransform(\n        mask_generator=dinv.physics.generator.GaussianMaskGenerator(\n            img_size=kspace_shape,\n            acceleration=4,\n        ),\n        seed_mask_generator=False,  # More diversity during training\n        estimate_coil_maps=False,  # Set to true if coil maps are not already set in physics.\n        # This will use ACS size from mask generator. If mask generator is None, then try find ACS size from metadata.\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note if the data is already undersampled raw kspace data (e.g. FastMRI test set)\nyou can also easily directly load it and their associated masks for testing or training\n(optionally specify separate target folder if targets are in a different folder):\n\n::\n\n        dataset = dinv.datasets.FastMRISliceDataset(\n            root=root,\n            target_root=target_root,\n            transform=dinv.datasets.MRISliceTransform()\n        )\n\nWe use the E2E-VarNet model designed for\nmulticoil MRI. For this example, we do not perform joint coil sensitivity map estimation and\nsimply assume they are flat. If you want to estimate the maps, either pass a model\nas the ``sensitivity_model`` parameter, or use a different model which uses precomputed maps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"e2e-varnet\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to modify the metrics used to crop the model output and take the magnitude when\ncomparing to the cropped magnitude RSS targets:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def crop(x_net, x):\n    \"\"\"Crop to GT shape then take magnitude.\"\"\"\n    return dinv.utils.MRIMixin().rss(\n        dinv.utils.MRIMixin().crop(x_net, shape=x.shape), multicoil=False\n    )\n\n\nclass CropPSNR(dinv.metric.PSNR):\n    def forward(self, x_net=None, x=None, *args, **kwargs):\n        return super().forward(crop(x_net, x), x, *args, **kwargs)\n\n\nclass CropMSE(dinv.metric.MSE):\n    def forward(self, x_net=None, x=None, *args, **kwargs):\n        return super().forward(crop(x_net, x), x, *args, **kwargs)\n\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    losses=dinv.loss.SupLoss(metric=CropMSE()),\n    metrics=CropPSNR(),\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=DataLoader(dataset),\n    epochs=1,\n    save_path=None,\n    show_progress_bar=False,\n    device=device,\n)\n_ = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore 3D MRI\n\nWe can also simulate 3D MRI data.\nHere, we use a demo 3D brain volume of shape ``(181, 217, 181)`` from the\n[BrainWeb](https://brainweb.bic.mni.mcgill.ca/brainweb/) dataset\nand simulate 3D single-coil or multi-coil Fourier measurements using\n:class:`deepinv.physics.MRI` or\n:class:`deepinv.physics.MultiCoilMRI`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = (\n    dinv.utils.load_np_url(\n        \"https://huggingface.co/datasets/deepinv/images/resolve/main/brainweb_t1_ICBM_1mm_subject_0.npy?download=true\"\n    )\n    .unsqueeze(0)\n    .unsqueeze(0)\n    .to(device)\n)\nx = torch.cat([x, torch.zeros_like(x)], dim=1)  # add imaginary dimension\n\nprint(x.shape)  # (B, C, D, H, W) where D is depth\n\nphysics = dinv.physics.MultiCoilMRI(img_size=x.shape[1:], three_d=True, device=device)\nphysics = dinv.physics.MRI(img_size=x.shape[1:], three_d=True, device=device)\n\ndinv.utils.plot_ortho3D([x, physics(x)], titles=[\"x\", \"y\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Explore dynamic MRI\n\nFinally, we show how to use the dynamic MRI for image sequence data of\nshape ``(B, C, T, H, W)`` where ``T`` is the time dimension. Note that\nthis is also compatible with 3D MRI. We use dynamic MRI data from the\n[CMRxRecon](https://cmrxrecon.github.io/) challenge of cardiac cine\nsequences and load them using :class:`deepinv.datasets.CMRxReconSliceDataset`\nprovided in deepinv. We download demo data from the first patient\nincluding ground truth images, undersampled kspace, and associated masks:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.datasets.download_archive(\n    dinv.utils.get_image_url(\"CMRxRecon.zip\"),\n    dinv.utils.get_data_home() / \"CMRxRecon.zip\",\n    extract=True,\n)\n\ndataset = dinv.datasets.CMRxReconSliceDataset(\n    dinv.utils.get_data_home() / \"CMRxRecon\",\n)\n\nx, y, params = next(iter(DataLoader(dataset)))\n\nprint(\n    f\"\"\"\n    Ground truth: {x.shape} (B, C, T, H, W)\n    Measurements: {y.shape}\n    Acc. mask: {params[\"mask\"].shape}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dynamic MRI data is directly compatible with existing functionality.\nFor example, you can train with this data by passing the dataset to\n:class:`deepinv.Trainer`, which will automatically load in the data\n``x, y, params``. Or, you can use the data directly with the physics\n:class:`deepinv.physics.DynamicMRI`.\n\nYou can also pass in a custom k-t acceleration mask generator to\ngenerate random time-varying masks:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics_generator = dinv.physics.generator.EquispacedMaskGenerator(\n    img_size=x.shape[1:],\n    acceleration=16,\n)\nphysics = dinv.physics.DynamicMRI(img_size=(512, 256), device=device)\n\ndataset = dinv.datasets.CMRxReconSliceDataset(\n    dinv.utils.get_data_home() / \"CMRxRecon\",\n    mask_generator=physics_generator,\n    mask_dir=None,\n)\n\nx, y, params = next(iter(DataLoader(dataset)))\n\nx = x.to(device)\ny = y.to(device)\nparams = {k: v.to(device) for k, v in params.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We provide a video plotting function, :class:`deepinv.utils.plot_videos`. Here, we\nvisualize t=5 frames of the ground truth ``x``, the mask, and the zero-filled\nreconstruction ``x_zf`` (and crop to square for better visibility):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_zf = physics.A_adjoint(y, **params)\n\ndinv.utils.plot(\n    {\n        f\"t={i}\": torch.cat([x[:, :, i], params[\"mask\"][:, :, i], x_zf[:, :, i]])[\n            ..., 128:384, :\n        ]\n        for i in range(5)\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}