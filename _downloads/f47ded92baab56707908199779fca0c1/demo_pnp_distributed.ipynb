{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Plug-and-Play (PnP) Reconstruction\n\nMany large-scale imaging problems involve operators that can be naturally decomposed as a stack of\nmultiple sub-operators:\n\n\\begin{align}A(x) = \\begin{bmatrix} A_1(x) \\\\ \\vdots \\\\ A_N(x) \\end{bmatrix}\\end{align}\n\nwhere each sub-operator $A_i$ is computationally expensive. Examples include multi-coil MRI,\nradio interferometry, or multi-sensor imaging systems. Additionally, the images being reconstructed\ncan be very large, making it challenging to fit the entire reconstruction process into a single device's memory.\n\nThe distributed framework enables you to parallelize both the physics computations (distributing operators\nacross devices) and the denoising step (using image tiling for large images). This allows you to solve\nlarge-scale inverse problems that would otherwise be difficult to solve on a single device.\n\nThis example demonstrates how to implement a distributed Plug-and-Play (PnP) reconstruction algorithm\nwhere both the stacked physics operators and the denoiser are distributed across multiple processes using\n:func:`deepinv.distributed.distribute`.\n\n**Usage:**\n\n```bash\n# Single process\npython examples/distributed/demo_pnp_distributed.py\n```\n```bash\n# Multi-process with torchrun (2 processes)\npython -m torch.distributed.run --nproc_per_node=2 examples/distributed/demo_pnp_distributed.py\n```\n**Key Features:**\n\n- Distribute multiple physics operators across processes/devices\n- Distribute denoiser with image tiling\n- PnP algorithm with distributed components\n- $\\ell_2$ data fidelity gradient computed using :func:`deepinv.distributed.DistributedDataFidelity.grad`\n\n**Key Steps:**\n\n1. Create stacked physics operators and measurements with reproducible noise\n2. Initialize distributed context\n3. Distribute physics with :func:`deepinv.distributed.distribute`\n4. Distribute denoiser with tiling configuration\n5. Create PnP prior and $\\ell_2$ data fidelity\n6. Run PnP iterations using :func:`deepinv.distributed.DistributedDataFidelity.grad` for gradient computation\n7. Visualize results and track convergence\n\n# Import modules and define noisy image generation\nWe start by importing `torch` and the modules of deepinv that we use in this example. We also define a function that generates noisy images to evaluate the distributed framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom deepinv.physics import GaussianNoise, stack\nfrom deepinv.physics.blur import Blur, gaussian_blur\nfrom deepinv.utils.demo import load_example\nfrom deepinv.optim.data_fidelity import L2\nfrom deepinv.optim.prior import PnP\nfrom deepinv.loss.metric import PSNR\nfrom deepinv.utils.plotting import plot, plot_curves\nfrom deepinv.models import DRUNet\n\n# Import the distributed framework\nfrom deepinv.distributed import DistributedContext, distribute\n\n\ndef create_physics_and_measurements(device, img_size=1024, seed=42):\n    \"\"\"\n    Create stacked physics operators and measurements using example images.\n\n    :param device: Device to create operators on\n    :param tuple img_size: Size of the image (H, W)\n    :param int seed: Random seed for reproducible noise generation\n\n    :returns: Tuple of (stacked_physics, measurements, clean_image)\n    \"\"\"\n    # Load example image in original size\n    clean_image = load_example(\n        \"CBSD_0010.png\",\n        grayscale=False,\n        device=device,\n        img_size=img_size,\n        resize_mode=\"resize\",\n    )\n\n    # Create different Gaussian blur kernels\n    kernels = [\n        gaussian_blur(sigma=1.0, device=str(device)),  # Small blur\n        gaussian_blur(sigma=2.0, device=str(device)),  # Medium blur\n        gaussian_blur(\n            sigma=(1.5, 3.0), angle=30, device=str(device)\n        ),  # Anisotropic blur\n    ]\n\n    # Noise levels for each operator\n    noise_levels = [0.03, 0.05, 0.04]\n\n    # Create physics operators\n    physics_list = []\n\n    for i, (kernel, noise_level) in enumerate(zip(kernels, noise_levels)):\n        # Create blur operator with circular padding and noise model\n        rng = torch.Generator(device=device).manual_seed(seed + i)\n        physics = Blur(\n            filter=kernel,\n            padding=\"circular\",\n            device=str(device),\n            noise_model=GaussianNoise(sigma=noise_level, rng=rng),\n        )\n\n        physics_list.append(physics)\n\n    # Stack physics operators into a single operator\n    stacked_physics = stack(*physics_list)\n\n    # Generate measurements (returns a TensorList)\n    measurements = stacked_physics(clean_image)\n\n    return stacked_physics, measurements, clean_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration of parallel pnp\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_iterations = 20\nstep_size = 0.5\ndenoiser_sigma = 0.05\nimg_size = 512\npatch_size = 256\noverlap = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define distributed context and run algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize distributed context (handles single and multi-process automatically)\nwith DistributedContext(seed=42) as ctx:\n\n    if ctx.rank == 0:\n        print(\"=\" * 70)\n        print(\"Distributed PnP Reconstruction\")\n        print(\"=\" * 70)\n        print(f\"\\nRunning on {ctx.world_size} process(es)\")\n        print(f\"   Device: {ctx.device}\")\n\n    # ---------------------------------------------------------------------------\n    # Step 1: Create stacked physics operators and measurements\n    # ---------------------------------------------------------------------------\n\n    stacked_physics, measurements, clean_image = create_physics_and_measurements(\n        ctx.device, img_size=img_size\n    )\n\n    if ctx.rank == 0:\n        print(f\"\\nCreated stacked physics with {len(stacked_physics)} operators\")\n        print(f\"   Image shape: {clean_image.shape}\")\n        print(f\"   Measurements type: {type(measurements).__name__}\")\n\n    # ---------------------------------------------------------------------------\n    # Step 2: Distribute physics operators\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\n\ud83d\udd27 Distributing physics operators...\")\n\n    distributed_physics = distribute(stacked_physics, ctx)\n\n    if ctx.rank == 0:\n        print(f\"   Distributed physics created\")\n        print(\n            f\"   Local operators on this rank: {len(distributed_physics.local_indexes)}\"\n        )\n\n    # ---------------------------------------------------------------------------\n    # Step 3: Create L2 data fidelity\n    # ---------------------------------------------------------------------------\n\n    data_fidelity = L2()\n\n    distributed_data_fidelity = distribute(\n        data_fidelity, ctx\n    )  # Distribute L2 data fidelity, optional.\n\n    if ctx.rank == 0:\n        print(f\"\\nCreated L2 data fidelity\")\n\n    # ---------------------------------------------------------------------------\n    # Step 4: Distribute denoiser with tiling\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\n Loading and distributing denoiser...\")\n        print(f\"   Patch size: {patch_size}x{patch_size}\")\n        print(f\"   Receptive field radius: {overlap}\")\n\n    denoiser = DRUNet(pretrained=\"download\").to(ctx.device)\n\n    distributed_denoiser = distribute(\n        denoiser,\n        ctx,\n        patch_size=patch_size,\n        overlap=overlap,\n    )\n\n    if ctx.rank == 0:\n        print(f\"   Distributed denoiser created\")\n\n    # ---------------------------------------------------------------------------\n    # Step 5: Create PnP prior with distributed denoiser\n    # ---------------------------------------------------------------------------\n\n    prior = PnP(denoiser=distributed_denoiser)\n\n    if ctx.rank == 0:\n        print(f\"\\nCreated PnP prior with distributed denoiser\")\n\n    # ---------------------------------------------------------------------------\n    # Step 6: Run distributed PnP algorithm\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nRunning PnP reconstruction ({num_iterations} iterations)...\")\n\n    # Initialize reconstruction with zeros\n    x = torch.zeros_like(clean_image)\n\n    # Track PSNR (only on rank 0)\n    psnr_metric = PSNR()\n    psnr_history = []\n\n    # PnP iterations\n    with torch.no_grad():\n        for it in range(num_iterations):\n            # Data fidelity gradient step using the data_fidelity.grad() method\n            grad = distributed_data_fidelity.grad(x, measurements, distributed_physics)\n\n            # Gradient descent step\n            x = x - step_size * grad\n\n            # Denoising step (proximal operator of prior)\n            x = prior.prox(x, sigma_denoiser=denoiser_sigma)\n\n            # Compute PSNR on rank 0\n            if ctx.rank == 0:\n                psnr_val = psnr_metric(x, clean_image).item()\n                psnr_history.append(psnr_val)\n\n                if it == 0 or (it + 1) % 5 == 0:\n                    print(\n                        f\"   Iteration {it+1}/{num_iterations}, PSNR: {psnr_val:.2f} dB\"\n                    )\n\n    # ---------------------------------------------------------------------------\n    # Step 7: Compare with non-distributed PnP (only on rank 0)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nComparing with non-distributed PnP reconstruction...\")\n\n        # Run non-distributed PnP\n        x_ref = torch.zeros_like(clean_image)\n        with torch.no_grad():\n            for it in range(num_iterations):\n                # Data fidelity gradient step using data_fidelity.grad()\n                grad_ref = data_fidelity.grad(x_ref, measurements, stacked_physics)\n                x_ref = x_ref - step_size * grad_ref\n\n                # Denoising step\n                x_ref = denoiser(x_ref, sigma=denoiser_sigma)\n\n        # Compare results\n        diff = torch.abs(x - x_ref)\n        mean_diff = diff.mean().item()\n        max_diff = diff.max().item()\n\n        psnr_ref = psnr_metric(x_ref, clean_image).item()\n        psnr_dist = psnr_metric(x, clean_image).item()\n\n        print(f\"   Non-distributed final PSNR: {psnr_ref:.2f} dB\")\n        print(f\"   Distributed final PSNR:     {psnr_dist:.2f} dB\")\n        print(f\"   PSNR difference:             {abs(psnr_dist - psnr_ref):.2f} dB\")\n        print(f\"   Mean absolute difference:    {mean_diff:.2e}\")\n        print(f\"   Max absolute difference:     {max_diff:.2e}\")\n\n        # Check that results are close\n        assert (\n            abs(psnr_dist - psnr_ref) < 1.0\n        ), f\"PSNR difference too large: {abs(psnr_dist - psnr_ref):.2f} dB\"\n        print(f\"   Results match well!\")\n\n    # ---------------------------------------------------------------------------\n    # Step 8: Visualize results (only on rank 0)\n    # ---------------------------------------------------------------------------\n\n    if ctx.rank == 0:\n        print(f\"\\nReconstruction completed!\")\n        print(f\"   Final PSNR: {psnr_history[-1]:.2f} dB\")\n\n        # Plot results\n        plot(\n            [clean_image, measurements[0], x],\n            titles=[\"Ground Truth\", \"Measurement (first)\", \"Reconstruction\"],\n            save_fn=\"distributed_pnp_result.png\",\n            figsize=(12, 4),\n        )\n\n        # Plot convergence curve\n        plot_curves({\"psnr\": [psnr_history]}, save_dir=\".\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}