{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Use iterative reconstruction algorithms\n\nFollow this example to reconstruct images using an iterative algorithm.\n\nThe library provides a flexible framework to define your own iterative reconstruction algorithm, which are generally\nwritten as the optimization of the following problem:\n\n\\begin{align}\\begin{equation}\n    \\label{eq:min_prob}\n    \\tag{1}\n    \\underset{x}{\\arg\\min} \\quad \\datafid{x}{y} + \\lambda \\reg{x},\n    \\end{equation}\\end{align}\n\nwhere $\\datafid{x}{y}$ is the data fidelity term, $\\reg{x}$ is the (explicit or implicit) regularization term,\nand $\\lambda$ is a regularization parameter. In this example, we demonstrate:\n\n1. How to define your own iterative algorithm\n2. How to package it as a :class:`reconstructor model <deepinv.models.Reconstructor>`\n3. How to use predefined algorithms using :class:`optim builder <deepinv.optim.optim_builder>`\n\n## 1. Defining your own iterative algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the physics of the problem\nHere we define a simple inpainting problem, where we want to reconstruct an image from partial measurements.\nWe also load an image of a butterfly to use as ground truth.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"butterfly.png\", device=device, img_size=(128, 128))\n\n# Forward operator, here inpainting with a mask of 50% of the pixels\nphysics = dinv.physics.Inpainting(img_size=(3, 128, 128), mask=0.5, device=device)\n\n# Generate measurements\ny = physics(x)\n\ndinv.utils.plot([x, y], titles=[\"Ground truth\", \"Measurements\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the data fidelity term and prior\nThe library provides a set of `data fidelity <data-fidelity>` terms and `priors <priors>`\nthat can be used in the optimization problem.\nHere we use the $\\ell_2$ data fidelity term and the Total Variation (TV) prior.\n\nThese classes provide all the necessary methods for the optimization problem, such as the evaluation of the term,\nthe gradient, and the proximal operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_fidelity = dinv.optim.L2()  # Data fidelity term\nprior = dinv.optim.TVPrior()  # Prior term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the iterative algorithm\nWe will use the Proximal Gradient Descent (PGD) algorithm to solve the\noptimization problem defined above, which is defined as\n\n\\begin{align}\\qquad x_{k+1} = \\operatorname{prox}_{\\gamma \\lambda \\regname} \\left( x_k - \\gamma \\nabla \\datafidname(x_k, y) \\right),\\end{align}\n\nwhere $\\operatorname{prox}_{\\gamma \\lambda \\regname}$ is the proximal operator of the regularization term,\n$\\nabla \\datafidname(x_k, y)$ is the gradient of the data fidelity term, $\\gamma$ is the stepsize.\nand $\\lambda$ is the regularization parameter.\n\nWe can choose the stepsize as $\\gamma < \\frac{2}{\\|A\\|^2}$, where $A$ is the forward operator,\nin order to ensure convergence of the algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lambd = 0.05  # Regularization parameter\n\n# Compute the squared norm of the operator A\nnorm_A2 = physics.compute_sqnorm(y, tol=1e-4, verbose=False).item()\nstepsize = 1.9 / norm_A2  # stepsize for the PGD algorithm\n\n# PGD algorithm\nmax_iter = 20  # number of iterations\nx_k = torch.zeros_like(x, device=device)  # initial guess\n\n# To store the cost at each iteration:\ncost_history = torch.zeros(max_iter, device=device)\n\nwith torch.no_grad():  # disable autodifferentiation\n    for it in range(max_iter):\n        u = x_k - stepsize * data_fidelity.grad(x_k, y, physics)  # Gradient step\n        x_k = prior.prox(u, gamma=lambd * stepsize)  # Proximal step\n        cost = data_fidelity(x_k, y, physics) + lambd * prior(x_k)  # Compute the cost\n        cost_history[it] = cost  # Store the cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the cost history\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 4))\nplt.plot(cost_history.detach().cpu().numpy(), marker=\"o\")\nplt.title(\"Cost history\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.grid()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the results and metrics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric = dinv.metric.PSNR()\n\ndinv.utils.plot(\n    {\n        f\"Ground truth\": x,\n        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n        f\"Recon w/ TV prior\\n {metric(x_k, x).item():.2f} dB\": x_k,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use a pretrained denoiser as prior\n\nWe can improve the reconstruction by using a pretrained denoiser as prior, by replacing the proximal operator\nwith a denoising step.\nThe library provides `a collection of classical and pretrained denoisers <denoisers>`\nthat can be used in iterative algorithms.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Plug-and-play algorithms can be sensitive to the choice of initialization.\n    Here we use the TV estimate as the initial guess.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_k = x_k.clone()\n\ndenoiser = dinv.models.DRUNet(device=device)  # Load a pretrained denoiser\n\nwith torch.no_grad():  # disable autodifferentiation\n    for it in range(max_iter):\n        u = x_k - stepsize * data_fidelity.grad(x_k, y, physics)  # Gradient step\n        x_k = denoiser(u, sigma=0.05)  # replace prox by denoising step\n\ndinv.utils.plot(\n    {\n        f\"Ground truth\": x,\n        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n        f\"Recon w/ PnP prior\\n {metric(x_k, x).item():.2f} dB\": x_k,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Package your algorithm as a Reconstructor\n\nThe iterative algorithm we defined above can be packaged as a :class:`Reconstructor <deepinv.optim.BaseOptim>`.\nThis allows you to :class:`test it <deepinv.test>` on different physics and datasets, and to use it in a more flexible way,\nincluding unfolding it and learning some of its parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyPGD(dinv.models.Reconstructor):\n    def __init__(self, data_fidelity, prior, stepsize, lambd, max_iter):\n        super().__init__()\n        self.data_fidelity = data_fidelity\n        self.prior = prior\n        self.stepsize = stepsize\n        self.lambd = lambd\n        self.max_iter = max_iter\n\n    def forward(self, y, physics, **kwargs):\n        \"\"\"Algorithm forward pass.\n\n        :param torch.Tensor y: measurements.\n        :param dinv.physics.Physics physics: measurement operator.\n        :return: torch.Tensor: reconstructed image.\n        \"\"\"\n        x_k = torch.zeros_like(y, device=y.device)  # initial guess\n\n        # Disable autodifferentiation, remove this if you want to unfold\n        with torch.no_grad():\n            for _ in range(self.max_iter):\n                u = x_k - self.stepsize * self.data_fidelity.grad(\n                    x_k, y, physics\n                )  # Gradient step\n                x_k = self.prior.prox(\n                    u, gamma=self.lambd * self.stepsize\n                )  # Proximal step\n\n        return x_k\n\n\ntv_algo = MyPGD(data_fidelity, prior, stepsize, lambd, max_iter)\n\n# Standard reconstructor forward pass\nx_hat = tv_algo(y, physics)\n\ndinv.utils.plot(\n    {\n        f\"Ground truth\": x,\n        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n        f\"Recon w/ custom PGD\\n {metric(x_hat, x).item():.2f} dB\": x_hat,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using a predefined optimization algorithm with `optim_builder`\n\nThe library also lets you define `standard optimization algorithms <optim_iterators>`\nas standard :class:`Reconstructors <deepinv.models.Reconstructor>` in one line of code using the :class:`deepinv.optim.optim_builder` function.\nFor example, the above PnP algorithm can be defined as follows:\n\n.. seealso::\n    See `the optimization examples <sphx_glr_auto_examples_optimization_demo_TV_minimisation.py>` for more examples of using `optim_builder`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prior = dinv.optim.PnP(denoiser=denoiser)  # prior with prox via denoising step\n\n\ndef custom_init(y: torch.Tensor, physics: dinv.physics.Physics) -> torch.Tensor:\n    \"\"\"\n    Custom initialization function for the optimization algorithm.\n    The function should return a dictionary with the key \"est\" containing a tuple\n    with the initial guess (the TV solution in this case)\n    and the dual variables (None in this case).\n    \"\"\"\n    primal = tv_algo(y, physics)\n    dual = None  #  No dual variables in this case\n    return {\"est\": (primal, dual)}\n\n\nmodel = dinv.optim.optim_builder(\n    iteration=\"PGD\",\n    prior=prior,\n    data_fidelity=data_fidelity,\n    params_algo={\"stepsize\": stepsize, \"g_param\": 0.05},\n    max_iter=max_iter,\n    custom_init=custom_init,\n)\n\nx_hat = model(y, physics)\n\ndinv.utils.plot(\n    {\n        f\"Ground truth\": x,\n        f\"Measurements\\n {metric(y, x).item():.2f} dB\": y,\n        f\"Reconstruction\\n {metric(x_hat, x).item():.2f} dB\": x_hat,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83c\udf89 Well done, you now know how to define your own iterative reconstruction algorithm!\n\n## What's next?\n\n* Check out more about optimization algorithms in the `optimization user guide <optim>`.\n* Check out diffusion and MCMC iterative algorithms in the `sampling user guide <sampling>`.\n* Check out more `iterative algorithms examples <sphx_glr_auto_examples_optimization>`.\n* Check out how to try the algorithm on a whole dataset by following the `bring your own dataset <sphx_glr_auto_examples_basics_demo_custom_dataset.py>` tutorial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}