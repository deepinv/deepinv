{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using state-of-the-art diffusion models from HuggingFace Diffusers with DeepInverse\n\nThis demo shows you how to use our wrapper\n:class:`deepinv.models.DiffusersDenoiserWrapper` to turn any SOTA models from the HuggingFace Hub to an image denoiser. It also can be used to perform unconditional image generation or for posterior sampling.\n\nSee more about the [diffusers pipeline](https://huggingface.co/docs/diffusers/index) and our posterior sampling [user guide](https://deepinv.github.io/deepinv/auto_examples/sampling/demo_diffusion_sde.html).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example requires the `diffusers` and `transformers` package. You can install it via `pip install diffusers transformers`.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport deepinv as dinv\nfrom deepinv.models.wrapper import DiffusersDenoiserWrapper\n\ndevice = dinv.utils.get_device()\ndtype = torch.float32\nfigsize = 2.5\n\nfrom deepinv.sampling import PosteriorDiffusion, EulerSolver, VarianceExplodingDiffusion\nfrom deepinv.optim import ZeroFidelity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------------------------------------\n\nLet us first load a pretrained diffusion model from the HuggingFace Hub. Here, we use the `google/ddpm-ema-celebahq-256` model.\nThis model is trained on 256x256 of CelebA dataset using the DDPM scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can wrap any diffusers model as a DeepInv denoiser using one line of code:\ndenoiser = DiffusersDenoiserWrapper(\n    mode_id=\"google/ddpm-ema-celebahq-256\", device=device\n)\n\n# Load an example image\nx = dinv.utils.load_example(\n    \"celeba_example2.jpg\",\n    img_size=256,\n    resize_mode=\"resize\",\n).to(device)\n\n# Add noise and test the denoiser\nsigma = 0.1\nx_noisy = x + sigma * torch.randn_like(x)\nwith torch.no_grad():\n    x_denoised = denoiser(x_noisy, sigma=sigma)\n\ndinv.utils.plot(\n    [x, x_noisy, x_denoised],\n    figsize=(figsize * 3, figsize),\n    titles=[\"Original image\", \"Noisy image\", \"Denoised image\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------\n\nIt is also possible to use the wrapped model for unconditional image generation.\nThe model was trained with DDPM scheduler, however we can use it with any SDE provided in DeepInv.\nHere, we use the Variance Exploding SDE with Euler solver for sampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps = 125\nrng = torch.Generator(device)\ntimesteps = torch.linspace(1, 0.001, num_steps)\nsolver = EulerSolver(timesteps=timesteps, rng=rng)\n\nsde = VarianceExplodingDiffusion(\n    device=device,\n    dtype=dtype,\n)\n\nmodel = PosteriorDiffusion(\n    data_fidelity=ZeroFidelity(),\n    sde=sde,\n    denoiser=denoiser,\n    solver=solver,\n    dtype=dtype,\n    device=device,\n    verbose=True,\n)\n\n\nsample, trajectory = model(\n    y=None,\n    physics=None,\n    x_init=(1, 3, 256, 256),\n    seed=42,\n    get_trajectory=True,\n)\ndinv.utils.plot(\n    sample,\n    titles=\"Unconditional generation\",\n    figsize=(figsize, figsize),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------\n\nSimilar to other denoisers in DeepInv, the wrapped diffusers model can be used for posterior sampling.\nBelow we use the same VE-SDE for posterior sampling in an inpainting problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the physics\n\nmask = torch.ones_like(x)\nmask[..., 70:150, 120:180] = 0\nphysics = dinv.physics.Inpainting(\n    mask=mask,\n    img_size=x.shape[1:],\n    device=device,\n    noise_model=dinv.physics.GaussianNoise(0.05),\n)\n\ny = physics(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepinv.sampling import DPSDataFidelity\n\nmodel = PosteriorDiffusion(\n    data_fidelity=DPSDataFidelity(denoiser=denoiser, weight=1.0),\n    denoiser=denoiser,\n    sde=sde,\n    solver=solver,\n    dtype=dtype,\n    device=device,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "posterior_sample = model(\n    y=y,\n    physics=physics,\n    x_init=(1, 3, 256, 256),\n    seed=15,\n)\ndinv.utils.plot(\n    [x, y, posterior_sample],\n    titles=[\"Original image\", \"Measurement\", \"Posterior sample\"],\n    figsize=(figsize * 3, figsize),\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}