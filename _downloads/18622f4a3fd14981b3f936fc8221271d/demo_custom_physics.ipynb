{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bring your own physics\n\nThis examples shows you how to use DeepInverse with your own physics.\n\nWhile DeepInverse offers a `large number of forward operators <physics>`,\nyou can also bring your own forward operator for your specific imaging problem.\n\nDeepInverse's modular `physics framework <physics_intro>` makes this easy by letting you inherit useful methods from the most appropriate\nphysics base class:\n\n* :class:`deepinv.physics.Physics` for non-linear operators;\n* :class:`deepinv.physics.LinearPhysics` for linear operators;\n* :class:`deepinv.physics.DecomposablePhysics` for linear operators with a closed-form singular value decomposition.\n\n.. seealso::\n\n    Often your physics can be modelled without much work. You could:\n\n    * Use an existing physics but with custom `params`, e.g. blur with a custom kernel, or MRI with a custom sampling pattern. See `parameter dependent operators <parameter-dependent-operators>`.\n    * Inherit from an existing physics but override or wrap a particular method e.g. :class:`deepinv.physics.MRI` $\\rightarrow$ :class:`deepinv.physics.DynamicMRI`\n    * Define a new operator by `combining existing operators <physics_combining>`.\n\nIn this example we will demonstrate creating a simple forward operator from scratch that converts RGB images to grayscale images.\nWe also show you how to exploit the singular value decomposition of the operator to speed up the evaluation of\nthe pseudo-inverse and proximal operators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a custom forward operator.\nDefining a new linear operator only requires a forward function $\\forw{\\cdot}$ and its adjoint operation $A^\\top(\\cdot)$,\ninheriting the remaining structure of the :class:`deepinv.physics.LinearPhysics` class.\n\nOnce the operator is defined, we can use any of the functions in the :class:`deepinv.physics.Physics` class and\n:class:`deepinv.physics.LinearPhysics` class, such as computing the norm of the operator, testing the adjointness,\ncomputing the proximal operator, etc.\n\n.. tip::\n    By default, the adjoint of a :class:`LinearPhysics <deepinv.physics.LinearPhysics>` is computed using autograd with :class:`deepinv.physics.adjoint_function`.\n    Note however that defining a closed form adjoint is generally more computationally efficient in memory and time.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To make the new physics compatible with other torch functionalities, all physics parameters (i.e. attributes of type :class:`torch.Tensor`) should be registered as [module buffers](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer) by using `self.register_buffer(param_name, param_tensor)`. This ensures methods like `.to(), .cuda()` work properly, allowing one to train a model using Distributed Data Parallel.</p></div>\n\n.. tip::\n    Inherit from `mixin <mixin>` classes to provide specialized methods for your physics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Decolorize(dinv.physics.LinearPhysics):\n    r\"\"\"\n    Converts RGB images to grayscale.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        coefficients = torch.tensor([0.2989, 0.5870, 0.1140], dtype=torch.float32)\n        self.register_buffer(\"coefficients\", coefficients)\n\n    def A(\n        self, x: torch.Tensor, coefficients: torch.Tensor = None, **kwargs\n    ) -> torch.Tensor:\n        \"\"\"Forward operator.\n\n        :param torch.Tensor x: input image with 3 color (RGB) channels, i.e. [*,3,*,*]\n        :param torch.Tensor coefficients: optionally set coefficients on the fly\n        :param dict kwargs: any other keyword parameters to set on the fly, such as noise model sigma\n        :return: torch.Tensor grayscale measurements\n        \"\"\"\n        super().update_parameters(coefficients=coefficients, **kwargs)\n\n        y = x * self.coefficients[None, :, None, None]\n        return torch.sum(y, dim=1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test physics\n\nWe test our physics on a toy image with 3 color channels.\nWe add a Gaussian noise model to the linear physics.\n\nWe simulate measurements using the forward operator $y=\\forw{x}+\\epsilon$.\nWe then leverage the linear physics base class to automatically compute the linear pseudo-inverse $A^\\dagger y$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.zeros(1, 3, 96, 128)\nx[:, 0, :32, :] = 1\nx[:, 1, 32:64, :] = 1\nx[:, 2, 64:, :] = 1\n\nphysics = Decolorize(\n    img_size=(3, 96, 128), noise_model=dinv.physics.GaussianNoise(sigma=0.1)\n)\n\ny = physics(x)\n\ndinv.utils.plot({\"x\": x, \"y\": y, \"Linear pseudo-inverse\": physics.A_dagger(y)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is often useful for reconstruction algorithms that the physics has unit norm, which you can verify using :func:`deepinv.physics.LinearPhysics.compute_norm`.\nWe see that this physics fails this.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"The linear operator has norm={physics.compute_sqnorm(x):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All parameters or buffers of the physics, such as `coefficients` in the case of `Decolorize`, can be updated on the fly\nwith `physics.update(**params)` or in the forward pass `physics(x, **params)`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"Original coefficients and sigma:\", physics.coefficients, physics.noise_model.sigma\n)\n\nphysics.update(coefficients=torch.tensor([1.0, 2.0, 3.0]), sigma=0.2)\n\nprint(\n    \"Updated coefficients and sigma via update:\",\n    physics.coefficients,\n    physics.noise_model.sigma,\n)\n\ny = physics(x, coefficients=torch.tensor([4.0, 5.0, 6.0]), sigma=0.3)\n\nprint(\n    \"Updated coefficients and sigma via forward pass:\",\n    physics.coefficients,\n    physics.noise_model.sigma,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a closed form adjoint\nInstead, if we know the closed form of the adjoint operator, we can implement it directly in\n:func:`deepinv.physics.LinearPhysics.A_adjoint`, instead of relying on\n:func:`autodifferentiation <deepinv.physics.adjoint_function>` which is generally less efficient.\n\nAn additional benefit of implementing the adjoint is that we no longer require the `img_size` parameter when creating the\noperator, which was needed for autodifferentiation.\n\nIf you implement your own adjoint, it is recommended to verify that it is well-defined using\n:func:`deepinv.physics.LinearPhysics.adjointness_test`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Decolorize2(Decolorize):\n    \"\"\"Override previous Decolorize using a closed-form adjoint.\"\"\"\n\n    def A_adjoint(\n        self, y: torch.Tensor, coefficients: torch.Tensor = None, **kwargs\n    ) -> torch.Tensor:\n        \"\"\"Closed-form adjoint operator.\n\n        :param torch.Tensor y: input grayscale measurements\n        :param torch.Tensor coefficients: optionally set coefficients on the fly\n        :param dict kwargs: any other keyword parameters to set on the fly, such as noise model sigma\n        :return: torch.Tensor adjoint reconstruction\n        \"\"\"\n        super().update_parameters(coefficients=coefficients, **kwargs)\n\n        return y * self.coefficients[None, :, None, None]\n\n\nphysics = Decolorize2()\n\nif physics.adjointness_test(x) < 1e-5:\n    print(\"The linear operator has a well defined adjoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a decomposable forward operator.\nIf the forward operator has a closed form singular value decomposition (SVD),\nyou should instead implement the operator using the :class:`deepinv.physics.DecomposablePhysics` base class.\n\nThe operator $A$ in this example has a known closed-form SVD:\n\n\\begin{align}A = U\\text{diag}(s)V^{\\top}\\end{align}\n\nwhere $\\text{diag}(s)$ is the `mask` of singular values.\n\n\n.. tip::\n   As in the case of `LinearPhysics`, `V` and `U_adjoint` are implemented using autodifferentiation by default,\n   but you can implement them directly if you know the closed form of the operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DecolorizeSVD(dinv.physics.DecomposablePhysics):\n    r\"\"\"\n    Converts RGB images to grayscale.\n\n    We use unnormalized coefficients and a singular value of 0.447.\n\n    Here, `U` and `U_adjoint` are set to the identity.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(mask=0.447, **kwargs)\n        coefficients = torch.tensor([0.6687, 1.3132, 0.2550], dtype=torch.float32)\n        self.register_buffer(\"coefficients\", coefficients)\n\n    def V_adjoint(self, x: torch.Tensor) -> torch.Tensor:\n        y = x * self.coefficients[None, :, None, None]\n        return torch.sum(y, dim=1, keepdim=True)\n\n    def V(self, y: torch.Tensor) -> torch.Tensor:\n        return y * self.coefficients[None, :, None, None]\n\n\nphysics2 = DecolorizeSVD(noise_model=dinv.physics.GaussianNoise(sigma=0.1))\n\ny2 = physics2(x)\n\ndinv.utils.plot({\"x\": x, \"y\": y2, \"Linear pseudo-inverse\": physics2.A_dagger(y2)})\n\nif physics.adjointness_test(x) < 1e-5:\n    print(\"The decomposable operator has a well defined transpose\")\n\nprint(f\"The decomposable operator has norm={physics.compute_sqnorm(x):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benefits of using a decomposable forward operator.\n\nThe main benefit of using a decomposable forward operator is that it provides closed form solutions for the\nproximal operator and the linear pseudo-inverse. Moreover, some algorithms, such as :class:`deepinv.sampling.DDRM`\nrequire the forward operator to be decomposable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\n\ndef sync_cuda():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n\n\nsync_cuda()\nstart = time.time()\nfor i in range(10):\n    xlin = physics.A_dagger(y)\n    xprox = physics.prox_l2(x, y, 0.1)\n\nsync_cuda()\nend = time.time()\nprint(f\"Elapsed time for LinearPhysics: {end - start:.2f} seconds\")\n\nsync_cuda()\nstart = time.time()\nfor i in range(10):\n    xlin2 = physics2.A_dagger(y)\n    xprox2 = physics2.prox_l2(x, y2, 0.1)\n\nsync_cuda()\nend = time.time()\nprint(f\"Elapsed time for DecomposablePhysics: {end - start:.2e} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83c\udf89 Well done, you now know how to implement your own physics!\n\n### What's next?\n* Check out `the example on how to inference a state-of-the-art general pretrained model <sphx_glr_auto_examples_basics_demo_pretrained_model.py>` with your new physics.\n* Check out the `example on how to fine-tune a foundation model <sphx_glr_auto_examples_models_demo_foundation_model.py>` to your own physics.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}