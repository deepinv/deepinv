{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Spatial unwrapping and modulo imaging\n\nThis demo shows the use of the :class:`deepinv.physics.SpatialUnwrapping` forward model and the :class:`deepinv.optim.ItohFidelity` for unwrapping problems, which occur in modulo imaging, interferometry SAR and other imaging applications.\nIt shows how to generate a wrapped phase image, apply blur and noise, and reconstruct the original phase using both DCT inversion and ADMM optimization.\n\n\nThe spatial unwrapping forward model can be mathematically described as follows:\n\n\\begin{align}y = w_t(x + n) = x + n - t \\cdot \\mathrm{q}((x + n) / t)\\end{align}\n\nwhere $x$ is the original image, $n$ is additive noise, and $w_t(\\cdot)$ denotes the modulo (wrapping) operation with threshold $t$.\nHere, $\\mathrm{q}(\\cdot)$ is either the rounding or flooring function, depending on the chosen mode ('round' or 'floor').\nThe goal is to recover $x$ from the observed wrapped image $y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport deepinv as dinv\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\nfrom deepinv.utils.demo import load_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load image and preprocess\nLoad example image and preprocess to emulate a high dynamic range image.\nImages are normalized to [0, 1] and then scaled to the desired dynamic range.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def channel_norm(x):\n    x = x - x.min(dim=-1, keepdim=True)[0].min(dim=-2, keepdim=True)[0]\n    x = x / x.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0]\n    return x\n\n\nsize = 256\ndynamic_range = 2  # dynamic range\nthreshold = 1.0  # threshold for spatial unwrapping\nfactor = 2  # oversampling factor to ensure Itoh condition\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nimg_size = (size, size)\nmode = \"round\"  # available modes: \"round\", \"floor\"\n\n\nx_rgb = load_example(\n    \"CBSD_0010.png\",\n    grayscale=False,\n    device=device,\n    dtype=torch.float32,\n    img_size=img_size,\n)\nx_rgb = channel_norm(x_rgb) * dynamic_range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Itoh condition\nThe Itoh condition requires that the difference between adjacent pixels is less than half of the threshold (here, 1.0)\nto enable perfect unwrapping. Specifically, $\\|Dx\\|_{\\infty} < t / 2$\nwhere $D$ denotes the spatial finite difference operator and $x$ is the original image :footcite:p:`itoh1982analysis`.\nWhen this condition is satisfied, the high dynamic range (HDR) image can be recovered from\nthe wrapped differences of the modulo image by minimizing the Itoh fidelity term:\n\n\\begin{align}\\begin{equation}  \\underset{x}{\\arg\\min} \\quad \\| Dx - w_t(Dy) \\|^2_2. \\end{equation}\\end{align}\n\nBelow, we illustrate this for a single row of the image by visualizing the pixel values, their differences, and the wrapped differences.\nWe can understand the condition by artificially blurring the images with a Gaussian kernel to reduce their high frequencies (and thus the $\\|Dx\\|_{\\infty}$) until the condition is verified.\nFor instace, with a blur of 0.1 it can be seen that the differences $Dx$ exceed the threshold (red dotted lines),\nand consequently, we observe a mismatch with the wrapped differences $w_t(Dy)$,\nwhile with a blur of 2.0, the differences $Dx$ remain within the threshold, matching the wrapped differences $w_t(Dy)$,\nindicating that the Itoh condition is satisfied.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "modulo_round = lambda x: x - torch.round(x)\nmodulo_fn = lambda x: x - torch.floor(x) if mode == \"floor\" else modulo_round(x)\nrow = 120\n\nrow_sel = x_rgb[0, 0, row, :]\n\n\ndef plot_itoh(sigma_blur):\n\n    # Select a row and apply Gaussian blur\n    row_x = row_sel.clone()\n\n    # Construct 1D Gaussian filter with given sigma\n    filter1d = dinv.physics.blur.gaussian_blur(\n        sigma=(sigma_blur, sigma_blur), angle=0.0\n    ).to(device)\n    # Reduce to 1D filter and normalize\n    filter1d = filter1d[..., filter1d.shape[2] // 2, :].squeeze()\n    filter1d = filter1d / filter1d.sum()\n\n    # Upsample by factor and convolve with Gaussian filter\n    row_x = torch.kron(row_x, torch.ones(1, factor).to(device)).squeeze()\n    row_x = torch.nn.functional.conv1d(\n        row_x[None, None, :], filter1d[None, None, :], padding=filter1d.shape[0] // 2\n    ).squeeze()\n\n    # Center around zero for \"round\" mode\n    if mode == \"round\":\n        row_x = row_x - dynamic_range / 2\n\n    # Compute differences and wrapped differences\n    row_dx = row_x[1:] - row_x[:-1]\n    row_y = modulo_fn(row_x)\n    row_wdy = modulo_round(row_y[1:] - row_y[:-1])\n\n    plt.figure(figsize=(10, 2.5))\n    plt.plot(row_x.cpu(), label=\"Pixel values\", linewidth=3, color=\"g\")\n    plt.plot(row_dx.cpu(), label=\"Dx\", linewidth=3, color=\"k\")\n    plt.plot(row_wdy.cpu(), label=\"w_t(Dy)\", linewidth=3, color=\"b\", linestyle=\"--\")\n    plt.axhline(threshold / 2, color=\"r\", linestyle=\"--\", label=\"t/2\")\n    plt.axhline(-threshold / 2, color=\"r\", linestyle=\"--\")\n    plt.title(f\"Itoh Condition, sigma={sigma_blur}, mode={mode}\")\n    plt.xlabel(\"Pixel Index\")\n    plt.ylabel(\"Difference\")\n    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.0, 1.0), ncol=4)\n    plt.show()\n\n\nplot_itoh(sigma_blur=0.1)\nplot_itoh(sigma_blur=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Resize and Gaussian blur\nTo satisfy the Itoh condition, we resize the image and apply a slight Gaussian blur.\nThe blur here is chosen similarly to the 1D analysis above, ensuring adjacent pixel differences are small enough for successful unwrapping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resize = transforms.Resize(size=(img_size[0] * factor, img_size[1] * factor))\nx_rgb = resize(x_rgb)\n\nif mode == \"round\":\n    x_rgb = x_rgb - dynamic_range / 2\n\nfilter_0 = dinv.physics.blur.gaussian_blur(sigma=(1, 1), angle=0.0)\nblur_op = dinv.physics.Blur(filter_0, device=device)\nx_rgb = blur_op(x_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Gaussian noise and modulo operation\nInclude Gaussian noise and wrap the image using SpatialUnwrapping physics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_model = dinv.physics.GaussianNoise(sigma=0.1)\nphysics = dinv.physics.SpatialUnwrapping(\n    threshold=threshold, mode=mode, noise_model=noise_model\n)\nphase_map = x_rgb\nwrapped_phase = physics(phase_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Invert with DCT and ADMM (ItohFidelity)\nWe provide two inversion methods: a simple DCT-based inversion and an ADMM-based inversion using the Itoh fidelity term and TV prior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ADMM-based inversion with TV prior and Itoh fidelity\nstepsize = 1e-4\nlam = 2.0 / stepsize\nprior = dinv.optim.TVPrior(n_it_max=10)\nfidelity = dinv.optim.ItohFidelity(threshold=threshold)\n\n# DCT-based inversion\nx_est = fidelity.D_dagger(wrapped_phase)\n\n\nparams_algo = {\"stepsize\": stepsize, \"lambda\": lam, \"g_param\": 1.0}\nmodel = dinv.optim.optim_builder(\n    iteration=\"ADMM\",\n    prior=prior,\n    data_fidelity=fidelity,\n    max_iter=10,\n    verbose=False,\n    params_algo=params_algo,\n)\nx_model = model(wrapped_phase, physics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize results\nHere we visualize the wrapped phase, original phase, and the reconstructions from both methods.\nWe also compute PSNR and SSIM metrics for both reconstructions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "psnr_fn = dinv.metric.PSNR()\nssim_fn = dinv.metric.SSIM()\n\n# Normalize for display\nx_est = channel_norm(x_est)\nx_model = channel_norm(x_model)\nphase_map = channel_norm(phase_map)\n\n# Compute metrics\npsnr_admm = psnr_fn(phase_map, x_model).item()\npsnr_dct = psnr_fn(phase_map, x_est).item()\nssim_admm = ssim_fn(phase_map, x_model).item()\nssim_dct = ssim_fn(phase_map, x_est).item()\n\n# Plot results\nimgs = [wrapped_phase[0], phase_map[0], x_est[0], x_model[0]]\ntitles = [\n    \"Wrapped Phase\",\n    \"Original Phase\",\n    f\"DCT Inversion\\n PSNR={psnr_dct:.2f} SSIM={ssim_dct:.2f}\",\n    f\"ADMM Inversion\\n PSNR={psnr_admm:.2f} SSIM={ssim_admm:.2f}\",\n]\n\ndinv.utils.plotting.plot(imgs, titles=titles, cmap=\"gray\", figsize=(20, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}