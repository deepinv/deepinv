{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised MRI reconstruction with Artifact2Artifact\n\nWe demonstrate the self-supervised Artifact2Artifact loss for solving an\nundersampled sequential MRI reconstruction problem without ground truth.\n\nThe Artifact2Artifact loss was introduced by :footcite:t:`liu2020rare`.\n\nIn our example, we use it to reconstruct **static** images, where the\nk-space measurements is a time-sequence, where each time step (phase)\nconsists of sampled lines such that the whole measurement is a set of\nnon-overlapping lines.\n\nFor a description of how Artifact2Artifact constructs the loss, see\n:class:`deepinv.loss.mri.Artifact2ArtifactLoss`.\n\nNote in our implementation, this is a special case of the generic\nsplitting loss: see :class:`deepinv.loss.SplittingLoss` for more\ndetails. See :class:`deepinv.loss.mri.Phase2PhaseLoss` for the related\nPhase2Phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nimport deepinv as dinv\nfrom deepinv.datasets import SimpleFastMRISliceDataset\nfrom deepinv.utils import get_data_home\nfrom deepinv.models.utils import get_weights_url\nfrom deepinv.models import MoDL\nfrom deepinv.physics.generator import (\n    GaussianMaskGenerator,\n    BernoulliSplittingMaskGenerator,\n)\n\ntorch.manual_seed(0)\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In this example, we use a mini demo subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_\n# as the base image dataset, consisting of knees of size 320x320, and then resized to 128x128 for speed.\n#\n# .. important::\n#\n#    By using this dataset, you confirm that you have agreed to and signed the `FastMRI data use agreement <https://fastmri.med.nyu.edu/>`_.\n#\n# .. seealso::\n#\n#   Datasets :class:`deepinv.datasets.FastMRISliceDataset` :class:`deepinv.datasets.SimpleFastMRISliceDataset`\n#       We provide convenient datasets to easily load both raw and reconstructed FastMRI images.\n#       You can download more data on the `FastMRI site <https://fastmri.med.nyu.edu/>`_.\n#\n#\n# We use a train set of size 1 and test set of size 1 in this demo for\n# speed to fine-tune the original model. To train the original\n# model from scratch, use a larger dataset of size ~150.\n#\n\nbatch_size = 1\nH = 128\n\ntransform = transforms.Compose([transforms.Resize(H)])\n\ntrain_dataset = SimpleFastMRISliceDataset(\n    get_data_home(), transform=transform, train=True, download=True, train_percent=0.5\n)\ntest_dataset = SimpleFastMRISliceDataset(\n    get_data_home(), transform=transform, train=False, train_percent=0.5\n)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define physics\n\nWe simulate a sequential k-space sampler, that, over the course of 4\nphases (i.e. frames), samples 64 lines (i.e 2x total undersampling from\n128) with Gaussian weighting (plus a few extra for the ACS signals in\nthe center of the k-space). We use\n:class:`deepinv.physics.SequentialMRI` to do this.\n\nFirst, we define a static 2x acceleration mask that all measurements use\n(of shape [B,C,H,W]):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask_full = GaussianMaskGenerator((2, H, H), acceleration=2, device=device).step(\n    batch_size=batch_size\n)[\"mask\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we randomly share the sampled lines across 4 time-phases into a\ntime-varying mask:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Split only in horizontal direction\nmasks = [mask_full[..., 0, :]]\nsplitter = BernoulliSplittingMaskGenerator((2, H), split_ratio=0.5, device=device)\n\nacs = 10\n\n# Split 4 times\nfor _ in range(2):\n    new_masks = []\n    for m in masks:\n        m1 = splitter.step(batch_size=batch_size, input_mask=m)[\"mask\"]\n        m2 = m - m1\n        m1[..., H // 2 - acs // 2 : H // 2 + acs // 2] = 1\n        m2[..., H // 2 - acs // 2 : H // 2 + acs // 2] = 1\n        new_masks.extend([m1, m2])\n    masks = new_masks\n\n# Merge masks into time dimension\nmask = torch.stack(masks, 2)\n\n# Convert to vertical lines\nmask = torch.stack([mask] * H, -2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define physics using this time-varying mask of shape [B,C,T,H,W]:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "physics = dinv.physics.SequentialMRI(mask=mask, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the sequential measurements using a sample image (run\nthis notebook yourself to display the video). We also visualize the\nframe-by-frame no-learning zero-filled reconstruction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = next(iter(train_dataloader)).to(device)\ny = physics(x)\ndinv.utils.plot_videos(\n    [physics.repeat(x, mask), y, mask, physics.A_adjoint(y, keep_time_dim=True)],\n    titles=[\"x\", \"y\", \"mask\", \"x_init\"],\n    display=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also visualize the flattened time-series, recovering the original 2x\nundersampling mask (note the actual undersampling factor is much lower\ndue to ACS lines):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dinv.utils.plot(\n    [x, physics.average(y), physics.average(mask), physics.A_adjoint(y)],\n    titles=[\"x\", \"y\", \"orig mask\", \"x_init\"],\n)\n\nprint(\"Total acceleration:\", (2 * 128 * 128) / mask.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define model\n\nAs a (static) reconstruction network, we use an unrolled network\n(half-quadratic splitting) with a trainable denoising prior based on the\nDnCNN architecture which was proposed in MoDL :footcite:t:`aggarwal2018modl`.\nSee :class:`deepinv.models.MoDL` for details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = MoDL().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prep loss\n\nPerform loss on all collected lines by setting ``dynamic_model`` to\nFalse. Then adapt model to perform Artifact2Artifact. We set\n``split_size=1`` to mean that each Artifact chunk containes only 1\nframe.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = dinv.loss.mri.Artifact2ArtifactLoss(\n    (2, 4, H, H), split_size=1, dynamic_model=False, device=device\n)\nmodel = loss.adapt_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train model\n\nOriginal model is trained for 100 epochs. We demonstrate loading the\npretrained model then fine-tuning with 1 epoch. Report PSNR and SSIM. To\ntrain from scratch, simply comment out the model loading code and\nincrease the number of epochs.\n\nTo simulate a realistic self-supervised learning scenario, we do not use any supervised metrics for training,\nsuch as PSNR or SSIM, which require clean ground truth images.\n\n.. tip::\n\n      We can use the same self-supervised loss for evaluation, as it does not require clean images,\n      to monitor the training process (e.g. for early stopping). This is done automatically when `metrics=None` and `early_stop>0` in the trainer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n\n# Load pretrained model\nfile_name = \"demo_artifact2artifact_mri.pth\"\nurl = get_weights_url(model_name=\"measplit\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url, map_location=lambda storage, loc: storage, file_name=file_name\n)\n\nmodel.load_state_dict(ckpt[\"state_dict\"], strict=False)\noptimizer.load_state_dict(ckpt[\"optimizer\"])\n\n# Initialize the trainer\ntrainer = dinv.Trainer(\n    model,\n    physics=physics,\n    epochs=1,\n    losses=loss,\n    optimizer=optimizer,\n    train_dataloader=train_dataloader,\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    metrics=None,\n    eval_dataloader=test_dataloader,\n    early_stop=2,  # early stop using the self-supervised loss on the test set\n    online_measurements=True,\n    device=device,\n    save_path=None,\n    verbose=True,\n    show_progress_bar=False,\n)\n\nmodel = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the model\n\nWe now assume that we have access to a small test set of ground-truth images to evaluate the performance of the trained network.\nand we compute the PSNR between the denoised images and the clean ground truth images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.plot_images = True\ntrainer.test(test_dataloader, metrics=[dinv.metric.PSNR(), dinv.metric.SSIM()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}