{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ud83d\ude80 To get started, install DeepInverse by creating a new cell and running `%pip install deepinv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Inference and fine-tune a foundation model\n\nThis example shows how to perform inference on and fine-tune the Reconstruct Anything Model (RAM) foundation model :footcite:p:`terris2025reconstruct` to solve inverse problems.\n\nThe :class:`Reconstruct Anything Model <deepinv.models.RAM>` is a model that has been trained to work on a large\nvariety of linear image reconstruction tasks and datasets (deblurring, inpainting, denoising, tomography, MRI, etc.)\nand is robust to a wide variety of imaging domains.\n\n.. tip::\n\n    * Want to use your own dataset? See `sphx_glr_auto_examples_basics_demo_custom_dataset.py`\n    * Want to use your own physics? See `sphx_glr_auto_examples_basics_demo_custom_physics.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nimport torch\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n\nmodel = dinv.models.RAM(device=device, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Zero-shot inference\n\nFirst, let's evaluate the zero-shot inference performance of the foundation model.\n\n### Accelerated medical imaging\n\nHere, we demonstrated reconstructing brain MRI from an accelerated noisy MRI scan from [FastMRI](https://fastmri.med.nyu.edu/):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"demo_mini_subset_fastmri_brain_0.pt\", device=device)\n\n# Define physics\nphysics = dinv.physics.MRI(noise_model=dinv.physics.GaussianNoise(0.05), device=device)\n\nphysics_generator = dinv.physics.generator.GaussianMaskGenerator(\n    (320, 320), device=device\n)\n\n# Generate measurement\ny = physics(x, **physics_generator.step())\n\n# Perform inference\nwith torch.no_grad():\n    x_hat = model(y, physics)\n    x_lin = physics.A_adjoint(y)\n\npsnr = dinv.metric.PSNR()\n\ndinv.utils.plot(\n    {\n        \"Ground truth\": x,\n        f\"Linear inverse\": x_lin,\n        f\"Pretrained RAM\": x_hat,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(x, x_lin).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computational photography\n\nJoint random motion deblurring and denoising, using a cropped image from color BSD:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"CBSD_0010.png\", img_size=(200, 200), device=device)\n\nphysics = dinv.physics.BlurFFT(\n    img_size=x.shape[1:],\n    noise_model=dinv.physics.GaussianNoise(sigma=0.05),\n    device=device,\n)\n\n# fmt: off\nphysics_generator = ( \n    dinv.physics.generator.MotionBlurGenerator((31, 31), l=2.0, sigma=2.4, device=device) +\n    dinv.physics.generator.SigmaGenerator(sigma_min=0.001, sigma_max=0.2, device=device)\n)\n# fmt: on\n\ny = physics(x, **physics_generator.step())\n\nwith torch.no_grad():\n    x_hat = model(y, physics)\n    x_lin = physics.A_adjoint(y)\n\ndinv.utils.plot(\n    {\n        \"Ground truth\": x,\n        f\"Linear inverse\": x_lin,\n        f\"Pretrained RAM\": x_hat,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(x, x_lin).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tomography\nComputed Tomography with limited angles\nusing data from the [The Cancer Imaging Archive](https://link.springer.com/article/10.1007/s10278-013-9622-7) of lungs:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"CT100_256x256_0.pt\", device=device)\n\nphysics = dinv.physics.Tomography(\n    img_width=256,\n    angles=10,\n    normalize=True,\n    device=device,\n)\n\ny = physics(x)\n\nwith torch.no_grad():\n    x_hat = model(y, physics)\n    x_lin = physics.A_dagger(y)\n\ndinv.utils.plot(\n    {\n        \"Ground truth\": x,\n        f\"FBP pseudo-inverse\": x_lin,\n        f\"Pretrained RAM\": x_hat,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(x, x_lin).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remote sensing\nSatellite denoising with Poisson-Gaussian noise using urban data from the [WorldView-3 satellite](https://earth.esa.int/eogateway/missions/worldview-3)\nover Jacksonville:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"JAX_018_011_RGB.tif\", device=device)[..., :300, :300]\n\nphysics = dinv.physics.Denoising(\n    noise_model=dinv.physics.PoissonGaussianNoise(sigma=0.1, gain=0.1)\n)\n\ny = physics(x)\n\nwith torch.no_grad():\n    x_hat = model(y, physics)\n    # Alternatively, use the model without physics:\n    # x_hat = model(y, sigma=0.1, gain=0.1)\n    x_lin = physics.A_adjoint(y)\n\ndinv.utils.plot(\n    {\n        \"Ground truth\": x,\n        f\"Linear inverse\": x_lin,\n        f\"Pretrained RAM\": x_hat,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(x, x_lin).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fine-tuning\nAs with all models, there may be a drop in performance when used zero-shot on problems or data outside those seen during training.\n\nFor instance, RAM is not trained on image demosaicing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dinv.utils.load_example(\"butterfly.png\", img_size=(127, 129), device=device)\n\nphysics = dinv.physics.Demosaicing(\n    img_size=x.shape[1:], noise_model=dinv.physics.PoissonNoise(0.1), device=device\n)\n\n# Generate measurement\ny = physics(x)\n\n# Run inference\nwith torch.no_grad():\n    x_hat = model(y, physics)\n\n# Show results\ndinv.utils.plot(\n    {\n        \"Original\": x,\n        f\"Measurement\": y,\n        f\"Reconstruction\": x_hat,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(x, y).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve results, we can fine-tune the model on our problem and data,\n**even in the absence of ground truth data**, using a `self-supervised loss <self-supervised-losses>`,\nand **even on a single image only**.\n\nHere, since this example is run in a no-GPU environment, we will use a small patch of the image to speed up training,\nbut in practice, we can use the full image.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can also fine-tune on larger datasets if you want, by replacing the `dataset <datasets>`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Take small patch\nx_train = x[..., :64, :64]\n\nphysics_train = dinv.physics.Demosaicing(\n    img_size=x_train.shape[1:],\n    noise_model=dinv.physics.PoissonNoise(0.1, clip_positive=True),\n    device=device,\n)\n\ny_train = physics_train(x_train)\n\n# Define training loss\nlosses = [\n    dinv.loss.R2RLoss(),\n    dinv.loss.EILoss(dinv.transform.Shift(shift_max=0.4), weight=0.1),\n]\n\ndataset = dinv.datasets.TensorDataset(y=y_train)\ntrain_dataloader = torch.utils.data.DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fine-tune using early stopping on a validation set, again without ground truth.\nWe use a small patch of another set of measurements as validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eval_dataloader = torch.utils.data.DataLoader(\n    dinv.datasets.TensorDataset(\n        y=physics_train(\n            dinv.utils.load_example(\"leaves.png\", device=device)[..., :64, :64]\n        )\n    )\n)\n\nmax_epochs = 20\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics_train,\n    eval_interval=5,\n    ckp_interval=max_epochs - 1,\n    metrics=None,\n    compute_eval_losses=True,  # use self-supervised loss for evaluation\n    early_stop_on_losses=True,  # stop using self-supervised eval loss\n    early_stop=2,  # early stop after 2 evals without improvement\n    device=device,\n    losses=losses,\n    epochs=max_epochs,\n    optimizer=torch.optim.Adam(model.parameters(), lr=5e-5),\n    train_dataloader=train_dataloader,\n    eval_dataloader=eval_dataloader,\n    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n)\n\nfinetuned_model = trainer.train()\n\nfinetuned_model = trainer.load_best_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use the fine-tuned model to reconstruct the image from the measurement `y`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    x_hat_ft = finetuned_model(y, physics)\n\n# Show results\ndinv.utils.plot(\n    {\n        \"Original\": x,\n        f\"Measurement\": y,\n        f\"Zero-shot \\nReconstruction\": x_hat,\n        f\"Fine-tuned \\nReconstruction\": x_hat_ft,\n    },\n    subtitles=[\n        \"PSNR:\",\n        f\"{psnr(y, x).item():.2f} dB\",\n        f\"{psnr(x, x_hat).item():.2f} dB\",\n        f\"{psnr(x, x_hat_ft).item():.2f} dB\",\n    ],\n    figsize=(6, 4),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":References:\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}